[{"path":"index.html","id":"welcome-to-moderndive-v2","chapter":"Welcome to ModernDive (v2)","heading":"Welcome to ModernDive (v2)","text":"website Statistical Inference via Data Science: ModernDive R Tidyverse (Second Edition)! Visit GitHub repository site. published print copy CRC Press 2025! can find summary updates version .First Edition book available https://moderndive.com. can find printed copy First Edition Amazon. can also purchase CRC Press.work Chester Ismay, Albert Y. Kim, Arturo Valdivia licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ","code":""},{"path":"foreword.html","id":"foreword","chapter":"Foreword","heading":"Foreword","text":"exciting times statistics data science education. (predicting statement continue true regardless whether reading foreword 2025 2050.)\n(isn’t always ?), statistics data science educator, can also feel bit overwhelming stay top new statistical, technological, pedagogical innovations.\nfind constantly asking, “teaching students correct content, relevant software, effective way?”\nmake us feel lost sea, let point great life raft found ModernDive.\nsea intro stats data science textbooks, ModernDive floats top list, let tell .\n(Note use ModernDive refers book shortened title version.\nalso matches nicely neat hex sticker Drs. Ismay, Kim, Valdivia created cover ModernDive, .)favorite aspect ModernDive, must pick favorite, students gain experience whole data analysis pipeline (see Figure 0.2).\nparticular, ModernDive one intro stats data science textbooks teaches students wrangle data.\n, data cleaning may groovy model building, ’s often prerequisite step!\nworld full messy data ModernDive equips students transform data via dplyr package.Speaking dplyr, students ModernDive exposed tidyverse suite R packages.\nDesigned common structure, tidyverse functions written easy learn use.\n, since intro stats data science students programming newbies, ModernDive carefully walks students new function presents provides frequent reinforcement many Learning checks dispersed throughout chapters.Overall, ModernDive includes wise choices placement topics.\nStarting data visualization, ModernDive gets students building ggplot2 graphs early continues reinforce important concepts graphically throughout book.\nmoving data wrangling data importing, modeling plays prominent role, two chapters devoted building regression models later chapter inference regression.\nLastly, statistical inference presented first computational lens using theory-based approach.\ninfer package used approaches allows easy comparisons simulation-based theory-based methods.first met two authors, Drs. Ismay Kim, attending workshop 2017 US Conference Teaching Statistics.\npushed us participants put data first use computers, instead math, engine statistical inference.\nexperience helped add data science concepts intro stats course introduced two really forward-thinking statistics data science educators.\naddition Dr. Valdivia second edition, exciting see ModernDive continue develop grow wonderful, timely textbook.\nnew edition includes even engaging datasets, code updates include fancy base-pipe, insights inference, materials leverage newer functions infer (make sure check magical fit() function!).\nrefresh, ModernDive continues lead pack truly contemporary approach learning introductory statistics data science.hope decided dive !Kelly S. McConville, Bucknell University","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Help! ’m completely new coding need learn R RStudio! ?’re asking question, ’ve come right place! Start “Introduction students” section.instructor hoping use book courses? recommend reading “Introduction students” section first. , read “Introduction instructors” section information teach book.looking connect contribute ModernDive? , read “Connect contribute” section information .curious publishing book? , read “book” section information open-source technology, particular R Markdown bookdown package.version 2.0.0 ModernDive published December 4, 2024. previous versions ModernDive, see “book” section.","code":""},{"path":"preface.html","id":"introduction-for-students","chapter":"Preface","heading":"Introduction for students","text":"book assumes prerequisites: algebra, calculus, prior programming/coding experience. intended gentle introduction practice analyzing data answering questions using data way data scientists, statisticians, data journalists, researchers .present map upcoming journey Figure 0.1.\nFIGURE 0.1: ModernDive flowchart.\n’ll first get started data Chapter 1 ’ll learn difference R RStudio, start coding R, install load first R packages, explore first dataset: domestic departure flights New York City airport 2023. ’ll cover following three portions book (Parts 2 4 combined single portion):Data science tidyverse. ’ll assemble data science toolbox using tidyverse packages. particular, ’ll\nCh.2: Visualize data using ggplot2 package.\nCh.3: Wrangle data using dplyr package.\nCh.4: Learn concept “tidy” data standardized data input output format packages tidyverse. Furthermore, ’ll learn import spreadsheet files R using readr package.\nCh.2: Visualize data using ggplot2 package.Ch.3: Wrangle data using dplyr package.Ch.4: Learn concept “tidy” data standardized data input output format packages tidyverse. Furthermore, ’ll learn import spreadsheet files R using readr package.Statistical/Data modeling moderndive. Using data science tools helper functions moderndive package, ’ll fit first data models. particular, ’ll\nCh.5: Discover basic regression models one explanatory variable.\nCh.6: Examine multiple regression models one explanatory variable.\nCh.5: Discover basic regression models one explanatory variable.Ch.6: Examine multiple regression models one explanatory variable.Statistical inference infer. using newly acquired data science tools, ’ll unpack statistical inference using infer package. particular, ’ll:\nCh.7: Learn role sampling variability plays statistical inference role sample size plays sampling variability.\nCh.8: Construct confidence intervals using bootstrapping.\nCh.9: Conduct hypothesis tests using permutation.\nCh.7: Learn role sampling variability plays statistical inference role sample size plays sampling variability.Ch.8: Construct confidence intervals using bootstrapping.Ch.9: Conduct hypothesis tests using permutation.Statistical/Data modeling moderndive (revisited): Armed understanding statistical inference, ’ll revisit review models ’ve constructed Ch.5 Ch.6. particular, ’ll:\nCh.10: Interpret confidence intervals hypothesis tests regression setting.\nCh.10: Interpret confidence intervals hypothesis tests regression setting.’ll end discussion means “tell story data” Chapter 11 presenting example case studies.1","code":""},{"path":"preface.html","id":"what-we-hope-you-will-learn-from-this-book","chapter":"Preface","heading":"What we hope you will learn from this book","text":"hope end book, ’ll learned :Use R tidyverse suite R packages data science.Fit first models data, using method known linear regression.Perform statistical inference using sampling, confidence intervals, hypothesis tests.Tell story data using tools.mean data stories? mean analysis involving data engages reader answering questions careful visuals thoughtful discussion. discussions data stories can found blog post “Tell Meaningful Story Data.”course book, develop “data science toolbox,” equipping tools data visualization, data formatting, data wrangling, statistical/data modeling using regression.particular, book lean heavily data visualization. today’s world, bombarded graphics attempt convey ideas. explore makes good graphic standard ways used convey relationships within data. general, ’ll use visualization way building almost ideas book.impart statistical lessons book, intentionally minimized number mathematical formulas used. Instead, ’ll develop conceptual understanding statistics using data visualization computer simulations. hope intuitive experience way statistics traditionally taught past commonly perceived.Finally, ’ll learn importance literate programming. mean ’ll learn write code useful just computer execute, also readers understand exactly analysis . part greater effort encourage reproducible research (see “Reproducible research” subsection Preface details). Hal Abelson coined phrase follow throughout book:Programs must written people read, incidentally machines execute.understand may challenging moments learn program. us continue struggle find often using web searches find answers reach colleagues help. long run though, can solve problems faster elegantly via programming. wrote book way help get started know huge community R users happy help everyone along well. community exists particular internet various forums websites stackoverflow.com.","code":""},{"path":"preface.html","id":"datascience-pipeline","chapter":"Preface","heading":"Data/science pipeline","text":"may think statistics just bunch numbers. commonly hear phrase “statistician” listening broadcasts sporting events. Statistics (particular, data analysis), addition describing numbers like baseball batting averages, plays vital role sciences. ’ll commonly hear phrase “statistically significant” thrown around media. ’ll see articles say, “Science now shows chocolate good .” Underpinning claims data analysis. end book, ’ll able better understand whether claims trusted whether wary. Inside data analysis many sub-fields discuss throughout book (though necessarily order):data collectiondata wranglingdata visualizationstatistical modelinginferencecorrelation regressioninterpretation resultsdata communication/storytellingThese sub-fields summarized Garrett Grolemund Hadley Wickham previously termed “data/science pipeline” Figure 0.2.\nFIGURE 0.2: Data/science pipeline.\nbegin digging grey Understand portion cycle data visualization, discussion meant tidy data data wrangling, conclude talking interpreting discussing results models via Communication. steps vital statistical analysis. , care statistics?’s reason many fields require statistics course. Scientific knowledge grows understanding statistical significance data analysis. needn’t intimidated statistics. ’s beast used , paired computation, ’ll see reproducible research sciences particularly increases scientific knowledge.","code":""},{"path":"preface.html","id":"reproducible-research","chapter":"Preface","heading":"Reproducible research","text":"important tool mindset, starting, end product reproducible. – Keith BaggerlyAnother goal book help readers understand importance reproducible analyses. hope get readers habit making analyses reproducible beginning. means ’ll trying help build new habits. take practice difficult times. ’ll see just important keep track code document well help later potential collaborators well.Copying pasting results one program word processor ideal way conduct efficient effective scientific research. ’s much important time spent data collection data analysis copying pasting plots back forth across variety programs.traditional analyses, error made original data, ’d need step entire process : recreate plots copy--paste new plots statistical analysis document. error prone frustrating use time. want help get away tedious activity can spend time science.talking computational reproducibility. – Yihui XieReproducibility means lot things terms different scientific fields. experiments conducted way another researcher follow steps get similar results? book, focus known computational reproducibility. refers able pass one’s data analysis, datasets, conclusions someone else get exactly results machine. allows time spent interpreting results considering assumptions instead error prone way starting scratch following list steps may different machine machine.","code":""},{"path":"preface.html","id":"final-note-for-students","chapter":"Preface","heading":"Final note for students","text":"point, interested instructor perspectives book, ways contribute collaborate, technical details book’s construction publishing, continue rest chapter. Otherwise, let’s get started R RStudio Chapter 1!","code":""},{"path":"preface.html","id":"introduction-for-instructors","chapter":"Preface","heading":"Introduction for instructors","text":"","code":""},{"path":"preface.html","id":"resources","chapter":"Preface","heading":"Resources","text":"resources help use ModernDive:’ve included review questions posed Learning checks. can find solutions Learning checks Appendices online version book. Appendices start https://moderndive.com/v2/appendixa.Dr. Jenny Smetzer Albert Y. Kim written series labs problem sets. can find https://moderndive.com/labs.can see webpages two courses use ModernDive:\nSmith College “SDS192 Introduction Data Science”: https://rudeboybert.github.io/SDS192/.\nSmith College “SDS220 Introduction Probability Statistics”: https://rudeboybert.github.io/SDS220/.\nSmith College “SDS192 Introduction Data Science”: https://rudeboybert.github.io/SDS192/.Smith College “SDS220 Introduction Probability Statistics”: https://rudeboybert.github.io/SDS220/.","code":""},{"path":"preface.html","id":"why-did-we-write-this-book","chapter":"Preface","heading":"Why did we write this book?","text":"book inspired byMathematical Statistics Resampling R (Chihara Hesterberg 2011)OpenIntro: Intro Stat Randomization Simulation (Diez, Barr, Çetinkaya-Rundel 2014)R Data Science (Grolemund Wickham 2017)first book, designed upper-level undergraduates graduate students, provides excellent resource use resampling impart statistical concepts like sampling distributions using computation instead large-sample approximations mathematical formulas. last two books free options learning introductory statistics data science, providing alternative many traditionally expensive introductory statistics textbooks.looking introductory statistics textbooks currently exist, found wasn’t one incorporated many newly developed R packages directly text, particular many packages included tidyverse set packages, ggplot2, dplyr, tidyr, readr focus book’s first part “Data Science tidyverse.”Additionally, wasn’t open-source easily reproducible textbook available exposed new learners four learning goals listed “Introduction students” subsection. wanted write book develop theory via computational techniques help novices master R language .","code":""},{"path":"preface.html","id":"who-is-this-book-for","chapter":"Preface","heading":"Who is this book for?","text":"book intended instructors traditional introductory statistics classes using RStudio, like inject data science topics syllabus. RStudio can used either server version desktop version. (discussed Subsection 1.1.1.) assume students taking class prior algebra, calculus, programming/coding experience.principles beliefs kept mind writing text. agree , book .Blur lines lecture lab\nincreased availability accessibility laptops open-source non-proprietary statistical software, strict dichotomy lab lecture can loosened.\n’s much harder students understand importance using software use week less. forget syntax much way someone learning foreign language forgets grammar rules. Frequent reinforcement key.\nincreased availability accessibility laptops open-source non-proprietary statistical software, strict dichotomy lab lecture can loosened.’s much harder students understand importance using software use week less. forget syntax much way someone learning foreign language forgets grammar rules. Frequent reinforcement key.Focus entire data/science research pipeline\nbelieve entirety Grolemund Wickham’s data/science pipeline seen Figure 0.2 taught.\nheed George Cobb’s call “minimize prerequisites research”: students answering questions data soon possible.\nbelieve entirety Grolemund Wickham’s data/science pipeline seen Figure 0.2 taught.heed George Cobb’s call “minimize prerequisites research”: students answering questions data soon possible.’s data\nleverage R packages rich, real, realistic datasets time easy--load R, nycflights23 fivethirtyeight packages.\nbelieve data visualization “gateway drug” statistics grammar graphics implemented ggplot2 package best way impart lessons. However, often hear: “can’t teach ggplot2 data visualization intro stats!” , like David Robinson, much optimistic found students largely successful learning .\ndplyr made data wrangling much accessible novices, hence much interesting datasets can explored.\nleverage R packages rich, real, realistic datasets time easy--load R, nycflights23 fivethirtyeight packages.believe data visualization “gateway drug” statistics grammar graphics implemented ggplot2 package best way impart lessons. However, often hear: “can’t teach ggplot2 data visualization intro stats!” , like David Robinson, much optimistic found students largely successful learning .dplyr made data wrangling much accessible novices, hence much interesting datasets can explored.Use simulation/resampling introduce statistical inference, probability/mathematical formulas\nInstead using formulas, large-sample approximations, probability tables, teach statistical concepts using simulation-based inference.\nallows de-emphasis traditional probability topics, freeing room syllabus topics. Bridges mathematical concepts given well help relation traditional topics modern approaches.\nInstead using formulas, large-sample approximations, probability tables, teach statistical concepts using simulation-based inference.allows de-emphasis traditional probability topics, freeing room syllabus topics. Bridges mathematical concepts given well help relation traditional topics modern approaches.Don’t fence students computation pool, throw !\nComputing skills essential working data 21st century. Given fact, feel shield students computing ultimately disservice.\nteaching course coding/programming per se, rather just enough computational algorithmic thinking necessary data analysis.\nComputing skills essential working data 21st century. Given fact, feel shield students computing ultimately disservice.teaching course coding/programming per se, rather just enough computational algorithmic thinking necessary data analysis.Complete reproducibility customizability\nfrustrated textbooks give examples, source code data . give source code examples well whole book! made choices occasionally hide code produces complicated figures, reviewing book’s GitHub repository provide code (see “book”).\nUltimately best textbook one ’ve written . know best audience, background, priorities. know best style types examples problems like best. Customization ultimate end. encourage take ’ve provided make work needs. make book , see “book” later Preface.\nfrustrated textbooks give examples, source code data . give source code examples well whole book! made choices occasionally hide code produces complicated figures, reviewing book’s GitHub repository provide code (see “book”).Ultimately best textbook one ’ve written . know best audience, background, priorities. know best style types examples problems like best. Customization ultimate end. encourage take ’ve provided make work needs. make book , see “book” later Preface.","code":""},{"path":"preface.html","id":"connect-and-contribute","chapter":"Preface","heading":"Connect and contribute","text":"like connect ModernDive, check following links:like receive periodic updates ModernDive (roughly every 6 months), please sign mailing list.’re X (formerly Twitter) https://x.com/ModernDive.like contribute ModernDive, many ways! love help feedback make book great possible! example, find errors, typos, areas improvement, please post issue GitHub issues page. familiar GitHub like contribute, see “book” section.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"authors like thank Nina Sonneborn, Dr. Alison Hill, Kristin Bott, Dr. Jenny Smetzer, Prof. Katherine Kinnaird, participants 2017 2019 USCOTS workshops feedback suggestions. ’d also like thank Dr. Andrew Heiss contributing nearly Subsection 1.2.3 “Errors, warnings, messages,” Evgeni Chasnovski creating geom_parallel_slopes() extension ggplot2 package plotting parallel slopes models, Smith College Statistical & Data Sciences students Starry Zhou Marium Tapal many edits book. special thanks goes Dr. Jude Weinstein-Jones, co-founder Learning Scientists, extensive feedback. Much appreciation also goes Jasmin Lörchner thorough read, continued support, thoughtful edits second edition book!honored Dr. Kelly S. McConville write Foreword editions book. Dr. McConville pioneer statistics education source great inspiration us continued update book get current form. Thanks additionally continued contributions members community book GitHub many individuals recommended book others. appreciative !Lastly, special shout student ever taken class us Pacific University, Reed College, Middlebury College, Amherst College, Smith College, Indiana University. couldn’t made book without !","code":""},{"path":"preface.html","id":"about-the-book","chapter":"Preface","heading":"About this book","text":"book written using bookdown package Yihui Xie(Xie 2025). package simplifies book publishing content written R Markdown. R Markdown source code ModernDive available GitHub:Latest online version\nErrata updates made online text. tracked GitHub via NEWS.md file .\nAvailable https://moderndive.com/v2/\nErrata updates made online text. tracked GitHub via NEWS.md file .Available https://moderndive.com/v2/Print second edition CRC Press print edition corresponds Version 2.0.0 released March 20, 2025 (source code). welcomed Dr. Arturo Valdivia co-author edition. deep knowledge statistics superb teaching experience invaluable improving book. summary updated v1.0.0 v2.0.0 (first print edition second print edition). Additional information changes book time available GitHub page .\nUpdated Datasets Code: Replaced datasets (promotions, evals, pennies) new ones (un_member_states_2024, spotify_by_genre, almonds_bowl). Adopted nycflights23 package instead nycflights13 introduced base R pipe (|>) instead tidyverse pipe (%>%). Also incorporated envoy_flights early_january_2023_weather moderndive package.\nContent Reorganization: Restructured sections Chapters 7 10 improved readability. Moved “Model Selection” Chapter 6 Chapter 10 split two new subsections per suggestions.\nEnhanced Theoretical Discussions: Improved theory-based discussions Chapters 7, 8, 10, 11, added sections better connect statistical inference based reviewer feedback.\nNew Examples Functions: Introduced coffee_quality old_faithful_2024 datasets examples Chapter 10, added use fit() function infer package simulation-based inference multiple linear regression, added infer coverage Chapter 11.\nCode Enhancements Clarifications: Standardized code use |>, addressed warnings group_by(), added relocate() Chapter 3.\nRevamped Learning Checks:: Updated designed new Learning checks throughout book better assess student understanding.\nUpdated Datasets Code: Replaced datasets (promotions, evals, pennies) new ones (un_member_states_2024, spotify_by_genre, almonds_bowl). Adopted nycflights23 package instead nycflights13 introduced base R pipe (|>) instead tidyverse pipe (%>%). Also incorporated envoy_flights early_january_2023_weather moderndive package.Content Reorganization: Restructured sections Chapters 7 10 improved readability. Moved “Model Selection” Chapter 6 Chapter 10 split two new subsections per suggestions.Enhanced Theoretical Discussions: Improved theory-based discussions Chapters 7, 8, 10, 11, added sections better connect statistical inference based reviewer feedback.New Examples Functions: Introduced coffee_quality old_faithful_2024 datasets examples Chapter 10, added use fit() function infer package simulation-based inference multiple linear regression, added infer coverage Chapter 11.Code Enhancements Clarifications: Standardized code use |>, addressed warnings group_by(), added relocate() Chapter 3.Revamped Learning Checks:: Updated designed new Learning checks throughout book better assess student understanding.Print first edition CRC Press print edition ModernDive corresponds Version 1.1.0 (typos fixed). Available https://moderndive.com/.Previous online versions Older versions may date:\nVersion 1.0.0 released November 25, 2019 (source code)\nVersion 0.6.1 released August 28, 2019 (source code)\nVersion 0.6.0 released August 7, 2019 (source code)\nVersion 0.5.0 released February 24, 2019 (source code)\nVersion 0.4.0 released July 21, 2018 (source code)\nVersion 0.3.0 released February 3, 2018 (source code)\nVersion 0.2.0 released August 2, 2017 (source code)\nVersion 0.1.3 released February 9, 2017 (source code)\nVersion 0.1.2 released January 22, 2017 (source code)\nVersion 1.0.0 released November 25, 2019 (source code)Version 0.6.1 released August 28, 2019 (source code)Version 0.6.0 released August 7, 2019 (source code)Version 0.5.0 released February 24, 2019 (source code)Version 0.4.0 released July 21, 2018 (source code)Version 0.3.0 released February 3, 2018 (source code)Version 0.2.0 released August 2, 2017 (source code)Version 0.1.3 released February 9, 2017 (source code)Version 0.1.2 released January 22, 2017 (source code)new paradigm textbooks? Instead traditional model textbook companies publishing updated editions textbook every years, apply software design influenced model publishing easily updated versions. can leverage open-source communities instructors developers ideas, tools, resources, feedback. , welcome GitHub pull requests.Finally, since book Creative Commons Attribution - NonCommercial – ShareAlike 4.0 license, feel free modify book wish non-commercial needs, please list authors top index.Rmd : “Chester Ismay, Albert Y. Kim, Arturo Valdivia, !”","code":""},{"path":"preface.html","id":"versions-of-r-packages-used","chapter":"Preface","heading":"Versions of R packages used","text":"’d like output computer match exactly output presented throughout book, may want use exact versions packages used. can find full listing packages versions . likely won’t relevant novices, included reproducibility.seeing different results book, recommend installing exact version packages used. can done first installing remotes package via install.packages(\"remotes\"). , use install_version() replacing package argument package name quotes version argument particular version number install ","code":"\nremotes::install_version(package = \"moderndive\", version = \"0.6.1\")"},{"path":"about-the-authors.html","id":"about-the-authors","chapter":"About the authors","heading":"About the authors","text":"Chester Ismay freelance data scientist, consultant, educator. also teaches Center Executive Professional Education Portland State University. completed PhD statistics Arizona State University 2013. previously worked variety roles including actuary Scottsdale Insurance Company (now Nationwide E&S/Specialty) Ripon College, Reed College, Pacific University. experience working online education previously Data Science Evangelist DataRobot, led data science, machine learning, data engineering -person virtual workshops DataRobot University. addition work ModernDive, also contributed initial developer infer R package author maintainer thesisdown R package.Webpage: https://chester.rbind.io/GitHub: https://github.com/ismaycAlbert Y. Kim Associate Professor Statistical & Data Sciences Smith College Northampton, MA, USA. completed PhD statistics University Washington 2011. Previously worked Search Ads Metrics Team Google Inc. well Reed, Middlebury, Amherst Colleges. addition work ModernDive, co-author resampledata SpatialEpi R packages. Dr. Kim Dr. Ismay, along Jennifer Chunn, co-authors fivethirtyeight package code datasets published data journalism website FiveThirtyEight.com.Webpage: http://rudeboybert.rbind.io/GitHub: https://github.com/rudeboybertArturo Valdivia Senior Lecturer Department Statistics Indiana University, Bloomington.\nearned PhD Statistics Arizona State University 2013.\nresearch interests focus statistical education, exploring innovative approaches help students grasp complex ideas clarity.\ncareer, taught wide range statistics courses, introductory advanced levels, 1,800 undergraduate students 900 graduate students pursuing master’s Ph.D. programs statistics, data science, disciplines. recognition teaching excellence, received Indiana University’s Trustees Teaching Award 2023.Webpage: https://avaldivi6.github.ioGitHub: https://github.com/avaldivi6","code":""},{"path":"getting-started.html","id":"getting-started","chapter":"1 Getting Started with Data in R","heading":"1 Getting Started with Data in R","text":"can start exploring data R, key concepts understand first:R RStudio?code R?R packages?’ll introduce concepts upcoming Sections 1.1–1.3. already somewhat familiar concepts, feel free skip Section 1.4 ’ll introduce first dataset: domestic flights departing one three main New York City (NYC) airports 2023. explore dataset depth much rest book.","code":""},{"path":"getting-started.html","id":"r-rstudio","chapter":"1 Getting Started with Data in R","heading":"1.1 What are R and RStudio?","text":"Throughout book, assume using R via RStudio. First time users often confuse two. simplest, R like car’s engine RStudio like car’s dashboard illustrated Figure 1.1.\nFIGURE 1.1: Analogy difference R RStudio.\nprecisely, R programming language runs computations, whereas\nRStudio\nintegrated development environment (IDE) provides interface adding many convenient features tools. just way access speedometer, rear-view mirrors, navigation system makes driving much easier, using RStudio’s interface makes using R much easier well.","code":""},{"path":"getting-started.html","id":"installing","chapter":"1 Getting Started with Data in R","heading":"1.1.1 Installing R and RStudio","text":"Note RStudio Server Posit (formerly RStudio) Cloud: instructor provided link access RStudio Server Posit Cloud, can skip section. recommend months working RStudio Server/Posit Cloud return instructions install software computer though.first need download install R RStudio (Desktop version) computer. important install R first install RStudio.must first: Download install R going https://cloud.r-project.org/.\nWindows user: Click “Download R Windows,” click “base,” click Download link.\nmacOS user: Click “Download R macOS,” “Latest release:” click R-X.X.X.pkg, R-X.X.X version number. example, latest version R December 4, 2024 R-4.4.2.\nLinux user: Click “Download R Linux” choose distribution information installing R setup.\nWindows user: Click “Download R Windows,” click “base,” click Download link.macOS user: Click “Download R macOS,” “Latest release:” click R-X.X.X.pkg, R-X.X.X version number. example, latest version R December 4, 2024 R-4.4.2.Linux user: Click “Download R Linux” choose distribution information installing R setup.must second: Download install RStudio https://posit.co/download/rstudio-desktop/.\nScroll “Installers Tarballs” near bottom page.\nClick download link corresponding computer’s operating system. \nScroll “Installers Tarballs” near bottom page.Click download link corresponding computer’s operating system. ","code":""},{"path":"getting-started.html","id":"using-r-via-rstudio","chapter":"1 Getting Started with Data in R","heading":"1.1.2 Using R via RStudio","text":"Recall car analogy earlier. Much don’t drive car interacting directly engine rather interacting elements car’s dashboard, won’t using R directly rather use RStudio’s interface. install R RStudio computer, ’ll two new programs (also called applications) can open. ’ll always work RStudio R application. Figure 1.2 shows icon clicking computer.\nFIGURE 1.2: Icons R versus RStudio computer.\nopen RStudio, see something similar Figure 1.3. (Note slight differences might exist RStudio interface updated default.)\nFIGURE 1.3: RStudio interface R.\nNote three panes three panels dividing screen: console pane, files pane, environment pane. course chapter, ’ll come learn purpose panes serves.","code":""},{"path":"getting-started.html","id":"code","chapter":"1 Getting Started with Data in R","heading":"1.2 How do I code in R?","text":"Now ’re set R RStudio, probably asking , “OK. Now use R?”. first thing note unlike statistical software programs like Excel, SPSS, Minitab provide point--click interfaces, R interpreted language. means type commands written R code. words, code/program R. Note ’ll use terms “coding” “programming” interchangeably book.required seasoned coder/computer programmer use R, still set basic programming concepts new R users need understand. Consequently, book book programming, still learn just enough basic programming concepts needed explore analyze data effectively.","code":""},{"path":"getting-started.html","id":"programming-concepts","chapter":"1 Getting Started with Data in R","heading":"1.2.1 Basic programming concepts and terminology","text":"now introduce basic programming concepts terminology. Instead asking memorize concepts terminology right now, ’ll guide ’ll “learn .” help learn, always use different font distinguish regular text computer_code. best way master topics , opinions, deliberate practice R lots repetition.Basics:\nConsole pane: enter commands. \nRunning code: act telling R perform act giving commands console.\nObjects: values saved R. ’ll show assign values objects display contents objects. \nData types: integers, doubles/numerics, logicals, characters. Integers values like -1, 0, 2, 4092. Doubles numerics larger set values containing integers also fractions decimal values like -24.932 0.8. Logicals either TRUE FALSE characters text “cabbage,” “Hamilton,” “Wire greatest TV show ever,” “ramen delicious.” Note characters often denoted quotation marks around .\nConsole pane: enter commands. Running code: act telling R perform act giving commands console.Objects: values saved R. ’ll show assign values objects display contents objects. Data types: integers, doubles/numerics, logicals, characters. Integers values like -1, 0, 2, 4092. Doubles numerics larger set values containing integers also fractions decimal values like -24.932 0.8. Logicals either TRUE FALSE characters text “cabbage,” “Hamilton,” “Wire greatest TV show ever,” “ramen delicious.” Note characters often denoted quotation marks around .Vectors: series values. created using c() function, c() stands “combine” “concatenate.” example, c(6, 11, 13, 31, 90, 92) creates six element series positive integer values .Factors: categorical data commonly represented R factors. Categorical data can also represented strings. ’ll study difference progress book.Data frames: rectangular spreadsheets. representations datasets R rows correspond observations columns correspond variables describe observations. ’ll cover data frames later Section 1.4.Conditionals:\nTesting equality R using == (=, typically used assignment). example, 2 + 1 == 3 compares 2 + 1 3 correct R code, 2 + 1 = 3 return error.\nBoolean algebra: TRUE/FALSE statements mathematical operators < (less ), <= (less equal), != (equal ). example, 4 + 2 >= 3 return TRUE, 3 + 5 <= 1 return FALSE.\nLogical operators: & representing “” well | representing “.” example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since clauses TRUE (first clause TRUE). hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since least one two clauses TRUE. \nTesting equality R using == (=, typically used assignment). example, 2 + 1 == 3 compares 2 + 1 3 correct R code, 2 + 1 = 3 return error.Boolean algebra: TRUE/FALSE statements mathematical operators < (less ), <= (less equal), != (equal ). example, 4 + 2 >= 3 return TRUE, 3 + 5 <= 1 return FALSE.Logical operators: & representing “” well | representing “.” example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since clauses TRUE (first clause TRUE). hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since least one two clauses TRUE. Functions, also called commands: Functions perform tasks R. take inputs called arguments return outputs. can either manually specify function’s arguments use function’s default values.\nexample, function seq() R generates sequence numbers. just run seq() return value 1. doesn’t seem useful! default arguments set seq(= 1, = 1). Thus, don’t pass different values change behavior, R just assumes want number 1. can change argument values updating values = sign. try seq(= 2, = 5) get result 2 3 4 5 might expect.\n’ll work functions lot throughout book ’ll get lots practice understanding behaviors. assist understanding function mentioned book, ’ll also include () seq() .\nexample, function seq() R generates sequence numbers. just run seq() return value 1. doesn’t seem useful! default arguments set seq(= 1, = 1). Thus, don’t pass different values change behavior, R just assumes want number 1. can change argument values updating values = sign. try seq(= 2, = 5) get result 2 3 4 5 might expect.’ll work functions lot throughout book ’ll get lots practice understanding behaviors. assist understanding function mentioned book, ’ll also include () seq() .list means exhaustive list programming concepts terminology needed become savvy R user; list large wouldn’t useful, especially novices. Rather, feel minimally viable list programming concepts terminology need know getting started. feel can learn rest go. Remember mastery concepts terminology build practice .","code":""},{"path":"getting-started.html","id":"messages","chapter":"1 Getting Started with Data in R","heading":"1.2.2 Errors, warnings, and messages","text":"One thing intimidates new R RStudio users reports errors, warnings, messages. R reports errors, warnings, messages glaring red font, makes seem like scolding . However, seeing red text console always bad.R show red text console pane three different situations:Errors: red text legitimate error, prefaced “Error …” try explain went wrong. Generally ’s error, code run. example, ’ll see Subsection 1.3.3 see Error ggplot(...) : find function \"ggplot\", means ggplot() function accessible package contains function (ggplot2) loaded library(ggplot2). Thus use ggplot() function without ggplot2 package loaded first.Warnings: red text warning, prefaced “Warning:” R try explain ’s warning. Generally code still work, caveats. example, see Chapter 2 create scatterplot based dataset two rows data missing entries needed create points scatterplot, see warning: Warning: Removed 2 rows containing missing values (geom_point). R still produce scatterplot remaining non-missing values, warning two points aren’t .Messages: red text doesn’t start either “Error” “Warning,” ’s just friendly message. ’ll see messages load R packages upcoming Subsection 1.3.2 read data saved spreadsheet files read_csv() function ’ll see Chapter 4. helpful diagnostic messages don’t stop code working. Additionally, ’ll see messages install packages using install.packages() discussed Subsection 1.3.1.Remember, see red text console, don’t panic. doesn’t necessarily mean anything wrong. Rather:text starts “Error,” figure ’s causing . Think errors red traffic light: something wrong!text starts “Warning,” figure ’s something worry . instance, get warning missing values scatterplot know missing values, ’re fine. ’s surprising, look data see ’s missing. Think warnings yellow traffic light: everything working fine, watch /pay attention.Otherwise, text just message. Read , wave back R, thank talking . Think messages green traffic light: everything working fine keep going!","code":""},{"path":"getting-started.html","id":"tips-code","chapter":"1 Getting Started with Data in R","heading":"1.2.3 Tips on learning to code","text":"Learning code/program quite similar learning foreign language. can daunting frustrating first. frustrations common normal feel discouraged learn. However, just learning foreign language, put effort afraid make mistakes, anybody can learn improve.useful tips keep mind learn program:Remember computers actually smart: may think computer smartphone “smart,” really people spent lot time energy designing appear “smart.” reality, tell computer everything needs . Furthermore, instructions give computer can’t mistakes , can ambiguous way.Take “copy, paste, tweak” approach: Especially learn first programming language need understand particularly complicated code, often much easier take existing code know works modify suit ends. opposed trying type code scratch. call “copy, paste, tweak” approach. early , suggest trying write code memory, rather take existing examples provided , copy, paste, tweak suit goals. start feeling confident, can slowly move away approach write code scratch. Think “copy, paste, tweak” approach training wheels child learning ride bike. getting comfortable, won’t need anymore.best way learn code : Rather learning code sake, find learning code goes much smoother goal mind working particular project, like analyzing data interested important .Practice key: Just method improve foreign language skills lots practice speaking, method improving coding skills lots practice. Don’t worry, however, ’ll give plenty opportunities !","code":""},{"path":"getting-started.html","id":"packages","chapter":"1 Getting Started with Data in R","heading":"1.3 What are R packages?","text":"Another point confusion many new R users idea R package. R packages extend functionality R providing additional functions, data, documentation. written worldwide community R users can downloaded free internet.example, among many packages use book ggplot2 package (Wickham et al. 2025) data visualization Chapter 2, dplyr package (Wickham et al. 2023) data wrangling Chapter 3, moderndive package (Kim Ismay 2024) accompanies book, infer package (Bray et al. 2025) “tidy” transparent statistical inference Chapters 8, 9, 10.good analogy R packages like apps can download onto mobile phone like Figure 1.4:\nFIGURE 1.4: Analogy R versus R packages.\nR like new mobile phone: certain amount features use first time, doesn’t everything. R packages like apps can download onto phone Apple’s App Store Android’s Google Play.Let’s continue analogy considering Instagram app editing sharing pictures. Say purchased new phone like share photo just taken friends Instagram. need :Install app: Since phone new include Instagram app, need download app either App Store Google Play. ’re set time . might need future update app.Open app: ’ve installed Instagram, need open .Instagram open phone, can proceed share photo friends family. process similar using R package. need :Install package: like installing app phone. packages installed default install R RStudio. Thus want use package first time, need install first. ’ve installed package, likely won’t install unless want update newer version.“Load” package: “Loading” package like opening app phone. Packages “loaded” default start RStudio computer; need “load” package want use every time start RStudio.Let’s perform two steps ggplot2 package data visualization.","code":""},{"path":"getting-started.html","id":"package-installation","chapter":"1 Getting Started with Data in R","heading":"1.3.1 Package installation","text":"Note RStudio Server Posit Cloud: instructor provided link access RStudio Server Posit Cloud, might need install packages, might preinstalled instructor. said, still good idea know process later using RStudio Server Posit Cloud, rather RStudio Desktop computer.two ways install R package: easy way advanced way. Let’s install ggplot2 package easy way first shown Figure 1.5. Files pane RStudio:Click “Packages” tab.Click “Install” next Update.Type name package “Packages (separate multiple space comma):” case, type ggplot2.Click “Install.”\nFIGURE 1.5: Installing packages R easy way.\nalternative slightly less convenient way install package typing install.packages(\"ggplot2\") console pane RStudio pressing Return/Enter keyboard. Note must include quotation marks around name package.Much like app phone, install package . However, want update previously installed package newer version, need re-install repeating earlier steps.\nLearning check\n(LC1.1) Repeat earlier installation steps, dplyr, nycflights23, knitr packages. install earlier mentioned dplyr package data wrangling, nycflights23 package containing data domestic flights leaving New York City airport 2023, knitr package generating easy--read tables R. ’ll use packages next section.","code":""},{"path":"getting-started.html","id":"package-loading","chapter":"1 Getting Started with Data in R","heading":"1.3.2 Package loading","text":"Recall ’ve installed package, need “load .” words, need “open .” using library() command. example, load ggplot2 package, run following code console pane. mean “run following code”? Either type copy--paste following code console pane hit Enter key.running earlier code, blinking cursor returns next > “prompt” sign, means successful ggplot2 package now loaded ready use. , however, get red “error message” reads ... ... means didn’t successfully install . example “error message” discussed Subsection 1.2.2. get error message, go back Subsection 1.3.1 R package installation make sure install ggplot2 package proceeding.\nLearning check\n(LC1.2) “Load” dplyr, nycflights23, knitr packages well repeating earlier steps.","code":"\nlibrary(ggplot2)Error in library(ggplot2) : there is no package called ‘ggplot2’"},{"path":"getting-started.html","id":"package-use","chapter":"1 Getting Started with Data in R","heading":"1.3.3 Package use","text":"One common mistake new R users make wanting use particular packages forget “load” first using library() command just saw. Remember: load package want use every time start RStudio. don’t first “load” package, attempt use one features, ’ll see error message similar :different error message one just saw package installed yet. R telling trying use function package yet “loaded.” R doesn’t know find function using. Almost new users forget starting , little annoying get used . However, ’ll remember practice time become second nature .","code":"Error: could not find function"},{"path":"getting-started.html","id":"nycflights","chapter":"1 Getting Started with Data in R","heading":"1.4 Explore your first datasets","text":"Let’s put everything ’ve learned far practice start exploring real data! Data comes us variety formats, pictures text numbers. Throughout book, ’ll focus datasets saved “spreadsheet”-type format. probably common way data collected saved many fields. Remember Subsection 1.2.1 “spreadsheet”-type datasets called data frames R. ’ll focus working data saved data frames throughout book.Let’s first load packages needed chapter, assuming ’ve already installed . Read Section 1.3 information install load R packages haven’t already.beginning subsequent chapters book, ’ll always list packages installed loaded order work chapter’s R code.","code":"\nlibrary(nycflights23)\nlibrary(dplyr)\nlibrary(knitr)"},{"path":"getting-started.html","id":"nycflights23-package","chapter":"1 Getting Started with Data in R","heading":"1.4.1 nycflights23 package","text":"Many us flown airplanes know someone . Air travel become ever-present aspect many people’s lives. look Departures flight information board airport, frequently see flights delayed variety reasons. ways can understand reasons cause flight delays?’d like arrive destinations time whenever possible. (Unless secretly love hanging airports. one people, pretend moment much anticipating final destination.) Throughout book, ’re going analyze data related domestic flights departing one New York City’s three main airports 2023: Newark Liberty International (EWR), John F. Kennedy International (JFK), LaGuardia Airport (LGA). ’ll access data using nycflights23 R package, contains five datasets saved five data frames:flights: Information flights.airlines: table matching airline names two-letter International Air Transport Association (IATA) airline codes (also known carrier codes) 14 airline companies. example, “DL” two-letter code Delta.planes: Information 4,840 physical aircraft used.weather: Hourly meteorological data three NYC airports. data frame 26,207 rows, roughly corresponding \\(365 \\times 24 \\times 3 = 26,280\\) possible hourly measurements one can observe three locations course year.airports: Names, codes, locations 1,255 domestic destinations.nycflights23 package updated version classic nycflights13 R package. nycflights23 authored ModernDive co-author Chester Ismay using anyflights R package developed Simon Couch. Simon granted permission ModernDive team create nycflights23 submit package CRAN.","code":""},{"path":"getting-started.html","id":"flights-data-frame","chapter":"1 Getting Started with Data in R","heading":"1.4.2 flights data frame","text":"’ll begin exploring flights data frame get idea structure. Run following code console, either typing cutting--pasting . displays contents flights data frame console. Note depending size monitor, output may vary slightly.Let’s unpack output:tibble: 435,352 x 19: tibble specific kind data frame R. particular data frame \n435,352 rows corresponding different observations. , observation flight.\n19 columns corresponding 19 variables describing observation.\n435,352 rows corresponding different observations. , observation flight.19 columns corresponding 19 variables describing observation.year, month, day, dep_time, sched_dep_time, dep_delay, arr_time different columns, words, different variables dataset.preview first 10 rows observations corresponding first 10 flights. R showing first 10 rows, showed 435,352 rows, overwhelm screen.... 435,342 rows` 11 variables: indicating us 435,342 rows data 11 variables fit screen.Unfortunately, output allow us explore data well, give nice preview. Let’s look different ways explore data frames.","code":"\nflights# A tibble: 435,352 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>     <dbl> <chr>    <int> <chr>   <chr> \n 1  2023     1     1        1           2038       203      328              3       205 UA         628 N25201  EWR   \n 2  2023     1     1       18           2300        78      228            135        53 DL         393 N830DN  JFK   \n 3  2023     1     1       31           2344        47      500            426        34 B6         371 N807JB  JFK   \n 4  2023     1     1       33           2140       173      238           2352       166 B6        1053 N265JB  JFK   \n 5  2023     1     1       36           2048       228      223           2252       211 UA         219 N17730  EWR   \n 6  2023     1     1      503            500         3      808            815        -7 AA         499 N925AN  EWR   \n 7  2023     1     1      520            510        10      948            949        -1 B6         996 N2043J  JFK   \n 8  2023     1     1      524            530        -6      645            710       -25 AA         981 N918AN  EWR   \n 9  2023     1     1      537            520        17      926            818        68 UA         206 N13113  EWR   \n10  2023     1     1      547            545         2      845            852        -7 NK         225 N912NK  EWR   \n# ℹ 435,342 more rows\n# ℹ 6 more variables: dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>"},{"path":"getting-started.html","id":"exploredataframes","chapter":"1 Getting Started with Data in R","heading":"1.4.3 Exploring data frames","text":"many ways get feel data contained data frame flights. present three functions take “argument” (input) data frame fourth method exploring one column data frame:Using View() function, brings RStudio’s built-data viewer.Using glimpse() function, included dplyr package.Using kable() function, included knitr package.Using $ “extraction operator,” used view single variable.1. View():Run View(flights) console RStudio, either typing cutting--pasting console pane. Explore data frame resulting pop viewer. get habit viewing data frames encounter. Note uppercase V View(). R case-sensitive, ’ll get error message run view(flights) instead View(flights).\nLearning check\n(LC1.3) ONE row flights dataset refer ?. Data airlineB. Data flightC. Data airportD. Data multiple flightsBy running View(flights), can explore different variables listed columns. Observe many different types variables. variables like distance, day, arr_delay call quantitative variables. variables numerical nature. variables categorical.look leftmost column View(flights) output, ’ll see column numbers. row numbers dataset. Glancing across row number, say row 5, can get idea row represents. allows identify object described given row taking note values columns specific row. often called observational unit. observational unit example individual flight departing New York City 2023. can identify observational unit determining “thing” measured described variables. ’ll talk observational units Subsection 1.4.4 identification measurement variables.2. glimpse():second way ’ll cover explore data frame using glimpse() function included dplyr package. Thus, can use glimpse() function ’ve loaded dplyr package running library(dplyr). function provides us alternative perspective exploring data frame View() function:Observe glimpse() give first entries variable row variable name. addition, data type (see Subsection 1.2.1) variable given immediately variable’s name inside < >. , int dbl refer “integer” “double,” computer coding terminology quantitative/numerical variables. “Doubles” take twice size store computer compared integers.contrast, chr refers “character,” computer terminology text data. forms, text data, carrier origin flight, categorical variables. time_hour variable another data type: dttm. types variables represent date time combinations. However, won’t work dates times book; leave topic data science books like Data Science: First Introduction Tiffany-Anne Timbers, Melissa Lee, Trevor Campbell R Data Science (Grolemund Wickham 2017).\nLearning check\n(LC1.4) examples dataset categorical variables? makes different quantitative variables?3. kable():final way explore entirety data frame using kable() function knitr package. Let’s explore different carrier codes airlines dataset two ways. Run lines code console:first glance, may appear much difference outputs. However, using tools producing reproducible reports R Markdown, latter code produces output much legible reader-friendly. ’ll see us use reader-friendly style many places book want print data frame nice table.4. $ operatorLastly, $ operator allows us extract explore single variable within data frame. example, run following console:used $ operator extract name variable return vector length 16. ’ll occasionally exploring data frames using $ operator, instead favoring View() glimpse() functions.","code":"\nglimpse(flights)Rows: 435,352\nColumns: 19\n$ year           <int> 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023,…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ dep_time       <int> 1, 18, 31, 33, 36, 503, 520, 524, 537, 547, 549, 551, 552, 554, 554, 558, 600, 600, 600, 603, 6…\n$ sched_dep_time <int> 2038, 2300, 2344, 2140, 2048, 500, 510, 530, 520, 545, 559, 600, 559, 600, 600, 605, 600, 600, …\n$ dep_delay      <dbl> 203, 78, 47, 173, 228, 3, 10, -6, 17, 2, -10, -9, -7, -6, -6, -7, 0, 0, 0, -2, 5, 0, 41, -9, -2…\n$ arr_time       <int> 328, 228, 500, 238, 223, 808, 948, 645, 926, 845, 905, 846, 857, 914, 725, 719, 729, 745, 810, …\n$ sched_arr_time <int> 3, 135, 426, 2352, 2252, 815, 949, 710, 818, 852, 901, 859, 911, 920, 735, 750, 752, 755, 840, …\n$ arr_delay      <dbl> 205, 53, 34, 166, 211, -7, -1, -25, 68, -7, 4, -13, -14, -6, -10, -31, -23, -10, -30, -18, 11, …\n$ carrier        <chr> \"UA\", \"DL\", \"B6\", \"B6\", \"UA\", \"AA\", \"B6\", \"AA\", \"UA\", \"NK\", \"B6\", \"B6\", \"AA\", \"AA\", \"UA\", \"UA\",…\n$ flight         <int> 628, 393, 371, 1053, 219, 499, 996, 981, 206, 225, 800, 93, 518, 165, 445, 449, 1084, 234, 465,…\n$ tailnum        <chr> \"N25201\", \"N830DN\", \"N807JB\", \"N265JB\", \"N17730\", \"N925AN\", \"N2043J\", \"N918AN\", \"N13113\", \"N912…\n$ origin         <chr> \"EWR\", \"JFK\", \"JFK\", \"JFK\", \"EWR\", \"EWR\", \"JFK\", \"EWR\", \"EWR\", \"EWR\", \"JFK\", \"LGA\", \"LGA\", \"JFK…\n$ dest           <chr> \"SMF\", \"ATL\", \"BQN\", \"CHS\", \"DTW\", \"MIA\", \"BQN\", \"ORD\", \"IAH\", \"FLL\", \"PBI\", \"MCO\", \"MIA\", \"MIA…\n$ air_time       <dbl> 367, 108, 190, 108, 80, 154, 192, 119, 258, 157, 164, 143, 159, 169, 116, 118, 116, 84, 236, 15…\n$ distance       <dbl> 2500, 760, 1576, 636, 488, 1085, 1576, 719, 1400, 1065, 1028, 950, 1096, 1089, 748, 719, 733, 4…\n$ hour           <dbl> 20, 23, 23, 21, 20, 5, 5, 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 38, 0, 44, 40, 48, 0, 10, 30, 20, 45, 59, 0, 59, 0, 0, 5, 0, 0, 0, 5, 0, 5, 30, 20, 15, 0, 20, …\n$ time_hour      <dttm> 2023-01-01 20:00:00, 2023-01-01 23:00:00, 2023-01-01 23:00:00, 2023-01-01 21:00:00, 2023-01-01…\nairlines\nkable(airlines)\nairlines$name"},{"path":"getting-started.html","id":"identification-vs-measurement-variables","chapter":"1 Getting Started with Data in R","heading":"1.4.4 Identification and measurement variables","text":"subtle difference kinds variables encounter data frames. identification variables measurement variables. example, let’s explore airports data frame showing output glimpse(airports):variables faa name identification variables uniquely identify airport. faa provides airport’s unique FAA code, name gives official name. variables used uniquely identify row data frame. remaining variables (lat, lon, alt, tz, dst, tzone) often called measurement characteristic variables: variables describe properties observational unit. example, lat long describe latitude longitude airport.Furthermore, sometimes single variable might enough uniquely identify observational unit: combinations variables might needed. absolute rule, organizational purposes considered good practice identification variables leftmost columns data frame.\nLearning check\n(LC1.5) properties airport variables lat, lon, alt, tz, dst, tzone describe airports data frame? Take best guess.(LC1.6) Provide names variables data frame least three variables one identification variable two .","code":"\nglimpse(airports)Rows: 1,255\nColumns: 8\n$ faa   <chr> \"AAF\", \"AAP\", \"ABE\", \"ABI\", \"ABL\", \"ABQ\", \"ABR\", \"ABY\", \"ACK\", \"ACT\", \"ACV\", \"ACY\", \"ADK\", \"ADM\", \"ADQ\",…\n$ name  <chr> \"Apalachicola Regional Airport\", \"Andrau Airpark\", \"Lehigh Valley International Airport\", \"Abilene Regio…\n$ lat   <dbl> 29.7, 29.7, 40.7, 32.4, 67.1, 35.0, 45.4, 31.5, 41.3, 31.6, 41.0, 39.5, 51.9, 34.3, 57.8, 33.0, 34.8, 38…\n$ lon   <dbl> -85.0, -95.6, -75.4, -99.7, -157.9, -106.6, -98.4, -84.2, -70.1, -97.2, -124.1, -74.6, -176.6, -97.0, -1…\n$ alt   <dbl> 20, 79, 393, 1791, 334, 5355, 1302, 197, 47, 516, 221, 75, 18, 777, 78, 644, 1016, 280, 441, 89, 722, 12…\n$ tz    <dbl> -5, -6, -5, -6, -9, -7, -6, -5, -5, -6, -8, -5, -10, -6, -9, -6, -6, -5, -9, -6, -6, -5, -9, -5, -5, -7,…\n$ dst   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ tzone <chr> \"America/New_York\", \"America/Chicago\", \"America/New_York\", \"America/Chicago\", \"America/Anchorage\", \"Amer…"},{"path":"getting-started.html","id":"help-files","chapter":"1 Getting Started with Data in R","heading":"1.4.5 Help files","text":"Another nice feature R help files, provide documentation various functions datasets. can bring help files adding ? name function data frame run console. presented page showing corresponding documentation exists. example, let’s look help file flights data frame.help file pop Help pane RStudio. questions function data frame included R package, get habit consulting help file right away.\nLearning check\n(LC1.7) Look help file airports data frame. Revise earlier guesses variables lat, lon, alt, tz, dst, tzone describe.","code":"\n?flights"},{"path":"getting-started.html","id":"conclusion","chapter":"1 Getting Started with Data in R","heading":"1.5 Conclusion","text":"chapter provides small set tools explore data R, ’s far exhaustive. Including everything overwhelm rather help. best way add toolbox running writing code RStudio much possible.","code":""},{"path":"getting-started.html","id":"additional-resources","chapter":"1 Getting Started with Data in R","heading":"1.5.1 Additional resources","text":"new world coding, R, RStudio feel benefit detailed introduction, suggest short book, Getting Used R, RStudio, R Markdown (Ismay Kennedy 2024), previewed Figure 1.6. includes screencasts can follow along pause learn. book also contains introduction R Markdown, tool used reproducible research.\nFIGURE 1.6: Preview Getting Used R, RStudio, R Markdown.\n","code":""},{"path":"getting-started.html","id":"whats-to-come","chapter":"1 Getting Started with Data in R","heading":"1.5.2 What’s to come?","text":"’re next heading “Data Science tidyverse” portion Chapter 2 shown Figure 1.7 feel important tool data scientist’s toolbox: data visualization. ’ll continue explore data included moderndive nycflights23 packages using ggplot2 package data visualization. Data visualization powerful tool add toolbox data exploration provides additional insight View() glimpse() functions can provide.\nFIGURE 1.7: ModernDive flowchart – Part !\n","code":""},{"path":"viz.html","id":"viz","chapter":"2 Data Visualization","heading":"2 Data Visualization","text":"begin development data science toolbox data visualization. visualizing data, gain valuable insights couldn’t initially obtain just looking raw data values. ’ll use ggplot2 package, provides easy way customize plots. ggplot2 rooted data visualization theory known grammar graphics (Wilkinson 2005), developed Leland Wilkinson. basic, graphics/plots/charts (use terms interchangeably book) provide nice way explore patterns data, presence outliers, distributions individual variables, relationships groups variables. Graphics designed emphasize findings insights want audience understand. , however, require balancing act. one hand, want highlight many interesting findings possible. hand, don’t want include much information overwhelms audience.see, plots also help us identify patterns outliers data. ’ll see common extension ideas compare distribution one numerical variable, center spread values, go across levels different categorical variable.","code":""},{"path":"viz.html","id":"needed-packages","chapter":"2 Data Visualization","heading":"Needed packages","text":"Let’s load packages needed chapter (assumes ’ve already installed ). Read Section 1.3 information install load R packages.","code":"\nlibrary(nycflights23)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(tibble)"},{"path":"viz.html","id":"grammarofgraphics","chapter":"2 Data Visualization","heading":"2.1 The grammar of graphics","text":"start discussion theoretical framework data visualization known “grammar graphics.” framework serves foundation ggplot2 package ’ll use extensively chapter. Think construct form sentences English combining different elements, like nouns, verbs, articles, subjects, objects, etc. can’t just combine elements arbitrary order; must following set rules known linguistic grammar. Similarly linguistic grammar, “grammar graphics” defines set rules constructing statistical graphics combining different types layers. grammar created Leland Wilkinson (Wilkinson 2005) implemented variety data visualization software platforms like R, also Plotly Tableau.","code":""},{"path":"viz.html","id":"components-of-the-grammar","chapter":"2 Data Visualization","heading":"2.1.1 Components of the grammar","text":"short, grammar tells us :statistical graphic mapping data variables aesthetic attributes geometric objects.Specifically, can break graphic following three essential components:data: dataset containing variables interest.geom: geometric object question. refers type object can observe plot. example: points, lines, bars.aes: aesthetic attributes geometric object. example, x/y position, color, shape, size. Aesthetic attributes mapped variables dataset.might wondering wrote terms data, geom, aes computer code type font. ’ll see shortly ’ll specify elements grammar R using terms. However, let’s first break grammar example.","code":""},{"path":"viz.html","id":"gapminder","chapter":"2 Data Visualization","heading":"2.1.2 Gapminder data","text":"February 2006, Swedish physician data advocate named Hans Rosling gave TED talk titled “best stats ’ve ever seen” presented global economic, health, development data website gapminder.org. example, data 142 countries 2007, let’s consider countries Table 2.1 peek data.\nTABLE 2.1: Gapminder 2007 Data: First 3 142 countries\nrow table corresponds country 2007. row, 5 columns:Country: Name country.Continent: five continents country part . Note “Americas” includes countries North South America Antarctica excluded.Life Expectancy: Life expectancy years.Population: Number people living country.GDP per Capita: Gross domestic product (US dollars).Now consider Figure 2.1, plots 142 data’s countries.\nFIGURE 2.1: Life expectancy GDP per capita 2007.\nLet’s view plot grammar graphics:data variable GDP per Capita gets mapped x-position aesthetic points.data variable Life Expectancy gets mapped y-position aesthetic points.data variable Population gets mapped size aesthetic points.data variable Continent gets mapped color aesthetic points.’ll see shortly data corresponds particular data frame data saved “data variables” correspond particular columns data frame. Furthermore, type geometric object considered plot points. said, example considering points, graphics limited just points. can also use lines, bars, geometric objects.Let’s summarize three essential components grammar Table 2.2.\nTABLE 2.2: Summary grammar graphics plot\n","code":""},{"path":"viz.html","id":"other-components","chapter":"2 Data Visualization","heading":"2.1.3 Other components","text":"components grammar graphics can control well. start delve deeper grammar graphics, ’ll start encounter topics frequently. book, ’ll keep things simple work two additional components:faceting breaks plot several plots split values another variable (Section 2.6) position adjustments barplots (Section 2.8) complex components like scales coordinate systems left advanced text R Data Science (Grolemund Wickham 2017). Generally speaking, grammar graphics allows high degree customization plots also consistent framework easily updating modifying .","code":""},{"path":"viz.html","id":"ggplot2-package","chapter":"2 Data Visualization","heading":"2.1.4 ggplot2 package","text":"book, use ggplot2 package data visualization, implementation grammar graphics R (Wickham et al. 2025). noted earlier, lot previous section written computer code type font. various components grammar graphics specified ggplot() function included ggplot2 package. purposes book, ’ll always provide ggplot() function following arguments (.e., inputs) minimum:data frame variables exist: data argument.mapping variables aesthetic attributes: mapping argument specifies aesthetic attributes involved.’ve specified components, add layers plot using + sign. essential layer add plot layer specifies type geometric object want plot involve: points, lines, bars, others. layers can add plot include plot title, axes labels, visual themes plots, facets (’ll see Section 2.6).Let’s now put theory grammar graphics practice.","code":""},{"path":"viz.html","id":"FiveNG","chapter":"2 Data Visualization","heading":"2.2 Five named graphs – the 5NG","text":"order keep things simple book, focus five different types graphics, commonly given name. term “five named graphs” abbreviated form, 5NG: scatterplotslinegraphshistogramsboxplotsbarplotsWe’ll also present variations plots, basic repertoire five graphics toolbox, can visualize wide array different variable types. Note certain plots appropriate categorical variables, others appropriate numerical variables.","code":""},{"path":"viz.html","id":"scatterplots","chapter":"2 Data Visualization","heading":"2.3 5NG#1: Scatterplots","text":"simplest 5NG scatterplots, also called bivariate plots. allow visualize relationship two numerical variables. may already familiar scatterplots, let’s view lens grammar graphics presented Section 2.1. Specifically, visualize relationship following two numerical variables envoy_flights data frame included moderndive package:dep_delay: departure delay horizontal “x” axis andarr_delay: arrival delay vertical “y” axisfor Envoy Airlines flights leaving NYC 2023. words, envoy_flights consist flights left NYC 2023, rather flights carrier MQ (Envoy Airlines’ carrier code).\nLearning check\n(LC2.1) Take look flights data frame nycflights23 package envoy_flights data frame moderndive package running View(flights) View(envoy_flights). respect data frames differ? example, think number rows dataset.","code":""},{"path":"viz.html","id":"geompoint","chapter":"2 Data Visualization","heading":"2.3.1 Scatterplots via geom_point","text":"Let’s now go code create desired scatterplot, keeping mind grammar graphics framework introduced Section 2.1. Let’s take look code break piece--piece.Within ggplot() function, specify two components grammar graphics arguments (.e., inputs):data envoy_flights data frame via data = envoy_flights.aesthetic mapping setting mapping = aes(x = dep_delay, y = arr_delay). Specifically, variable dep_delay maps x position aesthetic, variable arr_delay maps y position.add layer ggplot() function call using + sign. added layer question specifies third component grammar: geometric object. case, geometric object set points specifying geom_point(). running two lines code console, ’ll notice two outputs: warning message graphic shown Figure 2.2.\nFIGURE 2.2: Arrival delays versus departure delays Envoy Air flights NYC 2023.\nLet’s first unpack graphic Figure 2.2. Observe positive relationship exists dep_delay arr_delay: departure delays increase, arrival delays tend also increase. Observe also large mass points clustered near (0, 0), point indicating flights neither departed arrived late.Let’s turn attention warning message. R alerting us fact three rows ignored due missing. three rows, either value dep_delay arr_delay missing (recorded R NA), thus rows ignored plot.continue, let’s make observations code created scatterplot. Note + sign comes end lines, beginning. ’ll get error R put beginning line. adding layers plot, encouraged start new line + (pressing Return/Enter button keyboard) code layer new line. add layers plots, ’ll see greatly improve legibility code.stress importance adding layer specifying geometric object, consider Figure 2.3 layers added. geometric object specified, blank plot useful!\nFIGURE 2.3: plot layers.\n\nLearning check\n(LC2.2) practical reasons dep_delay arr_delay positive relationship?(LC2.3) variables weather data frame expect negative correlation (.e., negative relationship) dep_delay? ? Remember focusing numerical variables . Hint: Explore weather dataset using View() function.(LC2.4) believe cluster points near (0, 0)? (0, 0) correspond terms Envoy Air flights?(LC2.5) features plot stand ?(LC2.6) Create new scatterplot using different variables envoy_flights data frame modifying example given.","code":"\nggplot(data = envoy_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()Warning: Removed 3 rows containing missing values or values outside the scale range \n(`geom_point()`).\nggplot(data = envoy_flights, mapping = aes(x = dep_delay, y = arr_delay))"},{"path":"viz.html","id":"overplotting","chapter":"2 Data Visualization","heading":"2.3.2 Overplotting","text":"large mass points near (0, 0) Figure 2.2 can cause confusion since hard tell true number points plotted. result phenomenon called overplotting. one may guess, corresponds points plotted top . overplotting occurs, difficult know number points plotted. two methods address issue overplotting. Either byAdjusting transparency points orAdding little random “jitter” (random “nudges”) points.Method 1: Changing transparencyThe first way addressing overplotting change transparency/opacity points setting alpha argument geom_point(). can change alpha argument value 0 1, 0 sets points 100% transparent 1 sets points 100% opaque. default, alpha set 1. words, don’t explicitly set alpha value, R use alpha = 1.Note following code identical code Section 2.3 created scatterplot overplotting, alpha = 0.2 added geom_point() function:\nFIGURE 2.4: Arrival vs. departure delays scatterplot alpha = 0.2.\nkey feature note Figure 2.4 transparency points cumulative: areas high-degree overplotting darker, whereas areas lower degree less dark. Note, furthermore, aes() surrounding alpha = 0.2. mapping variable aesthetic attribute, rather merely changing default setting alpha. fact, ’ll receive error try change second line read geom_point(aes(alpha = 0.2)).Method 2: Jittering pointsThe second way addressing overplotting jittering points. means giving point small “nudge” random direction. can think “jittering” shaking points around bit plot. Let’s illustrate using simple example first. Say data frame 4 identical rows x y values: (0,0), (0,0), (0,0), (0,0). Figure 2.5, present regular scatterplot 4 points (left) jittered counterpart (right).\nFIGURE 2.5: Regular jittered scatterplot.\nleft-hand regular scatterplot, observe 4 points superimposed top . know 4 values plotted, fact might apparent others. right-hand jittered scatterplot, now plainly evident plot involves four points since point given random “nudge.”Keep mind, however, jittering strictly visualization tool; even creating jittered scatterplot, original values saved data frame remain unchanged. create jittered scatterplot, instead using geom_point(), use geom_jitter(). Observe following code similar code created scatterplot overplotting Subsection 2.3.1, geom_point() replaced geom_jitter().\nFIGURE 2.6: Arrival versus departure delays jittered scatterplot.\norder specify much jitter add, adjusted width height arguments geom_jitter(). corresponds hard ’d like shake plot horizontal x-axis units vertical y-axis units, respectively. case, axes minutes. much jitter add using width height arguments? one hand, important add just enough jitter break overlap points, hand, much completely alter original pattern points.can seen resulting Figure 2.6, case jittering doesn’t really provide much new insight. particular case, can argued changing transparency points setting alpha proved effective. better use jittered scatterplot? better alter points’ transparency? single right answer applies situations. need make subjective choice choice. least confronted overplotting, however, suggest make types plots see one better emphasizes point trying make.\nLearning check\n(LC2.7) setting alpha argument value useful scatterplots? information give regular scatterplot ?(LC2.8) viewing Figure 2.4, give approximate range arrival delays departure delays occur frequently. region changed compared observed plot without alpha = 0.2 set Figure 2.2?","code":"\nggplot(data = envoy_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point(alpha = 0.2)\nggplot(data = envoy_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_jitter(width = 30, height = 30)"},{"path":"viz.html","id":"summary","chapter":"2 Data Visualization","heading":"2.3.3 Summary","text":"Scatterplots display relationship two numerical variables. among commonly used plots can provide immediate way see trend one numerical variable versus another. However, try create scatterplot either one two variables numerical, might get strange results. careful!medium large datasets, may need play around different modifications scatterplots saw changing transparency/opacity points jittering points. tweaking often fun part data visualization, since ’ll chance see different relationships emerge tinker plots.","code":""},{"path":"viz.html","id":"linegraphs","chapter":"2 Data Visualization","heading":"2.4 5NG#2: Linegraphs","text":"next five named graphs linegraphs. Linegraphs show relationship two numerical variables variable x-axis, also called explanatory variable, sequential nature. words, inherent ordering variable.common examples linegraphs notion time x-axis: hours, days, weeks, years, etc. Since time sequential, connect consecutive observations variable y-axis line. Linegraphs notion time x-axis also called time series plots. Let’s illustrate linegraphs using another dataset nycflights23 package: weather data frame.Let’s explore weather data frame nycflights23 package running View(weather) glimpse(weather). Furthermore, let’s read associated help file running ?weather bring help file.Observe variable called wind_speed hourly wind speed recordings miles per hour weather stations near three major airports New York City: Newark (origin code EWR), John F. Kennedy International (JFK), LaGuardia (LGA).However, instead considering hourly wind speeds days 2023 three airports, simplicity let’s consider hourly wind speeds Newark airport first 15 days January. data accessible early_january_2023_weather data frame included moderndive package. words, early_january_2023_weather contains hourly weather observations origin equal EWR (Newark’s airport code), month equal 1, day less equal 15.\nLearning check\n(LC2.9) Take look weather data frame nycflights23 package early_january_2023_weather data frame moderndive package running View(weather) View(early_january_2023_weather). respect data frames differ?(LC2.10) View() flights data frame . time_hour variable uniquely identify hour measurement, whereas hour variable ?","code":""},{"path":"viz.html","id":"geomline","chapter":"2 Data Visualization","heading":"2.4.1 Linegraphs via geom_line","text":"Let’s create time series plot (seen Figure 2.7) hourly wind speeds saved early_january_2023_weather data frame using geom_line() create linegraph, instead using geom_point() like used previously create scatterplots:\nFIGURE 2.7: Hourly wind speed Newark January 1-15, 2023.\nMuch ggplot() code created scatterplot departure arrival delays Envoy Air flights Figure 2.2, let’s break code piece--piece terms grammar graphics:Within ggplot() function call, specify two components grammar graphics arguments:data early_january_2023_weather data frame setting data = early_january_2023_weather.aesthetic mapping setting mapping = aes(x = time_hour, y = temp). Specifically, variable time_hour maps x position aesthetic, variable wind_speed maps y position aesthetic.add layer ggplot() function call using + sign. layer question specifies third component grammar: geometric object question. case, geometric object line set specifying geom_line().\nLearning check\n(LC2.11) linegraphs avoided clear ordering horizontal axis?(LC2.12) linegraphs frequently used time explanatory variable x-axis?(LC2.13) Plot time series variable wind_speed Newark Airport first 15 days January 2023. Try select variable doesn’t lot missing (NA) values.","code":"\nggplot(data = early_january_2023_weather, \n       mapping = aes(x = time_hour, y = wind_speed)) +\n  geom_line()"},{"path":"viz.html","id":"summary-1","chapter":"2 Data Visualization","heading":"2.4.2 Summary","text":"Linegraphs, just like scatterplots, display relationship two numerical variables. However, preferred use linegraphs scatterplots variable x-axis (.e., explanatory variable) inherent ordering, notion time.","code":""},{"path":"viz.html","id":"histograms","chapter":"2 Data Visualization","heading":"2.5 5NG#3: Histograms","text":"Let’s consider wind_speed variable weather data frame , unlike linegraphs Section 2.4, let’s say don’t care relationship time, rather care values wind_speed distribute. words:smallest largest values?“center” “typical” value?values spread ?frequent infrequent values?One way visualize distribution single variable wind_speed plot horizontal line Figure 2.8:\nFIGURE 2.8: Plot hourly wind speed recordings NYC 2023.\ngives us general idea values wind_speed distribute: observe wind speeds vary around 0 miles per hour (0 kilometers per hour ) 38 miles per hour (approximately 61 kilometers per hour). appear recorded wind speeds 0 20 miles per hour (mph) outside range. However, high degree overplotting points, ’s hard get sense exactly many values , say, 10 mph 15 mph.commonly produced instead Figure 2.8 known histogram. histogram plot visualizes distribution numerical value follows:first cut x-axis series bins, bin represents range values.bin, count number observations fall range corresponding bin.bin, draw bar whose height marks corresponding count.Let’s drill-example histogram, shown Figure 2.9.\nFIGURE 2.9: Example histogram.\nLet’s focus wind speeds 10 mph 25 mph now. Observe three bins equal width 10 mph 25 mph. Thus three bins width 5 mph : one bin 10-15 mph range, another bin 15-20 mph range, another bin 20-25 mph range. Since:bin 10-15 mph range height around 8000. words, around 8000 hourly wind speed recordings 10 mph 15 mph.bin 15-20 mph range height around 2400. words, around 2400 hourly wind speed recordings 15 mph 20 mph.bin 20-25 mph range height around 700. words, around 700 hourly wind speed recordings 20 mph 25 mph.eight bins spanning 0 mph 40 mph x-axis interpretation.","code":""},{"path":"viz.html","id":"geomhistogram","chapter":"2 Data Visualization","heading":"2.5.1 Histograms via geom_histogram","text":"Let’s now present ggplot() code plot first histogram! Unlike scatterplots linegraphs, now one variable mapped aes(): single numerical variable wind_speed. y-aesthetic histogram, count observations bin, gets computed automatically. Furthermore, geometric object layer now geom_histogram(). running following code, ’ll see histogram Figure 2.10 well warning messages. ’ll discuss warning messages first.\nFIGURE 2.10: Histogram hourly wind speeds three NYC airports.\nfirst message telling us histogram constructed using bins = 30 30 equally spaced bins. known computer programming default value; unless override default number bins number specify, R choose 30 default. ’ll see next section change number bins another value default.second warning message telling us something similar warning message received ran code create scatterplot departure arrival delays Envoy Air flights Figure 2.2: rows missing NA value wind_speed, omitted histogram. R just giving us friendly heads-case.Now let’s unpack resulting histogram Figure 2.10. Observe values 30 mph rather rare. However, large number bins, ’s hard get sense range wind speeds spanned bin; everything one giant amorphous blob. let’s add white vertical borders demarcating bins adding color = \"white\" argument geom_histogram() ignore warning setting number bins better value:\nFIGURE 2.11: Histogram hourly wind speeds three NYC airports white borders.\nnow easier time associating ranges wind speeds bins Figure 2.11. can also vary color bars setting fill argument. example, can set bin colors “blue steel” setting fill = \"steelblue\":’re curious, run colors() see 657 possible choice colors R!","code":"\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram()`stat_bin()` using `bins=30`. Pick better value with `binwidth`.Warning: Removed 1033 rows containing non-finite outside the scale range (`stat_bin()`).\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(color = \"white\")\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")"},{"path":"viz.html","id":"adjustbins","chapter":"2 Data Visualization","heading":"2.5.2 Adjusting the bins","text":"Observe Figure 2.11 10-20 mph range appear roughly 8 bins. Thus bin width 10 divided 8, 1.125 mph, easily interpretable range work . Let’s improve adjusting number bins histogram one two ways:adjusting number bins via bins argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().Using first method, power specify many bins like cut x-axis . mentioned previous section, default number bins 20. can override default, say 20 bins, follows:Using second method, instead specifying number bins, specify width bins using binwidth argument geom_histogram() layer. example, let’s set width bin five mph.compare resulting histograms side--side Figure 2.12.\nFIGURE 2.12: Setting histogram bins two ways.\n\nLearning check\n(LC2.14) changing number bins 30 20 tell us distribution wind speeds?(LC2.15) classify distribution wind speeds symmetric skewed one direction another?(LC2.16) guess “center” value distribution? make choice?(LC2.17) data spread greatly center close? ?","code":"\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(bins = 20, color = \"white\")\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(binwidth = 5, color = \"white\")"},{"path":"viz.html","id":"summary-2","chapter":"2 Data Visualization","heading":"2.5.3 Summary","text":"Histograms, unlike scatterplots linegraphs, present information single numerical variable. Specifically, visualizations distribution numerical variable question.","code":""},{"path":"viz.html","id":"facets","chapter":"2 Data Visualization","heading":"2.6 Facets","text":"continuing next 5NG, let’s briefly introduce new concept called faceting. Faceting used ’d like split particular visualization values another variable. create multiple copies type plot matching x y axes, whose content differ.example, suppose interested looking histogram hourly wind speed recordings three NYC airports saw Figure 2.9 differed month. “split” histogram 12 possible months given year. words, plot histograms wind_speed month separately. adding facet_wrap(~ month) layer. Note ~ “tilde” can generally found key next “1” key US keyboards. tilde required ’ll receive error Error .quoted(facets) : object 'month' found don’t include .\nFIGURE 2.13: Faceted histogram hourly wind speeds month.\ncan also specify number rows columns grid using nrow ncol arguments inside facet_wrap(). example, say like faceted histogram 4 rows instead 3. simply add nrow = 4 argument facet_wrap(~ month).\nFIGURE 2.14: Faceted histogram 4 instead 3 rows.\nObserve Figures 2.13 2.14 majority wind speed observations months clustered 0 20 mph, observations exceeding 30 mph.\nhistograms show similar shape across months, distributions similar largest count larger speed outliers, indicating lower wind speeds common higher wind speeds.\nLearning check\n(LC2.18) things notice faceted plot? faceted plot help us see relationships two variables?(LC2.19) numbers 1-12 correspond plot? 10, 20, 30?(LC2.20) types datasets faceted plots work well comparing relationships variables? Give example describing nature variables important characteristics.(LC2.21) wind_speed variable weather dataset lot variability? say ?","code":"\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month)\nggplot(data = weather, mapping = aes(x = wind_speed)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month, nrow = 4)"},{"path":"viz.html","id":"boxplots","chapter":"2 Data Visualization","heading":"2.7 5NG#4: Boxplots","text":"faceted histograms one type visualization used compare distribution numerical variable split values another variable, another type visualization achieves goal side--side boxplot. boxplot constructed information provided five-number summary numerical variable. keep things simple now, let’s consider 2057 recorded hourly wind speed recordings month April, represented jittered point Figure 2.15.\nFIGURE 2.15: April wind speeds represented jittered points.\n2057 observations following five-number summary:Minimum: 0 mphFirst quartile (25th percentile): 5.8 mphMedian (second quartile, 50th percentile): 9.2 mphThird quartile (75th percentile): 12.7 mphMaximum: 29.92 mphIn leftmost plot Figure 2.16, let’s mark 5 values dashed horizontal lines top 2057 points. middle plot Figure 2.16 let’s add boxplot. rightmost plot Figure 2.16, let’s remove points dashed horizontal lines clarity’s sake.\nFIGURE 2.16: Building boxplot April wind speeds.\nboxplot visually summarize 2057 points cutting wind speed recordings quartiles dashed lines, quartile contains roughly 2057 \\(\\div\\) 4 \\(\\approx\\) 514 observations. Thus25% points fall bottom edge box, first quartile 5.8 mph. words, 25% observations 5.8 mph.25% points fall bottom edge box solid middle line, median 9.2 mph. Thus, 25% observations 5.8 mph 9.2 mph 50% observations 9.2 mph.25% points fall solid middle line top edge box, third quartile 12.7 mph. follows 25% observations 9.2 mph 12.7 mph 75% observations 12.7 mph.25% points fall top edge box. words, 25% observations 12.7 mph.middle 50% points lie within interquartile range (IQR) first third quartile. Thus, IQR example 12.7 - 5.8 = 6.905 mph. interquartile range measures numerical variable’s spread.Furthermore, rightmost plot Figure 2.16, see whiskers boxplot. whiskers stick either end box way minimum maximum observed wind speeds 0 mph 29.92 mph, respectively. However, whiskers don’t always extend smallest largest observed values . fact extend 1.5 \\(\\times\\) interquartile range either end box, case April wind speeds, 1.5 \\(\\times\\) 6.905 mph = 10.357 mph either end box. observed values outside range get marked points called outliers, marked , ’ll discuss next section.","code":""},{"path":"viz.html","id":"geomboxplot","chapter":"2 Data Visualization","heading":"2.7.1 Boxplots via geom_boxplot","text":"Let’s now create side--side boxplot hourly wind speeds split 12 months previously faceted histograms. mapping month variable x-position aesthetic, wind_speed variable y-position aesthetic, adding geom_boxplot() layer:\nFIGURE 2.17: Invalid boxplot specification.\nObserve Figure 2.17 plot provide information wind speed separated month. first warning message tells us . says “continuous” (numerical variable) x-position aesthetic. Boxplots, however, require categorical variable mapped x-position aesthetic.can convert numerical variable month factor categorical variable using factor() function. applying factor(month), month goes just numerical values 1, 2, …, 12 associated ordering. ordering, ggplot() now knows work variable produce plot.\nFIGURE 2.18: Side--side boxplot wind speed split month.\nresulting Figure 2.18 shows 12 separate “box whiskers” plots similar rightmost plot Figure 2.16 April wind speeds. Thus different boxplots shown “side--side.”“box” portions visualization represent 1st quartile, median (2nd quartile), 3rd quartile.height box (value 3rd quartile minus value 1st quartile) interquartile range (IQR). measure spread middle 50% values, longer boxes indicating variability.“whisker” portions plots extend bottoms tops boxes represent points less 25th percentile greater 75th percentiles, respectively. ’re set extend \\(1.5 \\times IQR\\) units away either end boxes. say “” ends whiskers correspond observed wind speeds. length whiskers shows data outside middle 50% values vary, longer whiskers indicating variability.dots representing values falling outside whiskers called outliers. can thought anomalous (“---ordinary”) values.important keep mind definition outlier somewhat arbitrary absolute. case, defined length whiskers, \\(1.5 \\times IQR\\) units long boxplot. Looking side--side plot can see months February March higher median wind speeds evidenced higher solid lines middle boxes. can easily compare wind speeds across months drawing imaginary horizontal lines across plot. Furthermore, heights 12 boxes quantified interquartile ranges informative ; tell us variability, spread, wind speeds recorded given month.\nLearning check\n(LC2.22) dots top plot January correspond ? Explain might occurred January produce points.(LC2.23) months seem highest variability wind speed? reasons can give ?(LC2.24) looked distribution numerical variable wind_speed split numerical variable month converted using factor() function order make side--side boxplot. boxplot wind_speed split numerical variable pressure similarly converted categorical variable using factor() informative?(LC2.25) Boxplots provide simple way identify outliers. may outliers easier identify looking boxplot instead faceted histogram?","code":"\nggplot(data = weather, mapping = aes(x = month, y = wind_speed)) +\n  geom_boxplot()Warning message:\n1: Continuous x aesthetic -- did you forget aes(group=...)? \nggplot(data = weather, mapping = aes(x = factor(month), y = wind_speed)) +\n  geom_boxplot()"},{"path":"viz.html","id":"summary-3","chapter":"2 Data Visualization","heading":"2.7.2 Summary","text":"Side--side boxplots provide us way compare distribution numerical variable across multiple values another variable. One can see median falls across different groups comparing solid lines center boxes.study spread numerical variable within one boxes, look length box also far whiskers extend either end box. Outliers even easily identified looking boxplot looking histogram marked distinct points.","code":""},{"path":"viz.html","id":"geombar","chapter":"2 Data Visualization","heading":"2.8 5NG#5: Barplots","text":"histograms boxplots tools visualize distribution numerical variables. Another commonly desired task visualize distribution categorical variable. simpler task, simply counting different categories within categorical variable, also known levels categorical variable. Often best way visualize different counts, also known frequencies, barplots (also called barcharts).One complication, however, data represented. categorical variable interest “pre-counted” ? example, run following code manually creates two data frames representing collection fruit: 3 apples 2 oranges.see fruits fruits_counted data frames represent collection fruit. Whereas fruits just lists fruit individually…… fruits_counted variable count represent “pre-counted” values fruit.Depending categorical data represented, ’ll need add different geometric layer type ggplot() create barplot, now explore.","code":"\nfruits <- tibble(fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\"))\nfruits_counted <- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2))# A tibble: 5 × 1\n  fruit \n  <chr> \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange# A tibble: 2 × 2\n  fruit  number\n  <chr>   <dbl>\n1 apple       3\n2 orange      2"},{"path":"viz.html","id":"barplots-via-geom_bar-or-geom_col","chapter":"2 Data Visualization","heading":"2.8.1 Barplots via geom_bar or geom_col","text":"Let’s generate barplots using two different representations basket fruit: 3 apples 2 oranges. Using fruits data frame 5 fruits listed individually 5 rows, map fruit variable x-position aesthetic add geom_bar() layer:\nFIGURE 2.19: Barplot counts pre-counted.\nHowever, using fruits_counted data frame fruits “pre-counted,” map fruit variable x-position aesthetic. , also map count variable y-position aesthetic, add geom_col() layer instead.\nFIGURE 2.20: Barplot counts pre-counted.\nCompare barplots Figures 2.19 2.20. identical reflect counts five fruits. However, depending categorical data represented, either “pre-counted” , must add different geom layer. categorical variable whose distribution want visualizeIs pre-counted data frame, use geom_bar().pre-counted data frame, use geom_col() y-position aesthetic mapped variable counts.Let’s now go back flights data frame nycflights23 package visualize distribution categorical variable carrier. words, let’s visualize number domestic flights New York City airline company flew 2023. Recall Subsection 1.4.3 first explored flights data frame, saw row corresponds flight. words, flights data frame like fruits data frame fruits_counted data frame flights pre-counted carrier. Thus use geom_bar() instead geom_col() create barplot. Much like geom_histogram(), one variable aes() aesthetic mapping: variable carrier gets mapped x-position. difference though, histograms bars touch whereas bar graphs white space bars going left right.\nFIGURE 2.21: Number flights departing NYC 2023 airline using geom_bar().\nObserve Figure 2.21 Republic Airline (YX), United Airlines (UA), JetBlue Airways (B6) flights depart NYC 2023. don’t know airlines correspond carrier codes, run View(airlines) see directory airlines. example, AA American Airlines Inc. Alternatively, say data frame number flights carrier pre-counted Table 2.3.\nTABLE 2.3: Number flights pre-counted carrier\norder create barplot visualizing distribution categorical variable carrier case, now use geom_col() instead geom_bar(), additional y = number aesthetic mapping top x = carrier. resulting barplot identical Figure 2.21.\nLearning check\n(LC2.26) histograms inappropriate categorical variables?(LC2.27) difference histograms barplots?(LC2.28) many Alaska Air flights departed NYC 2023?(LC2.29) 7th highest airline departed flights NYC 2023? better present table get answer quickly?","code":"\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()"},{"path":"viz.html","id":"must-avoid-pie-charts","chapter":"2 Data Visualization","heading":"2.8.2 Must avoid pie charts!","text":"One common plots used visualize distribution categorical data pie chart. may seem harmless enough, pie charts actually present problem humans unable judge angles well.Naomi Robbins describes book, Creating Effective Graphs (Robbins 2013), overestimate angles greater 90 degrees underestimate angles less 90 degrees. words, difficult us determine relative size one piece pie compared another.Let’s examine data used previous barplot number flights departing NYC airline Figure 2.21, time use pie chart Figure 2.22. Try answer following questions:much smaller portion pie Hawaiian Airlines Inc. (HA) compared United Airlines (UA)?third largest carrier terms departing flights?many carriers fewer flights Delta Air Lines Inc. (DL)?\nFIGURE 2.22: dreaded pie chart.\nquite difficult answer questions looking pie chart Figure 2.22, can much easily answer questions using barchart Figure 2.21. true since barplots present information way comparisons categories can made single horizontal lines, whereas pie charts present information way comparisons must made comparing angles.\nLearning check\n(LC2.30) pie charts avoided replaced barplots?(LC2.31) think people continue use pie charts?","code":""},{"path":"viz.html","id":"two-categ-barplot","chapter":"2 Data Visualization","heading":"2.8.3 Two categorical variables","text":"Barplots common way visualize frequency different categories, levels, single categorical variable. Another use barplots visualize joint distribution two categorical variables time.Let’s examine joint distribution outgoing domestic flights NYC carrier well origin, words, number flights carrier origin combination. corresponds number American Airlines flights JFK, number American Airlines flights LGA, number American Airlines flights EWR, number Endeavor Air flights JFK, . Recall ggplot() code created barplot carrier frequency Figure 2.21:can now map additional variable origin adding fill = origin inside aes() aesthetic mapping.\nFIGURE 2.23: Stacked barplot flight amount carrier origin.\nFigure 2.23 example stacked barplot. simple make, certain aspects ideal. example, difficult compare heights different colors bars, corresponding comparing number flights origin airport carriers.continue, let’s address common points confusion among new R users. First, fill aesthetic corresponds color used fill bars, color aesthetic corresponds color outline bars. identical added color histogram Subsection 2.5.1: set outline bars white setting color = \"white\" colors bars blue steel setting fill = \"steelblue\". Observe Figure 2.24 mapping origin color fill yields grey bars different colored outlines.\nFIGURE 2.24: Stacked barplot color aesthetic used instead fill.\nSecond, note fill another aesthetic mapping much like x-position; thus careful include within parentheses aes() mapping. following code, fill aesthetic specified outside aes() mapping yield error. fairly common error new ggplot users make:alternative stacked barplots side--side barplots, also known dodged barplots, seen Figure 2.25. code create side--side barplot identical code create stacked barplot, position = \"dodge\" argument added geom_bar(). words, overriding default barplot type, stacked barplot, specifying side--side barplot instead.\nFIGURE 2.25: Side--side barplot comparing number flights carrier origin.\nLastly, another type barplot faceted barplot. Recall Section 2.6 visualized distribution hourly wind speeds 3 NYC airports split month using facets. apply principle barplot visualizing frequency carrier split origin. Instead mapping origin fill include variable create small multiples plot across levels origin Figure 2.26.\nFIGURE 2.26: Faceted barplot comparing number flights carrier origin.\n\nLearning check\n(LC2.32) kinds questions easily answered looking Figure 2.23?(LC2.33) can say, anything, relationship airline airport NYC 2023 regard number departing flights?(LC2.34) might side--side barplot preferable stacked barplot case?(LC2.35) disadvantages using dodged barplot, general?(LC2.36) faceted barplot preferred side--side stacked barplots case?(LC2.37) information different carriers different airports easily seen faceted barplot?","code":"\nggplot(data = flights, mapping = aes(x = carrier)) + \n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, color = origin)) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier), fill = origin) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = \"dodge\")\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)"},{"path":"viz.html","id":"summary-4","chapter":"2 Data Visualization","heading":"2.8.4 Summary","text":"Barplots common way displaying distribution categorical variable, words frequency different categories (also called levels) occur. easy understand make easy make comparisons across levels. Furthermore, trying visualize relationship two categorical variables, many options: stacked barplots, side--side barplots, faceted barplots. Depending aspect relationship trying emphasize, need make choice three types barplots choice.","code":""},{"path":"viz.html","id":"data-vis-conclusion","chapter":"2 Data Visualization","heading":"2.9 Conclusion","text":"","code":""},{"path":"viz.html","id":"summary-table","chapter":"2 Data Visualization","heading":"2.9.1 Summary table","text":"Let’s recap five five named graphs (5NG) Table 2.4 summarizing differences. Using 5NG, ’ll able visualize distributions relationships variables contained wide array datasets. even case start map variables geometric object’s aesthetic attribute options, unlocking awesome power ggplot2 package.\nTABLE 2.4: Summary Five Named Graphs\n","code":""},{"path":"viz.html","id":"function-argument-specification","chapter":"2 Data Visualization","heading":"2.9.2 Function argument specification","text":"Let’s go important points specifying arguments (.e., inputs) functions. Run following two segments code:’ll notice code segments create barplot, even though second segment omitted data = mapping = code argument names. ggplot() function default assumes data argument comes first mapping argument comes second. long specify data frame question first aes() mapping second, can omit explicit statement argument names data = mapping =.Going forward rest book, ggplot() code like second segment: data = mapping = explicit naming argument omitted default ordering arguments respected. ’ll brevity’s sake; ’s common see style reviewing R users’ code.","code":"\n# Segment 1:\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\n# Segment 2:\nggplot(flights, aes(x = carrier)) +\n  geom_bar()"},{"path":"viz.html","id":"additional-resources-1","chapter":"2 Data Visualization","heading":"2.9.3 Additional resources","text":"R script file R code used chapter available .want unlock power ggplot2 package data visualization, suggest check RStudio’s “Data Visualization ggplot2” cheatsheet. cheatsheet summarizes much ’ve discussed chapter. particular, presents many 5 geometric objects covered chapter providing quick easy--read visual descriptions. geometric objects, also lists possible aesthetic attributes one can tweak. current version RStudio mid-2025, can access cheatsheet going RStudio Menu Bar -> Help -> Cheatsheets -> “Data Visualization ggplot2.” can see preview figure . Alternatively, can download cheat sheet going Posit Cheatsheets page link.\nFIGURE 2.27: Data Visualization ggplot2 cheatsheet (first page).\n","code":""},{"path":"viz.html","id":"whats-to-come-3","chapter":"2 Data Visualization","heading":"2.9.4 What’s to come","text":"Recall Figure 2.2 Section 2.3 visualized relationship departure delay arrival delay Envoy Air flights , rather flights. data saved envoy_flights data frame moderndive package.reality, envoy_flights data frame merely subset flights data frame nycflights23 package consisting flights left NYC 2023. created envoy_flights using following code uses dplyr package data wrangling:code takes flights data frame filter() return 357 rows carrier equal \"MQ\", Envoy Air’s carrier code. (Recall Section 1.2 testing equality specified == =.) code cycles back save output new data frame called envoy_flights using <- assignment operator.Similarly, recall Figure 2.7 Section 2.4 visualized hourly wind speed recordings Newark airport first 15 days January 2023. data saved early_january_2023_weather data frame moderndive package.reality, early_january_2023_weather data frame merely subset weather data frame nycflights23 package consisting hourly weather observations 2023 three NYC airports. created early_january_2023_weather using following dplyr code:code pares weather data frame new data frame early_january_2023_weather consisting hourly wind speed recordings origin == \"EWR\", month == 1, day less equal 15.two code segments preview Chapter 3 data wrangling using dplyr package. Data wrangling process transforming modifying existing data intent making appropriate analysis purposes. example, two code segments used filter() function create new data frames (envoy_flights early_january_2023_weather) choosing subset rows existing data frames (flights weather). next chapter, ’ll formally introduce filter() data-wrangling functions well pipe operator |> allows combine multiple data-wrangling actions single sequential chain actions. Chapter 3 data wrangling!","code":"\nlibrary(dplyr)\n\nenvoy_flights <- flights |> \n  filter(carrier == \"MQ\")\n\nggplot(data = envoy_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()\nearly_january_2023_weather <- weather |> \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\n\nggplot(data = early_january_2023_weather, mapping = aes(x = time_hour, y = temp)) +\n  geom_line()"},{"path":"wrangling.html","id":"wrangling","chapter":"3 Data Wrangling","heading":"3 Data Wrangling","text":"far journey, ’ve seen look data saved data frames using glimpse() View() functions Chapter 1, create data visualizations using ggplot2 package Chapter 2. particular, studied term “five named graphs” (5NG):scatterplots via geom_point()linegraphs via geom_line()boxplots via geom_boxplot()histograms via geom_histogram()barplots via geom_bar() geom_col()created visualizations using grammar graphics, maps variables data frame aesthetic attributes one 5 geometric objects. can also control aesthetic attributes geometric objects size color seen Gapminder data example Figure 2.1.chapter, ’ll introduce series functions dplyr package data wrangling allow take data frame “wrangle” (transform ) suit needs. functions include:filter() data frame’s existing rows pick subset . example, alaska_flights data frame.summarize() one columns/variables summary statistic. Examples summary statistics include median interquartile range temperatures saw Section 2.7 boxplots.group_by() rows. words, assign different rows part group. can combine group_by() summarize() report summary statistics group separately. example, say don’t want single overall average departure delay dep_delay three origin airports combined, rather three separate average departure delays, one computed three origin airports.mutate() existing columns/variables create new ones. example, convert hourly temperature readings Fahrenheit Celsius.arrange() rows. example, sort rows weather ascending descending order temp.join() another data frame matching along “key” variable. words, merge two data frames together.Notice used computer_code font describe actions want take data frames. dplyr package data wrangling intuitively verb-named functions easy remember.benefit learning use dplyr package data wrangling: similarity database querying language SQL (pronounced “sequel” spelled “S-Q-L”). SQL (stands “Structured Query Language”) used manage large databases quickly efficiently widely used many institutions lot data. SQL topic left book course database management, keep mind learn dplyr, can learn SQL easily. ’ll talk similarities Subsection 3.7.4.","code":""},{"path":"wrangling.html","id":"wrangling-packages","chapter":"3 Data Wrangling","heading":"Needed packages","text":"Let’s load packages needed chapter (assumes ’ve already installed ). needed, read Section 1.3 information install load R packages.","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(nycflights23)"},{"path":"wrangling.html","id":"piping","chapter":"3 Data Wrangling","heading":"3.1 The pipe operator: |>","text":"start data wrangling, let’s first introduce nifty tool part R since May 2021: native pipe operator |>. pipe operator allows us combine multiple operations R single sequential chain actions. modern R, native pipe operator |> now default chaining functions, replacing previously common tidyverse pipe (%>%) loaded dplyr package. Introduced R 4.1.0 May 2021, |> offers intuitive readable syntax data wrangling tasks, eliminating need additional package dependencies.’ll still often see R code using %>% older scripts searches online, ’ll use |> book. tidyverse pipe still works, don’t worry see code.Let’s start hypothetical example. Say like perform hypothetical sequence operations hypothetical data frame x using hypothetical functions f(), g(), h():Take x thenUse x input function f() thenUse output f(x) input function g() thenUse output g(f(x)) input function h()One way achieve sequence operations using nesting parentheses follows:code isn’t hard read since applying three functions: f(), g(), h() functions short name. , functions also one argument. However, can imagine get progressively harder read number functions applied sequence increases arguments function increase well. pipe operator |> comes handy. |> takes output one function “pipes” input next function. Furthermore, helpful trick read |> “” “.” example, can obtain output hypothetical sequence functions follows:read sequence :Take x thenUse output input next function f() thenUse output input next function g() thenUse output input next function h()approaches achieve goal, latter much human-readable can clearly read sequence operations line--line. hypothetical x, f(), g(), h()? Throughout chapter data wrangling:starting value x data frame. example, flights data frame explored Section 1.4.sequence functions, f(), g(), h(), mostly sequence number six data-wrangling verb-named functions listed introduction chapter. example, filter(carrier == \"MQ\") function argument specified previewed earlier.result transformed/modified data frame want. example, ’ll save result new data frame using <- assignment operator name alaska_flights via alaska_flights <-.Much like adding layers ggplot() using + sign, form single chain data wrangling operations combining verb-named functions single sequence using pipe operator |>. Furthermore, much like + sign come end lines constructing plots, pipe operator |> come end lines well.Keep mind, many advanced data-wrangling functions just six listed introduction chapter; ’ll see examples Section 3.8. However, just six verb-named functions ’ll able perform broad array data-wrangling tasks rest book.","code":"\nh(g(f(x)))\nx |> \n  f() |> \n  g() |> \n  h()\nenvoy_flights <- flights |> \n  filter(carrier == \"AS\")"},{"path":"wrangling.html","id":"filter","chapter":"3 Data Wrangling","heading":"3.2 filter rows","text":"\nFIGURE 3.1: Diagram filter() rows operation.\nfilter() function works much like “Filter” option Microsoft Excel; allows specify criteria values variable dataset filters rows match criteria.begin focusing flights New York City Phoenix, Arizona. dest destination code (airport code) Phoenix, Arizona \"PHX\". Run following look results RStudio’s spreadsheet viewer ensure flights heading Phoenix chosen :Note order code. First, take flights data frame flights filter() data frame dest equals \"PHX\" included. test equality using double equal sign == single equal sign =. words, filter(dest = \"PHX\") yield error. convention across many programming languages. new coding, ’ll probably forget use double equal sign == times get hang .can use operators beyond just == operator tests equality:> corresponds “greater ”< corresponds “less ”>= corresponds “greater equal ”<= corresponds “less equal ”!= corresponds “equal .” ! used many programming languages indicate “.”Furthermore, can combine multiple criteria using operators make comparisons:| corresponds “”& corresponds “”see many action, let’s filter flights rows departed JFK heading Burlington, Vermont (\"BTV\") Seattle, Washington (\"SEA\") departed months October, November, December. Run following:Note even though colloquially speaking one might say “flights leaving Burlington, Vermont Seattle, Washington,” terms computer operations, really mean “flights leaving Burlington, Vermont leaving Seattle, Washington.” given row data, dest can \"BTV\", \"SEA\", something else, \"BTV\" \"SEA\" time. Furthermore, note careful use parentheses around dest == \"BTV\" | dest == \"SEA\".can often skip use & just separate conditions comma. previous code return identical output btv_sea_flights_fall following code:Let’s present another example uses ! “” operator pick rows don’t match criteria. mentioned earlier, ! can read “.” filtering rows corresponding flights didn’t go Burlington, VT Seattle, WA., note careful use parentheses around (dest == \"BTV\" | dest == \"SEA\"). didn’t use parentheses follows:returning flights headed \"BTV\" headed \"SEA\", entirely different resulting data frame.Now say larger number airports want filter , say \"SEA\", \"SFO\", \"PHX\", \"BTV\", \"BDL\". continue use | () operator.progressively include airports, get unwieldy write. slightly shorter approach uses %% operator along c() function. Recall Subsection 1.2.1 c() function “combines” “concatenates” values single vector values. code filtering flights flights dest vector airports c(\"BTV\", \"SEA\", \"PHX\", \"SFO\", \"BDL\"). outputs many_airports , can see latter takes much less energy code. %% operator useful looking matches commonly one vector/variable compared another.final note, recommend filter() often among first verbs consider applying data. cleans dataset rows care , put differently, narrows scope data frame just observations care .\nLearning check\n(LC3.1) ’s another way using “” operator ! filter rows going Burlington, VT Seattle, WA flights data frame? Test using previous code.","code":"\nphoenix_flights <- flights |> \n  filter(dest == \"PHX\")\nView(phoenix_flights)\nbtv_sea_flights_fall <- flights |> \n  filter(origin == \"JFK\" & (dest == \"BTV\" | dest == \"SEA\") & month >= 10)\nView(btv_sea_flights_fall)\nbtv_sea_flights_fall <- flights |> \n  filter(origin == \"JFK\", (dest == \"BTV\" | dest == \"SEA\"), month >= 10)\nView(btv_sea_flights_fall)\nnot_BTV_SEA <- flights |> \n  filter(!(dest == \"BTV\" | dest == \"SEA\"))\nView(not_BTV_SEA)\nflights |> filter(!dest == \"BTV\" | dest == \"SEA\")\nmany_airports <- flights |> \n  filter(dest == \"SEA\" | dest == \"SFO\" | dest == \"PHX\" | \n         dest == \"BTV\" | dest == \"BDL\")\nmany_airports <- flights |> \n  filter(dest %in% c(\"SEA\", \"SFO\", \"PHX\", \"BTV\", \"BDL\"))\nView(many_airports)"},{"path":"wrangling.html","id":"summarize","chapter":"3 Data Wrangling","heading":"3.3 summarize variables","text":"next common task working data frames compute summary statistics. Summary statistics single numerical values summarize large number values. Commonly known examples summary statistics include mean (also called average) median (middle value). examples summary statistics might immediately come mind include sum, smallest value also called minimum, largest value also called maximum, standard deviation.\nSee Appendix online glossary summary statistics.Let’s calculate two summary statistics wind_speed temperature variable weather data frame: mean standard deviation (recall Section 1.4 weather data frame included nycflights23 package). compute summary statistics, need mean() sd() summary functions R. Summary functions R take many values return single value, shown Figure 3.2.\nFIGURE 3.2: Diagram illustrating summary() function R.\nprecisely, ’ll use mean() sd() summary functions within summarize() function dplyr package. Note can also use British English spelling summarise(). shown Figure 3.3, summarize() function takes data frame returns data frame one row corresponding summary statistics.\nFIGURE 3.3: Diagram summarize() rows.\n’ll save results new data frame called summary_windspeed two columns/variables: mean std_dev:values returned NA? NA R encodes missing values NA indicates “available” “applicable.” value particular row particular column exist, NA stored instead. Values can missing many reasons. Perhaps data collected someone forgot enter . Perhaps data collected difficult . Perhaps erroneous value someone entered corrected read missing. ’ll often encounter issues missing values working real data.Going back summary_windspeed output, default time try calculate summary statistic variable one NA missing values R, NA returned. work around fact, can set na.rm argument TRUE, rm short “remove”; ignore NA missing values return summary value non-missing values.code follows computes mean standard deviation non-missing values temp:Notice na.rm = TRUE used arguments mean() sd() summary functions individually, summarize() function.However, one needs cautious whenever ignoring missing values ’ve just done. upcoming Learning checks questions, ’ll consider possible ramifications blindly sweeping rows missing values “rug.” fact na.rm argument summary statistic function R set FALSE default. words, R ignore rows missing values default. R alerting presence missing data mindful missingness potential causes missingness throughout analysis.summary functions can use inside summarize() verb compute summary statistics? seen diagram Figure 3.2, can use function R takes many values returns just one. just :mean(): averagesd(): standard deviation, measure spreadmin() max(): minimum maximum values, respectivelyIQR(): interquartile rangesum(): total amount adding multiple numbersn(): count number rows group. particular summary function make sense group_by() covered Section 3.4.\nLearning check\n(LC3.2) Say doctor studying effect smoking lung cancer large number patients records measured five-year intervals. notices large number patients missing data points patient died, chooses ignore patients analysis. wrong doctor’s approach?(LC3.3) Modify earlier summarize() function code creates summary_windspeed data frame also use n() summary function: summarize(... , count = n()). returned value correspond ?(LC3.4) doesn’t following code work? Run code line--line instead , look data. words, select run summary_windspeed <- weather |> summarize(mean = mean(wind_speed, na.rm = TRUE)) first.","code":"\nsummary_windspeed <- weather |> \n  summarize(mean = mean(wind_speed), std_dev = sd(wind_speed))\nsummary_windspeed# A tibble: 1 × 2\n   mean std_dev\n  <dbl>   <dbl>\n1    NA      NA\nsummary_windspeed <- weather |> \n  summarize(mean = mean(wind_speed, na.rm = TRUE), \n            std_dev = sd(wind_speed, na.rm = TRUE))\nsummary_windspeed# A tibble: 1 × 2\n   mean std_dev\n  <dbl>   <dbl>\n1  9.43    5.27\nsummary_windspeed <- weather |>   \n  summarize(mean = mean(wind_speed, na.rm = TRUE)) |> \n  summarize(std_dev = sd(wind_speed, na.rm = TRUE))"},{"path":"wrangling.html","id":"groupby","chapter":"3 Data Wrangling","heading":"3.4 group_by rows","text":"\nFIGURE 3.4: Diagram group_by() summarize().\ncan modify code look average wind speed spread instead wind speed , keeping na.rm = TRUE set just case missing values stored temp column:Say instead single mean wind speed whole year, like 12 mean temperatures, one 12 months separately. words, like compute mean wind speed split month shown via generic diagram Figure 3.4. can “grouping” temperature observations values another variable, case 12 values variable month:code identical previous code created summary_windspeed, extra group_by(month) added summarize(). Grouping weather dataset month applying summarize() functions yields data frame displays mean standard deviation wind speed split 12 months year.important note group_by() function doesn’t change data frames . Rather changes meta-data, data data, specifically grouping structure. applying summarize() function data frame change.another example, consider diamonds data frame included ggplot2 package:Observe first line output reads # tibble: 53,940 x 10. example meta-data, case number observations/rows variables/columns diamonds. actual data subsequent table values. Now let’s pipe diamonds data frame group_by(cut):Observe now additional meta-data: # Groups: cut [5] indicating grouping structure meta-data set based 5 possible levels categorical variable cut: \"Fair\", \"Good\", \"Good\", \"Premium\", \"Ideal\". hand, observe data changed: still table 53,940 \\(\\times\\) 10 values. combining group_by() another data-wrangling operation, case summarize(), data actually transformed.like remove grouping structure meta-data, can pipe resulting data frame ungroup() function:Observe # Groups: cut [5] meta-data longer present.Let’s now revisit n() counting summary function briefly introduced previously. Recall n() function counts rows. opposed sum() summary function returns sum numerical variable. example, suppose ’d like count many flights departed three airports New York City:see LaGuardia (\"LGA\") flights departing 2023 followed Newark (\"EWR\") lastly \"JFK\". Note subtle important difference sum() n(); sum() returns sum numerical variable, n() returns count number rows/observations.","code":"\nsummary_temp <- weather |> \n  summarize(mean = mean(wind_speed, na.rm = TRUE), \n            std_dev = sd(wind_speed, na.rm = TRUE))\nsummary_temp# A tibble: 1 × 2\n   mean std_dev\n  <dbl>   <dbl>\n1  9.43    5.27\nsummary_monthly_windspeed <- weather |> \n  group_by(month) |> \n  summarize(mean = mean(wind_speed, na.rm = TRUE), \n            std_dev = sd(wind_speed, na.rm = TRUE))\nsummary_monthly_windspeed# A tibble: 12 × 3\n   month  mean std_dev\n   <int> <dbl>   <dbl>\n 1     1 10.3     6.01\n 2     2 10.9     6.60\n 3     3 12.4     6.36\n 4     4 10.0     5.02\n 5     5  8.88    4.45\n 6     6  8.52    4.42\n 7     7  7.96    4.36\n 8     8  8.83    4.34\n 9     9  8.92    4.66\n10    10  8.21    4.71\n11    11  9.48    4.81\n12    12  8.77    5.03\ndiamonds# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\ndiamonds |> \n  group_by(cut)# A tibble: 53,940 × 10\n# Groups:   cut [5]\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\ndiamonds |> \n  group_by(cut) |> \n  summarize(avg_price = mean(price))# A tibble: 5 × 2\n  cut       avg_price\n  <ord>         <dbl>\n1 Fair          4359.\n2 Good          3929.\n3 Very Good     3982.\n4 Premium       4584.\n5 Ideal         3458.\ndiamonds |> \n  group_by(cut) |> \n  ungroup()# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\nby_origin <- flights |> \n  group_by(origin) |> \n  summarize(count = n())\nby_origin# A tibble: 3 × 2\n  origin  count\n  <chr>   <int>\n1 EWR    138578\n2 JFK    133048\n3 LGA    163726"},{"path":"wrangling.html","id":"grouping-by-more-than-one-variable","chapter":"3 Data Wrangling","heading":"Grouping by more than one variable","text":"limited grouping one variable. Say want know number flights leaving three New York City airports month. can also group second variable month using group_by(origin, month):Note additional message appears specifying grouping done. .groups argument summarize() four options: drop_last, drop, keep, rowwise:drop_last drops last grouping variable,drop drops grouping variables,keep keeps grouping variables, androwwise turns row group.circumstances, default drop_last drops last grouping variable. message informing us default behavior drop last grouping variable, case month.Observe 36 rows by_origin_monthly 12 months 3 airports (EWR, JFK, LGA). group_by(origin, month) group_by(origin) group_by(month)? Let’s investigate:happened second group_by(month) overwrote grouping structure meta-data earlier group_by(origin), end grouping month. lesson want group_by() two variables, include variables time group_by() adding comma variable names.\nLearning check\n(LC3.5) Recall Chapter 2 looked wind speeds months NYC. standard deviation column summary_monthly_temp data frame tell us temperatures NYC throughout year?(LC3.6) code required get mean standard deviation wind speed day 2023 NYC?(LC3.7) Recreate by_monthly_origin, instead grouping via group_by(origin, month), group variables different order group_by(month, origin). differs resulting dataset?(LC3.8) identify many flights left three airports carrier?(LC3.9) filter() operation differ group_by() followed summarize()?","code":"\nby_origin_monthly <- flights |> \n  group_by(origin, month) |> \n  summarize(count = n())`summarise()` has grouped output by 'origin'. You can override using the `.groups` argument.\nby_origin_monthly# A tibble: 36 × 3\n# Groups:   origin [3]\n   origin month count\n   <chr>  <int> <int>\n 1 EWR        1 11623\n 2 EWR        2 10991\n 3 EWR        3 12593\n 4 EWR        4 12022\n 5 EWR        5 12371\n 6 EWR        6 11339\n 7 EWR        7 11646\n 8 EWR        8 11561\n 9 EWR        9 11373\n10 EWR       10 11805\n# ℹ 26 more rows\nby_origin_monthly_incorrect <- flights |> \n  group_by(origin) |> \n  group_by(month) |> \n  summarize(count = n())\nby_origin_monthly_incorrect# A tibble: 12 × 2\n   month count\n   <int> <int>\n 1     1 36020\n 2     2 34761\n 3     3 39514\n 4     4 37476\n 5     5 38710\n 6     6 35921\n 7     7 36211\n 8     8 36765\n 9     9 35505\n10    10 36586\n11    11 34521\n12    12 33362"},{"path":"wrangling.html","id":"mutate","chapter":"3 Data Wrangling","heading":"3.5 mutate existing variables","text":"\nFIGURE 3.5: Diagram mutate() columns.\nAnother common transformation data create/compute new variables based existing ones shown Figure 3.5. example, say comfortable thinking temperature degrees Celsius (°C) instead degrees Fahrenheit (°F). formula convert temperatures °F °C \\[\n\\text{temp C} = \\frac{\\text{temp F} - 32}{1.8}\n\\]can apply formula temp variable using mutate() function dplyr package, takes existing variables mutates create new ones.code, mutate() weather data frame creating new variabletemp_in_C = (temp - 32) / 1.8,overwrite original weather data frame. overwrite data frame weather, instead assigning result new data frame like weather_new?rough rule thumb, long losing original information might need later, ’s acceptable practice overwrite existing data frames updated ones, . hand, overwrite variable temp, instead created new variable called temp_in_C? , erased original information contained temp temperatures Fahrenheit may still valuable us.Let’s now compute monthly average temperatures °F °C using group_by() summarize() code saw Section 3.4:Let’s consider another example. Passengers often frustrated flight departs late, aren’t annoyed , end, pilots can make time flight. known airline industry gain, create variable using mutate() function:Let’s take look dep_delay, arr_delay, resulting gain variables first 5 rows updated flights data frame Table 3.1.\nTABLE 3.1: First five rows departure/arrival delay gain variables\nflight first row departed 203 minutes late arrived 205 minutes late, “gained time air” gain -2 minutes, hence gain \\(203 - 205 = -2\\), loss 2 minutes. hand, flight third row departed late (dep_delay 47) arrived 34 minutes late (arr_delay 34), “gained time air” \\(47 - 34 = 13\\) minutes, hence gain 13.Let’s look summary statistics gain variable considering multiple summary functions summarize() code:see example median gain 11 minutes, largest +101 minutes largest negative gain (loss) -321 minutes! However, code take time type practice. ’ll see later Subsection 5.1.1 much succinct way compute variety common summary statistics: using tidy_summary() function moderndive package.Recall Section 2.5 since gain numerical variable, can visualize distribution using histogram.\nFIGURE 3.6: Histogram gain variable.\nresulting histogram Figure 3.6 provides additional perspective gain variable summary statistics computed earlier. example, note values gain right around 0.close discussion mutate() function create new variables, note can create multiple new variables mutate() code. Furthermore, within mutate() code can refer new variables just created. example, consider mutate() code Hadley Wickham Garrett Grolemund show Chapter 5 R Data Science (Grolemund Wickham 2017):\nLearning check\n(LC3.10) positive values gain variable flights correspond ? negative values? zero value?(LC3.11) create dep_delay arr_delay columns simply subtracting dep_time sched_dep_time similarly arrivals? Try code explain differences result actually appears flights.(LC3.12) can say distribution gain? Describe sentences using plot gain_summary data frame values.","code":"\nweather <- weather |> \n  mutate(temp_in_C = (temp - 32) / 1.8)\nsummary_monthly_temp <- weather |> \n  group_by(month) |> \n  summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), \n            mean_temp_in_C = mean(temp_in_C, na.rm = TRUE))\nsummary_monthly_temp# A tibble: 12 × 3\n   month mean_temp_in_F mean_temp_in_C\n   <int>          <dbl>          <dbl>\n 1     1           42.8           6.02\n 2     2           40.3           4.62\n 3     3           43.9           6.61\n 4     4           56.0          13.3 \n 5     5           62.4          16.9 \n 6     6           69.6          20.9 \n 7     7           79.2          26.2 \n 8     8           75.3          24.1 \n 9     9           70.1          21.2 \n10    10           61.0          16.1 \n11    11           47.0           8.31\n12    12           43.9           6.62\nflights <- flights |> \n  mutate(gain = dep_delay - arr_delay)\ngain_summary <- flights |> \n  summarize(\n    min = min(gain, na.rm = TRUE),\n    q1 = quantile(gain, 0.25, na.rm = TRUE),\n    median = quantile(gain, 0.5, na.rm = TRUE),\n    q3 = quantile(gain, 0.75, na.rm = TRUE),\n    max = max(gain, na.rm = TRUE),\n    mean = mean(gain, na.rm = TRUE),\n    sd = sd(gain, na.rm = TRUE),\n    missing = sum(is.na(gain))\n  )\ngain_summary# A tibble: 1 × 8\n    min    q1 median    q3   max  mean    sd missing\n  <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>   <int>\n1  -321     1     11    20   101  9.35  18.4   12534\nggplot(data = flights, mapping = aes(x = gain)) +\n  geom_histogram(color = \"white\", bins = 20)\nflights <- flights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours\n  )"},{"path":"wrangling.html","id":"arrange","chapter":"3 Data Wrangling","heading":"3.6 arrange and sort rows","text":"One commonly performed data-wrangling tasks sort data frame’s rows alphanumeric order one variables. dplyr package’s arrange() function allows us sort/reorder data frame’s rows according values specified variable.Suppose interested determining frequent destination airports domestic flights departing New York City 2023:Observe default rows resulting freq_dest data frame sorted alphabetical order destination. Say instead like see data, sorted least number flights (num_flights) instead:, however, opposite want. rows sorted least frequent destination airports displayed first. arrange() always returns rows sorted ascending order default. switch ordering “descending” order instead, use desc() function :","code":"\nfreq_dest <- flights |> \n  group_by(dest) |> \n  summarize(num_flights = n())\nfreq_dest# A tibble: 118 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 ABQ           228\n 2 ACK           916\n 3 AGS            20\n 4 ALB          1581\n 5 ANC            95\n 6 ATL         17570\n 7 AUS          4848\n 8 AVL          1617\n 9 AVP           145\n10 BDL           701\n# ℹ 108 more rows\nfreq_dest |> \n  arrange(num_flights)# A tibble: 118 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 LEX             1\n 2 AGS            20\n 3 OGG            20\n 4 SBN            24\n 5 HDN            28\n 6 PNS            71\n 7 MTJ            77\n 8 ANC            95\n 9 VPS           109\n10 AVP           145\n# ℹ 108 more rows\nfreq_dest |> \n  arrange(desc(num_flights))# A tibble: 118 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 BOS         19036\n 2 ORD         18200\n 3 MCO         17756\n 4 ATL         17570\n 5 MIA         16076\n 6 LAX         15968\n 7 FLL         14239\n 8 CLT         12866\n 9 DFW         11675\n10 SFO         11651\n# ℹ 108 more rows"},{"path":"wrangling.html","id":"joins","chapter":"3 Data Wrangling","heading":"3.7 join data frames","text":"Another common data transformation task “joining” “merging” two different datasets. example, flights data frame, variable carrier lists carrier code different flights. corresponding airline names \"UA\" \"AA\" might somewhat easy guess (United American Airlines), airlines codes \"VX\", \"HA\", \"B6\"? information provided separate data frame airlines.see airlines, carrier carrier code, name full name airline company. Using table, can see \"G4\", \"HA\", \"B6\" correspond Allegiant Air, Hawaiian Airlines, JetBlue, respectively. However, wouldn’t nice information single data frame instead two separate data frames? can “joining” flights airlines data frames.values variable carrier flights data frame match values variable carrier airlines data frame. case, can use variable carrier key variable match rows two data frames. Key variables almost always identification variables uniquely identify observational units saw Subsection 1.4.4. ensures rows data frames appropriately matched join. Hadley Garrett (Grolemund Wickham 2017) created diagram Figure 3.7 show different data frames nycflights23 package linked various key variables:\nFIGURE 3.7: Data relationships nycflights R Data Science.\n","code":"\nView(airlines)"},{"path":"wrangling.html","id":"matching-key-variable-names","chapter":"3 Data Wrangling","heading":"3.7.1 Matching key variable names","text":"flights airlines data frames, key variable want join/merge/match rows name: carrier. Let’s use inner_join() function join two data frames, rows matched variable carrier, compare resulting data frames:Observe flights flights_joined data frames identical except flights_joined additional variable name. values name correspond airline companies’ names indicated airlines data frame.visual representation inner_join() shown Figure 3.8 (Grolemund Wickham 2017). types joins available (left_join(), right_join(), outer_join(), anti_join()), inner_join() solve nearly problems ’ll encounter book.\nFIGURE 3.8: Diagram inner join R Data Science.\n","code":"\nflights_joined <- flights |> \n  inner_join(airlines, by = \"carrier\")\nView(flights)\nView(flights_joined)"},{"path":"wrangling.html","id":"diff-key","chapter":"3 Data Wrangling","heading":"3.7.2 Different key variable names","text":"Say instead interested destinations domestic flights departing NYC 2023, ask questions like: “cities airports ?”, “\"ORD\" Orlando?”, “\"FLL\"?”.airports data frame contains airport codes airport:However, look airports flights data frames, ’ll find airport codes variables different names. airports, airport code faa, whereas flights airport codes origin dest. fact highlighted visual representation relationships data frames Figure 3.7.order join two data frames airport code, inner_join() operation use = c(\"dest\" = \"faa\") argument modified code syntax allowing us join two data frames key variable different name:Let’s construct chain pipe operators |> computes number flights NYC destination, also includes information destination airport:case didn’t know, \"ORD\" airport code Chicago O’Hare airport \"FLL\" main airport Fort Lauderdale, Florida, can seen airport_name variable.","code":"\nView(airports)\nflights_with_airport_names <- flights |> \n  inner_join(airports, by = c(\"dest\" = \"faa\"))\nView(flights_with_airport_names)\nnamed_dests <- flights |>\n  group_by(dest) |>\n  summarize(num_flights = n()) |>\n  arrange(desc(num_flights)) |>\n  inner_join(airports, by = c(\"dest\" = \"faa\")) |>\n  rename(airport_name = name)\nnamed_dests# A tibble: 118 × 9\n   dest  num_flights airport_name                                          lat    lon   alt    tz dst   tzone           \n   <chr>       <int> <chr>                                               <dbl>  <dbl> <dbl> <dbl> <chr> <chr>           \n 1 BOS         19036 General Edward Lawrence Logan International Airport  42.4  -71.0    20    -5 A     America/New_York\n 2 ORD         18200 Chicago O'Hare International Airport                 42.0  -87.9   672    -6 A     America/Chicago \n 3 MCO         17756 Orlando International Airport                        28.4  -81.3    96    -5 A     America/New_York\n 4 ATL         17570 Hartsfield Jackson Atlanta International Airport     33.6  -84.4  1026    -5 A     America/New_York\n 5 MIA         16076 Miami International Airport                          25.8  -80.3     8    -5 A     America/New_York\n 6 LAX         15968 Los Angeles International Airport                    33.9 -118.    125    -8 A     America/Los_Ang…\n 7 FLL         14239 Fort Lauderdale Hollywood International Airport      26.1  -80.2     9    -5 A     America/New_York\n 8 CLT         12866 Charlotte Douglas International Airport              35.2  -80.9   748    -5 A     America/New_York\n 9 DFW         11675 Dallas Fort Worth International Airport              32.9  -97.0   607    -6 A     America/Chicago \n10 SFO         11651 San Francisco International Airport                  37.6 -122.     13    -8 A     America/Los_Ang…\n# ℹ 108 more rows"},{"path":"wrangling.html","id":"multiple-key-variables","chapter":"3 Data Wrangling","heading":"3.7.3 Multiple key variables","text":"Say instead want join two data frames multiple key variables. example, Figure 3.7, see order join flights weather data frames, need one key variable: year, month, day, hour, origin. combination 5 variables act uniquely identify observational unit weather data frame: hourly weather recordings 3 NYC airports.achieve specifying vector key variables join using c() function. Recall Subsection 1.2.1 c() short “combine” “concatenate.” \nLearning check\n(LC3.13) Looking Figure 3.7, joining flights weather (, words, matching hourly weather values flight), need join year, month, day, hour, origin, just hour?(LC3.14) surprises top 10 destinations NYC 2023?","code":"\nflights_weather_joined <- flights |>\n  inner_join(weather, by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\nView(flights_weather_joined)"},{"path":"wrangling.html","id":"normal-forms","chapter":"3 Data Wrangling","heading":"3.7.4 Normal forms","text":"data frames included nycflights23 package form minimizes redundancy data. example, flights data frame saves carrier code airline company; include actual name airline. example, ’ll see first row flights carrier equal UA, include airline name “United Air Lines Inc.”names airline companies included name variable airlines data frame. order airline company name included flights, join two data frames follows:capable performing join data frames keys common relate one another: carrier variable flights airlines data frames. key variable(s) base joins often identification variables mentioned previously.important property ’s known normal forms data. process decomposing data frames less redundant tables without losing information called normalization. information available Wikipedia.dplyr SQL mentioned introduction chapter use normal forms. Given share commonalities, learn either two tools, can learn easily.\nLearning check\n(LC3.15) advantages data normal forms? disadvantages?","code":"\njoined_flights <- flights |> \n  inner_join(airlines, by = \"carrier\")\nView(joined_flights)"},{"path":"wrangling.html","id":"other-verbs","chapter":"3 Data Wrangling","heading":"3.8 Other verbs","text":"useful data-wrangling verbs:select() subset variables/columns.relocate() variables/columns new position.rename() variables/columns new names.Return top_n() values variable.","code":""},{"path":"wrangling.html","id":"select","chapter":"3 Data Wrangling","heading":"3.8.1 select variables","text":"\nFIGURE 3.9: Diagram select() columns.\n’ve seen flights data frame nycflights23 package contains 19 different variables. can identify names 19 variables running glimpse() function dplyr package:However, say need two 19 variables, say carrier flight. can select() two variables:function makes easier explore large datasets since allows us limit scope variables care . example, select() smaller number variables shown Figure 3.9, make viewing dataset RStudio’s spreadsheet viewer digestible.Let’s say instead want drop, de-select, certain variables. example, consider variable year flights data frame. variable isn’t quite “variable” always 2023 hence doesn’t change. Say want remove variable data frame. can deselect year using - sign:Another way selecting columns/variables specifying range columns:select() columns month day, well arr_time sched_arr_time, drop rest.helper functions starts_with(), ends_with(), contains() can used select variables/columns match conditions. examples,Lastly, select() function can also used reorder columns used everything() helper function. example, suppose want hour, minute, time_hour variables appear immediately year, month, day variables, discarding rest variables. following code, everything() pick remaining variables:","code":"\nglimpse(flights)\nflights |> \n  select(carrier, flight)\nflights_no_year <- flights |> select(-year)\nflight_arr_times <- flights |> select(month:day, arr_time:sched_arr_time)\nflight_arr_times\nflights |> select(starts_with(\"a\"))\nflights |> select(ends_with(\"delay\"))\nflights |> select(contains(\"time\"))\nflights_reorder <- flights |> \n  select(year, month, day, hour, minute, time_hour, everything())\nglimpse(flights_reorder)"},{"path":"wrangling.html","id":"relocate","chapter":"3 Data Wrangling","heading":"3.8.2 relocate variables","text":"Another (usually shorter) way reorder variables using relocate() function. function allows move variables new position data frame. example, want move hour, minute, time_hour variables appear immediately year, month, day variables, can use following code:","code":"\nflights_relocate <- flights |> \n  relocate(hour, minute, time_hour, .after = day)\nglimpse(flights_relocate)"},{"path":"wrangling.html","id":"rename","chapter":"3 Data Wrangling","heading":"3.8.3 rename variables","text":"One useful function rename(), may guessed changes name variables. Suppose want focus dep_time arr_time change dep_time arr_time departure_time arrival_time instead flights_time_new data frame:Note case used single = sign within rename(). example, departure_time = dep_time renames dep_time variable new name departure_time. testing equality like using ==. Instead want assign new variable departure_time values dep_time delete variable dep_time. Note new dplyr users often forget new variable name comes equal sign.","code":"\nflights_time_new <- flights |> \n  select(dep_time, arr_time) |> \n  rename(departure_time = dep_time, arrival_time = arr_time)\nglimpse(flights_time_new)"},{"path":"wrangling.html","id":"top_n-values-of-a-variable","chapter":"3 Data Wrangling","heading":"3.8.4 top_n values of a variable","text":"can also return top n values variable using top_n() function. example, can return data frame top 10 destination airports using example Subsection 3.7.2. Observe set number values return n = 10 wt = num_flights indicate want rows corresponding top 10 values num_flights. See help file top_n() running ?top_n information.Let’s arrange() results descending order num_flights:\nLearning check\n(LC3.16) ways select three dest, air_time, distance variables flights? Give code showing least three different ways.(LC3.17) one use starts_with(), ends_with(), contains() select columns flights data frame? Provide three different examples total: one starts_with(), one ends_with(), one contains().(LC3.18) might want use select() function data frame?(LC3.19) Create new data frame shows top 5 airports largest arrival delays NYC 2023.","code":"\nnamed_dests |> top_n(n = 10, wt = num_flights)\nnamed_dests |> \n  top_n(n = 10, wt = num_flights) |> \n  arrange(desc(num_flights))"},{"path":"wrangling.html","id":"wrangling-conclusion","chapter":"3 Data Wrangling","heading":"3.9 Conclusion","text":"","code":""},{"path":"wrangling.html","id":"summary-table-1","chapter":"3 Data Wrangling","heading":"3.9.1 Summary table","text":"Let’s recap data-wrangling verbs Table 3.2. Using verbs pipe |> operator Section 3.1, ’ll able write easily legible code perform almost data wrangling data transformation necessary rest book.\nTABLE 3.2: Summary data-wrangling verbs\n\nLearning check\n(LC3.20) Let’s now put newly acquired data-wrangling skills test!airline industry measure passenger airline’s capacity available seat miles, equal number seats available multiplied number miles kilometers flown summed flights.example, let’s consider scenario Figure 3.10. Since airplane 4 seats travels 200 miles, available seat miles \\(4 \\times 200 = 800\\).\nFIGURE 3.10: Example available seat miles one flight.\nExtending idea, let’s say airline 2 flights using plane 10 seats flew 500 miles 3 flights using plane 20 seats flew 1000 miles, available seat miles \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles.Using datasets included nycflights23 package, compute available seat miles airline sorted descending order. completing necessary data-wrangling steps, resulting data frame 14 rows (one airline) 2 columns (airline name available seat miles). hints:Crucial: Unless confident , worthwhile starting code right away. Rather, first sketch paper necessary data wrangling steps using exact code, rather high-level pseudocode informal yet detailed enough articulate . way won’t confuse trying (algorithm) going (writing dplyr code).Take close look datasets using View() function: flights, weather, planes, airports, airlines identify variables necessary compute available seat miles.Figure 3.7 showing various datasets can joined also useful.Consider data-wrangling verbs Table 3.2 toolbox!","code":""},{"path":"wrangling.html","id":"additional-resources-2","chapter":"3 Data Wrangling","heading":"3.9.2 Additional resources","text":"R script file R code used chapter available .online Appendix C, provide page data wrangling ‘tips tricks’ consisting common data wrangling questions ’ve encountered student projects (shout Dr. Jenny Smetzer work setting !):Dealing missing valuesReordering bars barplotShowing money axisChanging values inside cellsConverting numerical variable categorical oneComputing proportionsDealing %, commas, dollar signsHowever, provide tips tricks page covering possible data wrangling questions long useful!\nwant unlock power dplyr package data wrangling, suggest check RStudio’s “Data Transformation dplyr” cheatsheet. cheatsheet summarizes much ’ve discussed chapter, particular intermediate level advanced data-wrangling functions, providing quick easy--read visual descriptions. fact, many diagrams illustrating data wrangling operations chapter, Figure 3.1 filter(), originate cheatsheet.current version RStudio 2025, can access cheatsheet going RStudio Menu Bar -> Help -> Cheatsheets -> “Data Transformation dplyr.” can see preview figure .\nFIGURE 3.11: Data Transformation dplyr cheatsheet (first page).\ntop data-wrangling verbs examples presented section, ’d like see examples using dplyr package data wrangling, check Chapter 5 R Data Science (Grolemund Wickham 2017).","code":""},{"path":"wrangling.html","id":"whats-to-come-1","chapter":"3 Data Wrangling","heading":"3.9.3 What’s to come?","text":"far book, ’ve explored, visualized, wrangled data saved data frames. data frames saved spreadsheet-like format: rectangular shape certain number rows corresponding observations certain number columns corresponding variables describing observations.’ll see upcoming Chapter 4 actually two ways represent data spreadsheet-type rectangular format: (1) “wide” format (2) “tall/narrow” format. tall/narrow format also known “tidy” format R user circles. distinction “tidy” non-“tidy” formatted data subtle, immense implications data science work. almost packages used book, including ggplot2 package data visualization dplyr package data wrangling, assume data frames “tidy” format.Furthermore, now ’ve explored, visualized, wrangled data saved within R packages. want analyze data saved Microsoft Excel, Google Sheets, “Comma-Separated Values” (CSV) file? Section 4.1, ’ll show import data R using readr package.","code":""},{"path":"tidy.html","id":"tidy","chapter":"4 Data Importing and Tidy Data","heading":"4 Data Importing and Tidy Data","text":"Subsection 1.2.1, introduced concept data frame R: rectangular spreadsheet-like representation data rows correspond observations columns correspond variables describing observation. Section 1.4, started exploring first data frame: flights data frame included nycflights23 package. Chapter 2, created visualizations based data included flights data frames weather. Chapter 3, learned take existing data frames transform/modify suit ends.final chapter “Data Science tidyverse” portion book, extend ideas discussing type data formatting called “tidy” data. see data stored “tidy” format just everyday definition term “tidy” might suggest: data “neatly organized.” Instead, define term “tidy” ’s used data scientists use R, outlining set rules data saved.Knowledge type data formatting necessary treatment data visualization Chapter 2 data wrangling Chapter 3. data used already “tidy” format. chapter, ’ll now see format essential using tools covered now. Furthermore, also useful subsequent chapters book cover regression statistical inference. First, however, ’ll show import spreadsheet data R.","code":""},{"path":"tidy.html","id":"tidy-packages","chapter":"4 Data Importing and Tidy Data","heading":"Needed packages","text":"Let’s load packages needed chapter (assumes ’ve already installed ). needed, read Section 1.3 information install load R packages.Note load fivethirtyeight package, ’ll receive following message:larger datasets need installed separately, like senators house_district_forecast. install , recommend \ninstall fivethirtyeightdata package running: install.packages(‘fivethirtyeightdata’, repos =\n‘https://fivethirtyeightdata.github.io/drat/’, type = ‘source’)message can ignored purposes book, ’d like explore larger datasets, can install fivethirtyeightdata package suggested.","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(nycflights23)\nlibrary(fivethirtyeight)"},{"path":"tidy.html","id":"csv","chapter":"4 Data Importing and Tidy Data","heading":"4.1 Importing data","text":"point, ’ve almost entirely used data stored inside R package. Say instead data saved computer somewhere online. can analyze data R? Spreadsheet data often saved one following three formats:First, Comma Separated Values .csv file. can think .csv file bare-bones spreadsheet :line file corresponds one row data/one observation.Values line separated commas. words, values different variables separated commas row.first line often, always, header row indicating names columns/variables.Second, Excel .xlsx spreadsheet file. format based Microsoft’s proprietary Excel software. opposed bare-bones .csv files, .xlsx Excel files contain lot meta-data (data data). Recall saw previous example meta-data Section 3.4 adding “group structure” meta-data data frame using group_by() verb. examples Excel spreadsheet meta-data include use bold italic fonts, colored cells, different column widths, formula macros.Third, Google Sheets file, “cloud” online-based way work spreadsheet. Google Sheets allows download data comma separated values .csv Excel .xlsx formats. One way import Google Sheets data R go Google Sheets menu bar -> File -> Download -> Select “Microsoft Excel” “Comma-separated values” load data R. advanced way import Google Sheets data R using googlesheets4 package, method leave advanced data science book.’ll cover two methods importing .csv .xlsx spreadsheet data R: one using console using RStudio’s graphical user interface, abbreviated “GUI.”","code":""},{"path":"tidy.html","id":"using-the-console","chapter":"4 Data Importing and Tidy Data","heading":"4.1.1 Using the console","text":"First, let’s import Comma Separated Values .csv file exists internet. .csv file dem_score.csv contains ratings level democracy different countries spanning 1952 1992 accessible https://moderndive.com/data/dem_score.csv. Let’s use read_csv() function readr (Wickham, Hester, Bryan 2024) package read web, import R, save data frame called dem_score.dem_score data frame, minimum value -10 corresponds highly autocratic nation, whereas value 10 corresponds highly democratic nation. Note also backticks surround different variable names. Variable names R default allowed start number include spaces, can get around fact surrounding column name backticks. ’ll revisit dem_score data frame case study upcoming Section 4.3.Note read_csv() function included readr package different read.csv() function comes installed R. difference names might seem trivial (_ instead .), read_csv() function , opinion, easier use since can easily read data web generally imports data much faster speed. Furthermore, read_csv() function included readr saves data frames tibbles default.","code":"\nlibrary(readr)\ndem_score <- read_csv(\"https://moderndive.com/data/dem_score.csv\")\ndem_score# A tibble: 96 × 10\n   country    `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`\n   <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 Albania        -9     -9     -9     -9     -9     -9     -9     -9      5\n 2 Argentina      -9     -1     -1     -9     -9     -9     -8      8      7\n 3 Armenia        -9     -7     -7     -7     -7     -7     -7     -7      7\n 4 Australia      10     10     10     10     10     10     10     10     10\n 5 Austria        10     10     10     10     10     10     10     10     10\n 6 Azerbaijan     -9     -7     -7     -7     -7     -7     -7     -7      1\n 7 Belarus        -9     -7     -7     -7     -7     -7     -7     -7      7\n 8 Belgium        10     10     10     10     10     10     10     10     10\n 9 Bhutan        -10    -10    -10    -10    -10    -10    -10    -10    -10\n10 Bolivia        -4     -3     -3     -4     -7     -7      8      9      9\n# ℹ 86 more rows"},{"path":"tidy.html","id":"using-rstudios-interface","chapter":"4 Data Importing and Tidy Data","heading":"4.1.2 Using RStudio’s interface","text":"Let’s read exact data, time Excel file saved computer. Furthermore, ’ll using RStudio’s graphical interface instead running read_csv() console. First, download Excel file dem_score.xlsx going https://moderndive.com/data/dem_score.xlsx, thenGo Files pane RStudio.Navigate directory (.e., folder computer) downloaded dem_score.xlsx Excel file saved. example, might Downloads folder.Click dem_score.xlsx.Click “Import Dataset…”point, see screen pop-like Figure 4.1. clicking “Import” button bottom right Figure 4.1, RStudio save spreadsheet’s data data frame called dem_score display contents spreadsheet viewer.\nFIGURE 4.1: Importing Excel file R.\nFurthermore, note “Code Preview” block bottom right Figure 4.1. can copy paste code reload data later programmatically, instead repeating manual point--click process.","code":""},{"path":"tidy.html","id":"tidy-data-ex","chapter":"4 Data Importing and Tidy Data","heading":"4.2 Tidy data","text":"Let’s now switch gears learn concept “tidy” data format motivating example fivethirtyeight package. fivethirtyeight package (Kim, Ismay, Chunn 2021) provides access datasets used many articles published data journalism website, FiveThirtyEight.com. complete list 128 datasets included fivethirtyeight package, check package webpage going : https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html.Let’s focus attention drinks data frame look first 5 rows:reading help file running ?drinks, ’ll see drinks data frame containing results survey average number servings beer, spirits, wine consumed 193 countries. data originally reported FiveThirtyEight.com Mona Chalabi’s article: “Dear Mona Followup: People Drink Beer, Wine Spirits?”.Let’s apply data-wrangling verbs learned Chapter 3 drinks data frame:filter() consider 4 countries: United States, China, Italy, Saudi Arabia, thenselect() columns except total_litres_of_pure_alcohol using - sign, thenrename() beer_servings, spirit_servings, wine_servings beer, spirit, wine, respectively.save resulting data frame drinks_smaller:Let’s now ask question: “Using drinks_smaller data frame, create side--side barplot Figure 4.2?”. Recall saw barplots displaying two categorical variables Subsection 2.8.3.\nFIGURE 4.2: Comparing alcohol consumption 4 countries.\nLet’s break grammar graphics introduced Section 2.1:categorical variable country four levels (China, Italy, Saudi Arabia, USA) mapped x-position bars.numerical variable servings mapped y-position bars (height bars).categorical variable type three levels (beer, spirit, wine) mapped fill color bars.Observe drinks_smaller three separate variables beer, spirit, wine. order use ggplot() function recreate barplot Figure 4.2 however, need single variable type three possible values: beer, spirit, wine. map type variable fill aesthetic plot. words, recreate barplot Figure 4.2, data frame look like :Observe drinks_smaller drinks_smaller_tidy rectangular shape contain 12 numerical values (3 alcohol types 4 countries), formatted differently. drinks_smaller formatted ’s known “wide” format, whereas drinks_smaller_tidy formatted ’s known “long/narrow” format.context data science R, long/narrow format also known “tidy” format. order use ggplot2 dplyr packages data visualization data wrangling, input data frames must “tidy” format. Thus, non-“tidy” data must converted “tidy” format first. convert non-“tidy” data frames like drinks_smaller “tidy” data frames like drinks_smaller_tidy, let’s define “tidy” data.","code":"# A tibble: 5 × 5\n  country     beer_servings spirit_servings wine_servings total_litres_of_pure_alcohol\n  <chr>               <int>           <int>         <int>                        <dbl>\n1 Afghanistan             0               0             0                          0  \n2 Albania                89             132            54                          4.9\n3 Algeria                25               0            14                          0.7\n4 Andorra               245             138           312                         12.4\n5 Angola                217              57            45                          5.9\ndrinks_smaller <- drinks |> \n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) |> \n  select(-total_litres_of_pure_alcohol) |> \n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\ndrinks_smaller_tidy# A tibble: 12 × 3\n   country      type   servings\n   <chr>        <chr>     <int>\n 1 China        beer         79\n 2 Italy        beer         85\n 3 Saudi Arabia beer          0\n 4 USA          beer        249\n 5 China        spirit      192\n 6 Italy        spirit       42\n 7 Saudi Arabia spirit        5\n 8 USA          spirit      158\n 9 China        wine          8\n10 Italy        wine        237\n11 Saudi Arabia wine          0\n12 USA          wine         84"},{"path":"tidy.html","id":"tidy-definition","chapter":"4 Data Importing and Tidy Data","heading":"4.2.1 Definition of tidy data","text":"surely heard word “tidy” life:“Tidy room!”“Write homework tidy way, easier provide feedback.”Marie Kondo’s best-selling book, Life-Changing Magic Tidying : Japanese Art Decluttering Organizing, Netflix TV series Tidying Marie Kondo.mean data “tidy”? “tidy” clear English meaning “organized,” word “tidy” data science using R means data follows standardized format. follow Hadley Wickham’s British English definition “tidy” data (Wickham 2014) shown also Figure 4.3:dataset collection values, usually either numbers (quantitative) strings AKA text data (qualitative/categorical). Values organised two ways. Every value belongs variable observation. variable contains values measure underlying attribute (like height, temperature, duration) across units. observation contains values measured unit (like person, day, city) across attributes.“Tidy” data standard way mapping meaning dataset structure. dataset messy tidy depending rows, columns tables matched observations, variables types. tidy data:variable forms column.observation forms row.type observational unit forms table.\nFIGURE 4.3: Tidy data graphic R Data Science.\nexample, say following table stock prices Table 4.1:\nTABLE 4.1: Stock prices (non-tidy format)\nAlthough data rectangular spreadsheet format, “tidy.” three variables (date, stock name, stock price), three separate columns. tidy data, variable column, shown Table 4.2. tables present information, different formats.\nTABLE 4.2: Stock prices (tidy format)\nhand, consider data Table 4.3.\nTABLE 4.3: Example tidy data\ncase, even though variable “Boeing Price” occurs just like non-“tidy” data Table 4.1, data “tidy” since three variables three unique pieces information: Date, Boeing price, Weather day.\nLearning check\n(LC4.1) common characteristics “tidy” data frames?(LC4.2) makes “tidy” data frames useful organizing data?","code":""},{"path":"tidy.html","id":"converting-to-tidy-data","chapter":"4 Data Importing and Tidy Data","heading":"4.2.2 Converting to tidy data","text":"book far, ’ve seen data frames already “tidy” format. Furthermore, rest book, ’ll mostly see data frames already “tidy” format well. always case however datasets world. original data frame wide (non-“tidy”) format like use ggplot2 dplyr packages, first convert “tidy” format. , recommend using pivot_longer() function tidyr package (Wickham, Vaughan, Girlich 2024).Going back drinks_smaller data frame earlier:convert “tidy” format using pivot_longer() function tidyr package follows:set arguments pivot_longer() follows:names_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .values_to name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric values beer, wine, spirit columns drinks_smaller data corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols little nuanced, let’s consider code ’s written slightly differently produces output:Note third argument now specifies columns want “tidy” c(beer, spirit, wine), instead columns don’t want “tidy” using -country. use c() function create vector columns drinks_smaller ’d like “tidy.” Note since three columns appear one another drinks_smaller data frame, also following cols argument:drinks_smaller_tidy “tidy” formatted data frame, can now produce barplot saw Figure 4.2 using geom_col(). done Figure 4.4. Recall Section 2.8 barplots use geom_col() geom_bar(), since like map “pre-counted” servings variable y-aesthetic bars.\nFIGURE 4.4: Comparing alcohol consumption 4 countries using geom_col().\nConverting “wide” format data “tidy” format often confuses new R users. way learn get comfortable pivot_longer() function practice, practice, practice using different datasets. example, run ?pivot_longer look examples bottom help file. ’ll show another example using pivot_longer() convert “wide” formatted data frame “tidy” format Section 4.3.however want convert “tidy” data frame “wide” format, need use pivot_wider() function instead. Run ?pivot_wider look examples bottom help file examples.can also view examples pivot_longer() pivot_wider() tidyverse.org webpage. ’s nice example check different functions available data tidying case study using data World Health Organization webpage. Furthermore, week R4DS Online Learning Community posts dataset weekly #TidyTuesday event might serve nice place find data explore transform.\nLearning check\n(LC4.3) Take look airline_safety data frame included fivethirtyeight data package. Run following:reading help file running ?airline_safety, see airline_safety data frame containing information different airline companies’ safety records. data originally reported data journalism website, FiveThirtyEight.com, Nate Silver’s article, “Travelers Avoid Flying Airlines Crashes Past?”. Let’s consider variables airline relating fatalities simplicity:data frame “tidy” format. convert data frame “tidy” format, particular variable fatalities_years indicating incident year variable count fatality counts?","code":"\ndrinks_smaller# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\ndrinks_smaller_tidy <- drinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy# A tibble: 12 × 3\n   country      type   servings\n   <chr>        <chr>     <int>\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\nggplot(drinks_smaller_tidy, aes(x = country, y = servings, fill = type)) +\n  geom_col(position = \"dodge\")\nairline_safety\nairline_safety_smaller <- airline_safety |> \n  select(airline, starts_with(\"fatalities\"))\nairline_safety_smaller# A tibble: 56 × 3\n   airline               fatalities_85_99 fatalities_00_14\n   <chr>                            <int>            <int>\n 1 Aer Lingus                           0                0\n 2 Aeroflot                           128               88\n 3 Aerolineas Argentinas                0                0\n 4 Aeromexico                          64                0\n 5 Air Canada                           0                0\n 6 Air France                          79              337\n 7 Air India                          329              158\n 8 Air New Zealand                      0                7\n 9 Alaska Airlines                      0               88\n10 Alitalia                            50                0\n# ℹ 46 more rows"},{"path":"tidy.html","id":"nycflights23-package-1","chapter":"4 Data Importing and Tidy Data","heading":"4.2.3 nycflights23 package","text":"Recall nycflights23 package introduced Section 1.4 data domestic flights departing New York City 2023. Let’s revisit flights data frame running View(flights). saw flights rectangular shape, 435,352 rows corresponding flight 22 columns corresponding different characteristics/measurements flight. satisfied first two criteria definition “tidy” data Subsection 4.2.1: “variable forms column” “observation forms row.” third property “tidy” data “type observational unit forms table”?Recall saw Subsection 1.4.3 observational unit flights data frame individual flight. words, rows flights data frame refer characteristics/measurements individual flights. Also included nycflights23 package data frames rows representing different observational units (Ismay, Couch, Wickham 2025):airlines: translation two letter IATA carrier codes airline company names (14 total). observational unit airline company.planes: aircraft information 4,840 planes used, .e., observational unit aircraft.weather: hourly meteorological data (8,736 observations) three NYC airports, .e., observational unit hourly measurement weather one three airports.airports: airport names locations. observational unit airport.organization information five data frames follows third “tidy” data property: observations corresponding observational unit saved table, .e., data frame. think property old English expression: “birds feather flock together.”","code":""},{"path":"tidy.html","id":"case-study-tidy","chapter":"4 Data Importing and Tidy Data","heading":"4.3 Case study: democracy in Guatemala","text":"section, ’ll show another example convert data frame isn’t “tidy” format (“wide” format) data frame “tidy” format (“long/narrow” format). ’ll using pivot_longer() function tidyr package .Furthermore, ’ll make use functions ggplot2 dplyr packages produce time-series plot showing democracy scores changed 40 years 1952 1992 Guatemala. Recall saw time-series plots Section 2.4 creating linegraphs using geom_line().Let’s use dem_score data frame imported Section 4.1, focus data corresponding Guatemala.Let’s lay grammar graphics saw Section 2.1.First know need set data = guat_dem use geom_line() layer, aesthetic mapping variables? ’d like see democracy score changed years, need map:year x-position aesthetic anddemocracy_score y-position aestheticNow stuck predicament, much like drinks_smaller example Section 4.2. see variable named country, value \"Guatemala\". variables denoted different year values. Unfortunately, guat_dem data frame “tidy” hence appropriate format apply grammar graphics, thus use ggplot2 package just yet.need take values columns corresponding years guat_dem convert new “names” variable called year. Furthermore, need take democracy score values inside data frame turn new “values” variable called democracy_score. resulting data frame three columns: country, year, democracy_score. Recall pivot_longer() function tidyr package us:set arguments pivot_longer() follows:names_to name variable new “tidy” data frame contain column names original data. Observe set names_to = \"year\". resulting guat_dem_tidy, column year contains years Guatemala’s democracy scores measured.values_to name variable new “tidy” data frame contain values original data. Observe set values_to = \"democracy_score\". resulting guat_dem_tidy column democracy_score contains 1 \\(\\times\\) 9 = 9 democracy scores numeric values.third argument columns either want don’t want “tidy.” Observe set cols = -country indicating don’t want “tidy” country variable guat_dem rather variables 1952 1992.last argument names_transform tells R type variable year set . Without specifying integer ’ve done , pivot_longer() set character value default.can now create time-series plot Figure 4.5 visualize democracy scores Guatemala changed 1952 1992 using geom_line(). Furthermore, ’ll use labs() function ggplot2 package add informative labels aes()thetic attributes plot, case x y positions.\nFIGURE 4.5: Democracy scores Guatemala 1952-1992.\nNote forgot include names_transform argument specifying year character format, gotten error since geom_line() wouldn’t known sort character values year right order.\nLearning check\n(LC4.4) Convert dem_score data frame \n“tidy” data frame assign name dem_score_tidy resulting long-formatted data frame.(LC4.5) Read life expectancy data stored https://moderndive.com/data/le_mess.csv convert “tidy” data frame.","code":"\nguat_dem <- dem_score |> \n  filter(country == \"Guatemala\")\nguat_dem# A tibble: 1 × 10\n  country   `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`\n  <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Guatemala      2     -6     -5      3      1     -3     -7      3      3\nguat_dem_tidy <- guat_dem |> \n  pivot_longer(names_to = \"year\", \n               values_to = \"democracy_score\", \n               cols = -country,\n               names_transform = list(year = as.integer)) \nguat_dem_tidy# A tibble: 9 × 3\n  country    year democracy_score\n  <chr>     <int>           <dbl>\n1 Guatemala  1952               2\n2 Guatemala  1957              -6\n3 Guatemala  1962              -5\n4 Guatemala  1967               3\n5 Guatemala  1972               1\n6 Guatemala  1977              -3\n7 Guatemala  1982              -7\n8 Guatemala  1987               3\n9 Guatemala  1992               3\nggplot(guat_dem_tidy, aes(x = year, y = democracy_score)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Democracy Score\")"},{"path":"tidy.html","id":"tidyverse-package","chapter":"4 Data Importing and Tidy Data","heading":"4.4 tidyverse package","text":"Notice beginning chapter loaded following four packages, among four frequently used R packages data science:Recall ggplot2 data visualization, dplyr data wrangling, readr importing spreadsheet data R, tidyr converting data “tidy” format. much quicker way load packages individually loading : installing loading tidyverse package. tidyverse package acts “umbrella” package whereby installing/loading install/load multiple packages .installing tidyverse package normal package seen Section 1.3, running:running:purrr, tibble, stringr, forcats left advanced book; check R Data Science learn packages.remainder book, ’ll start every chapter running library(tidyverse), instead loading various component packages individually. tidyverse “umbrella” package gets name fact functions packages designed common inputs outputs: data frames “tidy” format. standardization input output data frames makes transitions different functions different packages seamless possible. information, check tidyverse.org webpage package.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)"},{"path":"tidy.html","id":"tidy-data-conclusion","chapter":"4 Data Importing and Tidy Data","heading":"4.5 Conclusion","text":"","code":""},{"path":"tidy.html","id":"additional-resources-3","chapter":"4 Data Importing and Tidy Data","heading":"4.5.1 Additional resources","text":"R script file R code used chapter available .want learn using readr tidyr package, suggest check RStudio’s “Data Import Cheat Sheet.” current version RStudio mid-2025, can access cheatsheet going RStudio Menu Bar -> Help -> Cheat Sheets -> “Browse Cheat Sheets…” -> Scroll page “Data import reader, readxl, googlesheets4…” information using readr, readxl googlesheets4 packages import data “Data tidying tidyr cheatsheet” information using tidyr package “tidy” data. can see preview cheatsheets figures .\nFIGURE 4.6: Data Import cheatsheet (first page): readr package.\n\nFIGURE 4.7: Data Tidying cheatsheet (first page): tidyr package.\n","code":""},{"path":"tidy.html","id":"whats-to-come-2","chapter":"4 Data Importing and Tidy Data","heading":"4.5.2 What’s to come?","text":"Congratulations! ’ve completed “Data Science tidyverse” portion book. ’ll now move “Statistical modeling moderndive” portion book Chapters 5 6, ’ll leverage data visualization wrangling skills model relationships different variables data frames.However, ’re going leave Chapter 10 “Inference Regression” ’ve covered statistical inference Chapters 7, 8, 9. Onwards upwards Statistical/Data Modeling shown Figure 4.8!\nFIGURE 4.8: ModernDive flowchart – Part II!\n","code":""},{"path":"regression.html","id":"regression","chapter":"5 Simple Linear Regression","heading":"5 Simple Linear Regression","text":"introduced data visualization Chapter 2, data wrangling Chapter 3, data importing “tidy” data Chapter 4. chapter, work regression, method helps us study relationship outcome variable response one explanatory variables regressors.\nmethod starts proposing statistical model. Data collected used estimate coefficients parameters model, results typically used two purposes:explanation want describe changes one regressors associated changes response, quantify changes, establish regressors truly association response, determine whether model used describe relationship response explanatory variables seems appropriate.prediction want determine, based observed values regressors, value response ? concerned regressors relate interact one another response, simply want good predictions possible.illustration, assume want study relationship blood pressure potential risk factors daily salt intake, age, physical activity levels.\nresponse blood pressure, regressors risk factors.\nuse linear regression explanation, may want determine whether reducing daily salt intake real effect lowering blood pressure, much blood pressure decreases individual reduces salt intake half.\ninformation may help target individuals specific age group advice dietary changes manage blood pressure.\nhand, use linear regression prediction, like determine, accurately possible, blood pressure given individual based data collected salt intake, age, physical activity levels. chapter, use linear regression explanation.basic commonly-used type regression linear regression.\nLinear regression involves numerical response one regressors can numerical categorical. called linear regression statistical model describes relationship expected response regressors assumed linear. particular, model single regressor, linear regression equation line.\nLinear regression foundation almost type regression related method.Chapter 5, introduce linear regression one regressor. Section 5.1, explanatory variable numerical. scenario known simple linear regression. Section 5.2, explanatory variable categorical.Chapter 6 multiple regression, extend ideas work models two explanatory variables. Section 6.1, work two numerical explanatory variables. Section 6.2, work one numerical one categorical explanatory variable study model without interactions.Chapter 10 inference regression, revisit regression models analyze results using statistical inference, method discussed Chapters 7, 8, 9 sampling, bootstrapping confidence intervals, hypothesis testing \\(p\\)-values, respectively. focus also using linear regression prediction instead explanation.begin regression single explanatory variable. also introduce correlation coefficient, discuss “correlation versus causation,” determine whether model fits data observed.","code":""},{"path":"regression.html","id":"reg-packages","chapter":"5 Simple Linear Regression","heading":"Needed packages","text":"now load packages needed chapter (assumes ’ve already installed ). chapter, introduce new packages:tidyverse “umbrella” (Wickham 2023) package. Recall discussion Section 4.4 loading tidyverse package running library(tidyverse) loads following commonly used data science packages :\nggplot2 data visualization\ndplyr data wrangling\ntidyr converting data “tidy” format\nreadr importing spreadsheet data R\nwell advanced purrr, tibble, stringr, forcats packages\nggplot2 data visualizationdplyr data wranglingtidyr converting data “tidy” formatreadr importing spreadsheet data RAs well advanced purrr, tibble, stringr, forcats packagesThe moderndive package datasets functions tidyverse-friendly introductory linear regression well data frame summary function.needed, read Section 1.3 information install load R packages.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)"},{"path":"regression.html","id":"model1","chapter":"5 Simple Linear Regression","heading":"5.1 One numerical explanatory variable","text":"introduce model needed simple linear regression, present example.\ncountries exhibit high fertility rates others significantly lower ones? correlations fertility rates life expectancy across different continents nations? underlying socioeconomic factors influencing trends?questions interest demographers policy makers, understanding fertility rates important planning development. analyzing dataset UN member states, includes variables country codes (ISO), fertility rates, life expectancy 2022, researchers can uncover patterns make predictions fertility rates based life expectancy.section, aim explain differences fertility rates function one numerical variable: life expectancy. countries higher life expectancy also lower fertility rates? instead countries higher life expectancy tend higher fertility rates? relationship life expectancy fertility rates? answer questions modeling relationship fertility rates life expectancy using simple linear regression :numerical outcome variable \\(y\\) (country’s fertility rate) andA single numerical explanatory variable \\(x\\) (country’s life expectancy).","code":""},{"path":"regression.html","id":"model1EDA","chapter":"5 Simple Linear Regression","heading":"5.1.1 Exploratory data analysis","text":"data 193 current UN member states (2024) can found un_member_states_2024 data frame included moderndive package. However, keep things simple include rows don’t missing data na.omit() select() subset variables ’ll consider chapter, save data new data frame called UN_data_ch5:crucial step kind analysis modeling performing exploratory data analysis, EDA short. EDA gives sense distributions individual variables data, whether potential relationships exist variables, whether outliers /missing values, (importantly) build model. three common steps EDA:crucially, looking raw data values.Computing summary statistics, means, medians, maximums.Creating data visualizations.perform first common step exploratory data analysis: looking raw data values. step seems trivial, unfortunately many data analysts ignore . However, getting early sense raw data looks like can often prevent many larger issues road.can using RStudio’s spreadsheet viewer using glimpse() function introduced Subsection 1.4.3 exploring data frames:Observe Rows: 181 indicates 181 rows/observations UN_data_ch5 filtering missing values, row corresponds one observed country/member state. important note observational unit individual country. Recall Subsection 1.4.3 observational unit “type thing” measured variables.full description variables included un_member_states_2024 can found reading associated help file (run ?un_member_states_2024 console). Let’s describe 4 variables selected UN_data_ch5:iso: identification variable used distinguish 181 countries filtered dataset.fert_rate: numerical variable representing country’s fertility rate 2022 corresponding expected number children born per woman child-bearing years. outcome variable \\(y\\) interest.life_exp: numerical variable representing country’s average life expectancy 2022 years. primary explanatory variable \\(x\\) interest.obes_rate: numerical variable representing country’s obesity rate 2016. another explanatory variable \\(x\\) use Learning check end subsection.alternative way look raw data values choosing random sample rows UN_data_ch5 piping slice_sample() function dplyr package. set n argument 5, indicating want random sample 5 rows. display results Table 5.1. Note due random nature sampling, likely end different subset 5 rows.\nTABLE 5.1: random sample 5 193 total countries (181 without missing data)\nlooked raw values UN_data_ch5 data frame got preliminary sense data. can now compute summary statistics. start computing mean median numerical outcome variable fert_rate numerical explanatory variable life_exp. using summarize() function dplyr along mean() median() summary functions saw Section 3.3.However, want summary statistics well, standard deviation (measure spread), minimum maximum values, various percentiles?Typing summary statistic functions summarize() long tedious. Instead, use convenient tidy_summary() function moderndive package. function takes data frame, summarizes , returns commonly used summary statistics tidy format. take UN_data_ch5 data frame, select() outcome explanatory variables fert_rate life_exp, pipe tidy_summary function:can also directly providing columns ’d like summary inside tidy_summary() function:return results numerical variables fert_rate life_exp:column: name column summarizedn: number non-missing valuesgroup: NA (missing) numerical columns, break categorical variable levelstype: type column (numeric, character, factor, logical)min: minimum valueQ1: 1st quartile: value 25% observations smaller (25th percentile)mean: average value measuring central tendencymedian: 2nd quartile: value 50% observations smaller (50th percentile)Q3: 3rd quartile: value 75% observations smaller (75th percentile)max: maximum valuesd: standard deviation value measuring spreadLooking output, can see values variables distribute. example, median fertility rate 2, whereas median life expectancy 75.14 years. middle 50% fertility rates 1.6 3.2 (first third quartiles), middle 50% life expectancies 69.36 78.31.tidy_summary() function returns known univariate summary statistics: functions take single variable return numerical summary variable. However, also exist bivariate summary statistics: functions take two variables return summary two variables.particular, two variables numerical, can compute correlation coefficient. Generally speaking, coefficients quantitative expressions specific phenomenon. correlation coefficient measures strength linear relationship two numerical variables. value goes -1 1 :-1 indicates perfect negative relationship:\none variable increases, value variable tends go , following straight line.0 indicates relationship:\nvalues variables go /independently .+1 indicates perfect positive relationship:\nvalue one variable goes , value variable tends go well linear fashion.Figure 5.1 gives examples nine different correlation coefficient values hypothetical numerical variables \\(x\\) \\(y\\).\nFIGURE 5.1: Nine different correlation coefficients.\nexample, observe top right plot correlation coefficient -0.75 negative linear relationship \\(x\\) \\(y\\), strong negative linear relationship \\(x\\) \\(y\\) correlation coefficient -0.9 -1.correlation coefficient can computed using get_correlation() function moderndive package. case, inputs function two numerical variables want calculate correlation coefficient.put name outcome variable left-hand side ~ “tilde” sign, putting name explanatory variable right-hand side. known R’s formula notation. use “formula” syntax regression later chapter.alternative way compute correlation use cor() summary function within summarize():case, correlation coefficient -0.812 indicates relationship fertility rate life expectancy “moderately negative.” certain amount subjectivity interpreting correlation coefficients, especially close extreme values -1, 0, 1. develop intuition correlation coefficients, play “Guess Correlation” 1980’s style video game mentioned Subsection 5.4.1.now perform last step EDA: creating data visualizations. Since fert_rate life_exp variables numerical, scatterplot appropriate graph visualize data. using geom_point() display result Figure 5.2. Furthermore, set alpha value 0.1 check overplotting.\nFIGURE 5.2: Scatterplot relationship life expectancy fertility rate.\nsee much overplotting due little overlap points. life expectancy entries appear fall 70 80 years, fertility rate entries fall 1.5 3.5 births. Furthermore, opinions may vary, opinion relationship fertility rate life expectancy “moderately negative.” consistent earlier computed correlation coefficient -0.812.build scatterplot Figure 5.2 adding “best-fitting” line: possible lines can draw scatterplot, line “best” fits cloud points. adding new geom_smooth(method = \"lm\", se = FALSE) layer ggplot() code created scatterplot Figure 5.2. method = \"lm\" argument sets line “linear model.” se = FALSE argument suppresses standard error uncertainty bars. (’ll define concept standard error later Subsection 7.3.4.)\nFIGURE 5.3: Scatterplot life expectancy fertility rate regression line.\nline resulting Figure 5.3 called “regression line.” regression line visual summary relationship two numerical variables, case outcome variable fert_rate explanatory variable life_exp. negative slope blue line consistent earlier observed correlation coefficient -0.812 suggesting negative relationship two variables: country’s population higher life expectancy tends lower fertility rate. ’ll see later, however, correlation coefficient slope regression line always sign (positive negative), typically value.Furthermore, regression line “best-fitting” minimizes mathematical criteria. present mathematical criteria Subsection 5.3.2, suggest read subsection first reading rest section regression one numerical explanatory variable.\nLearning check\n(LC5.1) Conduct new exploratory data analysis outcome variable \\(y\\) fert_rate obes_rate new explanatory variable \\(x\\). Remember, involves three things:Looking raw data values.Computing summary statistics.Creating data visualizations.can say relationship obesity rate fertility rate based exploration?(LC5.2)\nmain purpose performing exploratory data analysis (EDA) fitting regression model?. predict future values.B. understand relationship variables detect potential issues.C. create variables.D. generate random samples.(LC5.3) following correct correlation coefficient?. ranges -2 2.B. measures strength non-linear relationships.C. ranges -1 1 measures strength linear relationships.D. always zero.","code":"\nUN_data_ch5 <- un_member_states_2024 |>\n  select(iso, \n         life_exp = life_expectancy_2022, \n         fert_rate = fertility_rate_2022, \n         obes_rate = obesity_rate_2016)|>\n  na.omit()\nglimpse(UN_data_ch5)Rows: 181\nColumns: 4\n$ iso       <chr> \"AFG\", \"ALB\", \"DZA\", \"AGO\", \"ATG\", \"ARG\", \"ARM\", \"AUS\", \"AUT\", \"AZE\", \"BHS\", \"BHR\", \"BGD\", \"BRB\", \"B…\n$ life_exp  <dbl> 53.6, 79.5, 78.0, 62.1, 77.8, 78.3, 76.1, 83.1, 82.3, 74.2, 76.1, 79.9, 74.7, 78.5, 74.3, 81.9, 75.8…\n$ fert_rate <dbl> 4.3, 1.4, 2.7, 5.0, 1.6, 1.9, 1.6, 1.6, 1.5, 1.6, 1.4, 1.8, 1.9, 1.6, 1.5, 1.6, 2.0, 4.7, 1.4, 2.5, …\n$ obes_rate <dbl> 5.5, 21.7, 27.4, 8.2, 18.9, 28.3, 20.2, 29.0, 20.1, 19.9, 31.6, 29.8, 3.6, 23.1, 24.5, 22.1, 24.1, 9…\nUN_data_ch5 |>\n  slice_sample(n = 5)\nUN_data_ch5 |>\n  summarize(mean_life_exp = mean(life_exp), \n            mean_fert_rate = mean(fert_rate),\n            median_life_exp = median(life_exp), \n            median_fert_rate = median(fert_rate))\nUN_data_ch5 |> \n  select(fert_rate, life_exp) |> \n  tidy_summary()\nUN_data_ch5 |> \n  tidy_summary(columns = c(fert_rate, life_exp))\nUN_data_ch5 |> \n  get_correlation(formula = fert_rate ~ life_exp)# A tibble: 1 × 1\n     cor\n   <dbl>\n1 -0.812\nUN_data_ch5 |> \n  summarize(correlation = cor(fert_rate, life_exp))\nggplot(UN_data_ch5, \n       aes(x = life_exp, y = fert_rate)) +\n  geom_point(alpha = 0.1) +\n  labs(x = \"Life Expectancy\", y = \"Fertility Rate\")\nggplot(UN_data_ch5, aes(x = life_exp, y = fert_rate)) +\n  geom_point(alpha = 0.1) +\n  labs(x = \"Life Expectancy\", \n    y = \"Fertility Rate\",\n    title = \"Relationship of life expectancy and fertility rate\") +\n  geom_smooth(method = \"lm\", se = FALSE)"},{"path":"regression.html","id":"model1table","chapter":"5 Simple Linear Regression","heading":"5.1.2 Simple linear regression","text":"may recall secondary/high school algebra equation line \\(y = + b\\cdot x\\). (Note \\(\\cdot\\) symbol equivalent \\(\\times\\) “multiply ” mathematical symbol. ’ll use \\(\\cdot\\) symbol rest book succinct.) defined two coefficients \\(\\) \\(b\\). intercept coefficient \\(\\) value \\(y\\) \\(x = 0\\). slope coefficient \\(b\\) \\(x\\) increase \\(y\\) every increase one \\(x\\). also called “rise run.”However, defining regression line like one Figure 5.3, use slightly different notation: equation regression line \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) . intercept coefficient \\(b_0\\), \\(b_0\\) value \\(\\widehat{y}\\) \\(x = 0\\). slope coefficient \\(x\\) \\(b_1\\), .e., increase \\(\\widehat{y}\\) every increase one \\(x\\). put “hat” top \\(y\\)? ’s form notation commonly used regression indicate “fitted value,” value \\(y\\) regression line given \\(x\\) value discussed Subsection 5.1.3.know regression line Figure 5.3 negative slope \\(b_1\\) corresponding explanatory \\(x\\) variable life_exp. ? countries tend higher life_exp values, tend lower fert_rate values. However, numerical value slope \\(b_1\\)? intercept \\(b_0\\)? compute two values hand, rather use computer!can obtain values intercept \\(b_0\\) slope life_exp \\(b_1\\) outputting linear regression coefficients. done two steps:first “fit” linear regression model using lm() function save demographics_model.get regression coefficients applying coef() demographics_model.first focus interpreting regression coefficients, later revisit code produced . coefficients intercept \\(b_0 = 12.599\\) slope \\(b_1 = -0.137\\) life_exp. Thus equation regression line Figure 5.3 follows:\\[\n\\begin{aligned}\n\\widehat{y} &= b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{fertility}\\_\\text{rate}} &= b_0 + b_{\\text{life}\\_\\text{expectancy}} \\cdot \\text{life}\\_\\text{expectancy}\\\\\n&= 12.599 + (-0.137) \\cdot \\text{life}\\_\\text{expectancy}\n\\end{aligned}\n\\]intercept \\(b_0\\) = 12.599 average fertility rate \\(\\widehat{y}\\) = \\(\\widehat{\\text{fertility}\\_\\text{rate}}\\) countries life_exp 0. graphical terms, line intersects \\(y\\) axis \\(x\\) = 0. Note, however, intercept regression line mathematical interpretation, practical interpretation , since observing life_exp 0 impossible. Furthermore, looking scatterplot regression line Figure 5.3, countries life expectancy anywhere near 0.greater interest slope \\(b_{\\text{life}\\_\\text{expectancy}}\\) life_exp -0.137. summarizes relationship fertility rate life expectancy variables. Note sign negative, suggesting negative relationship two variables. means countries higher life expectancies tend lower fertility rates. Recall correlation coefficient -0.812. negative sign, different value. Recall also correlation’s interpretation “strength linear association.” slope’s interpretation little different:every increase 1 unit life_exp, associated decrease , average, 0.137 units fert_rate.state associated increase necessarily causal increase. Perhaps may higher life expectancies directly cause lower fertility rates. Instead, wealthier countries tend stronger educational backgrounds, improved health, higher standard living, lower fertility rates, time wealthy countries also tend higher life expectancies. Just two variables strongly associated, necessarily mean one causes . summed often-quoted phrase, “correlation necessarily causation.” discuss idea Subsection 5.3.1.Furthermore, say associated decrease average 0.137 units fert_rate, might two countries whose life_exp values differ 1 unit, difference fertility rates may exactly \\(-0.137\\). slope \\(-0.137\\) saying across possible countries, average difference fertility rate two countries whose life expectancies differ one \\(-0.137\\).Now learned compute equation regression line Figure 5.3 using model coefficient values interpret resulting intercept slope, revisit code generated coefficients:First, “fit” linear regression model data using lm() function save demographics_model. say “fit,” mean “find best fitting line data.” lm() stands “linear model” used lm(y ~ x, data = df_name) :y outcome variable, followed tilde ~. case, y set fert_rate.x explanatory variable. case, x set life_exp.combination y ~ x called model formula. (Note order y x.) case, model formula fert_rate ~ life_exp. saw model formulas earlier get_correlation() function Subsection 5.1.1.df_name name data frame contains variables y x. case, data UN_data_ch5 data frame.Second, take saved model demographics_model apply coef() function obtain regression coefficients. gives us components regression equation line: intercept \\(b_0\\) slope \\(b_1\\).\nLearning check\n(LC5.4) Fit simple linear regression using lm(fert_rate ~ obes_rate, data = UN_data_ch5) obes_rate new explanatory variable \\(x\\). Learn “best-fitting” line regression coefficients applying coef() function. regression results match earlier exploratory data analysis?(LC5.5) intercept term \\(b_0\\) represent simple linear regression?. change outcome one-unit change explanatory variable.B. predicted value outcome explanatory variable zero.C. standard error regression.D. correlation outcome explanatory variables.(LC5.6) best describes “slope” simple linear regression line?. increase explanatory variable one-unit increase outcome.B. average explanatory variable.C. change outcome one-unit increase explanatory variable.D. minimum value outcome variable.(LC5.7) negative slope simple linear regression indicate?. outcome variable decreases explanatory variable increases.B. explanatory variable remains constant outcome variable increases.C. correlation coefficient zero.D. outcome variable increases explanatory variable increases.","code":"\n# Fit regression model:\ndemographics_model <- lm(fert_rate ~ life_exp, data = UN_data_ch5)\n# Get regression coefficients\ncoef(demographics_model)\n# Fit regression model:\ndemographics_model <- lm(fert_rate ~ life_exp, data = UN_data_ch5)\n# Get regression coefficients:\ncoef(demographics_model)"},{"path":"regression.html","id":"model1points","chapter":"5 Simple Linear Regression","heading":"5.1.3 Observed/fitted values and residuals","text":"just saw get value intercept slope regression line output coef() function. Now instead say want information individual observations. example, focus 21st 181 countries UN_data_ch5 data frame Table 5.2. corresponds UN member state Bosnia Herzegovina (BIH).\nTABLE 5.2: Data 21st country 193\nvalue \\(\\widehat{y}\\) regression line corresponding country’s life_exp value 77.98? Figure 5.4 mark three values corresponding results Bosnia Herzegovina give statistical names:Circle: observed value \\(y\\) = 1.3 country’s actual fertility rate.Square: fitted value \\(\\widehat{y}\\) value regression line \\(x =  \\texttt{life\\_exp} = 77.98\\), computed intercept slope previous regression table:\\[\\widehat{y} = b_0 + b_1 \\cdot x = 12.599 + (-0.137) \\cdot 77.98 = 1.894\\]\n* Arrow: length arrow residual computed subtracting fitted value \\(\\widehat{y}\\) observed value \\(y\\). residual can thought model’s error “lack fit” particular observation. case country, \\(y - \\widehat{y} = 1.3 - 1.894 = -0.594\\).\nFIGURE 5.4: Example observed value, fitted value, residual.\nNow say want compute fitted value \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) residual \\(y - \\widehat{y}\\) 181 UN member states complete data 2024. Recall country corresponds one 181 rows UN_data_ch5 data frame also one 181 points regression plot Figure 5.4.repeat previous calculations performed hand 181 times, tedious time consuming. Instead, use computer get_regression_points() function. apply get_regression_points() function demographics_model, saved lm() model previous section. Table 5.3 present results 21st 24th courses brevity.TABLE 5.3: Regression points (21st 24th countries)function example known computer programming wrapper function. takes pre-existing functions “wraps” single function hides inner workings. concept illustrated Figure 5.5.\nFIGURE 5.5: concept wrapper function.\nneed worry inputs look like outputs look like; leave details “hood car.” regression modeling example, get_regression_points() function takes saved lm() linear regression model input returns data frame regression predictions output. interested learning get_regression_points() function’s inner workings, check Subsection 5.3.3.inspect individual columns match elements Figure 5.4:fert_rate column represents observed outcome variable \\(y\\). y-position 181 black points.life_exp column represents values explanatory variable \\(x\\). x-position 181 black points.fert_rate_hat column represents fitted values \\(\\widehat{y}\\). corresponding value regression line 181 \\(x\\) values.residual column represents residuals \\(y - \\widehat{y}\\). 181 vertical distances 181 black points regression line.Just 21st country UN_data_ch5 dataset (first row table), repeat calculations 24th country (fourth row Table 5.3). corresponds country Brunei (BRN):fert_rate \\(= 1.7\\) observed fert_rate \\(y\\) country.life_exp \\(= 80.590\\) value explanatory variable life_exp \\(x\\) Brunei.fert_rate_hat \\(= 1.535 = 12.599 + (-0.137) \\cdot 80.590\\) fitted value \\(\\widehat{y}\\) regression line country.residual \\(= 0.165 = 1.7 - 1.535\\) value residual country. words, model’s fitted value 0.165 fertility rate units Brunei.like, can skip ahead Subsection 5.3.2 learn processes behind makes “best-fitting” regression lines. primer, “best-fitting” line refers line minimizes sum squared residuals possible lines can draw points. Section 5.2, ’ll discuss another common scenario categorical explanatory variable numerical outcome variable.\nLearning check\n(LC5.8) “wrapper function” context statistical modeling R?. function directly fits regression model without using functions.B. function combines functions simplify complex operations provide user-friendly interface.C. function removes missing values dataset analysis.D. function handles categorical data regression models.(LC5.9) Generate data frame residuals Learning check model used obes_rate explanatory \\(x\\) variable.(LC5.10) following statements true regression line simple linear regression model?. regression line represents average outcome variable.B. regression line minimizes sum squared differences observed predicted values.C. regression line always slope zero.D. regression line useful correlation variables.","code":"\nregression_points <- get_regression_points(demographics_model)\nregression_points"},{"path":"regression.html","id":"model2","chapter":"5 Simple Linear Regression","heading":"5.2 One categorical explanatory variable","text":"unfortunate truth life expectancy across countries world. International development agencies interested studying differences life expectancy hopes identifying governments allocate resources address problem. section, explore differences life expectancy two ways:Differences continents: significant differences average life expectancy six populated continents world: Africa, North America, South America, Asia, Europe, Oceania?Differences within continents: life expectancy vary within world’s five continents? example, spread life expectancy among countries Africa larger spread life expectancy among countries Asia?answer questions, use updated version gapminder data frame visualized Figure 2.1 Subsection 2.1.2 grammar graphics. updated un_member_states_2024 data saw earlier chapter. included moderndive package international development statistics life expectancy, GDP per capita, population 193 countries years near 2024. use data basic regression , now using explanatory variable \\(x\\) categorical, opposed numerical explanatory variable model used previous Section 5.1:numerical outcome variable \\(y\\) (country’s life expectancy) andA single categorical explanatory variable \\(x\\) (continent country part ).explanatory variable \\(x\\) categorical, concept “best-fitting” regression line little different one saw previously Section 5.1 explanatory variable \\(x\\) numerical. study differences shortly Subsection 5.2.2, first conduct exploratory data analysis.","code":""},{"path":"regression.html","id":"model2EDA","chapter":"5 Simple Linear Regression","heading":"5.2.1 Exploratory data analysis","text":"data 193 countries can found un_member_states_2024 data frame included moderndive package. However, keep things simple, select() subset variables ’ll consider chapter focus rows missing values na.omit(). ’ll save data new data frame called gapminder2022:perform first common step exploratory data analysis: looking raw data values. can using RStudio’s spreadsheet viewer using glimpse() command introduced Subsection 1.4.3 exploring data frames:Observe Rows: 188 indicates 188 rows/observations gapminder2022, row corresponds one country. words, observational unit individual country. Furthermore, observe variable continent type <fct>, stands factor, R’s way encoding categorical variables.full description variables included un_member_states_2024 can found reading associated help file (run ?un_member_states_2024 console). However, fully describe 4 variables selected gapminder2022:country: identification variable type character/text used distinguish 188 countries dataset.life_exp: numerical variable country’s life expectancy birth. outcome variable \\(y\\) interest.continent: categorical variable five levels. “levels” correspond possible categories: Africa, Asia, Americas, Europe, Oceania. explanatory variable \\(x\\) interest.gdp_per_capita: numerical variable country’s GDP per capita US inflation-adjusted dollars ’ll use another outcome variable \\(y\\) Learning check end subsection.next look random sample three 188 countries Table 5.4.\nTABLE 5.4: Random sample 3 188 countries\nRandom sampling likely produce different subset 3 rows ’s shown. Now looked raw values gapminder2022 data frame got sense data, compute summary statistics. apply tidy_summary() moderndive package. Recall function takes data frame, summarizes , returns commonly used summary statistics. take gapminder2022 data frame, select() outcome explanatory variables life_exp continent, pipe tidy_summary() Table 5.5:\nTABLE 5.5: Summary life expectancy continent variables\ntidy_summary() output now reports summaries categorical variables numerical variables reviewed . Let’s focus just discussing results categorical factor variable continent:n: number non-missing entries groupgroup: Breaks categorical variable unique levels. variable, corresponding Africa, Asia, North South America, Europe, Oceania.type: data type variable. , factor.min sd: missing since calculating five-number summary, mean, standard deviation categorical variables doesn’t make sense.Turning attention summary statistics numerical variable life_exp, observe global median life expectancy 2022 75.14. Thus, half world’s countries (96 countries) life expectancy less 75.14. mean life expectancy 73.55 lower, however. mean life expectancy lower median?can answer question performing last three common steps exploratory data analysis: creating data visualizations. visualize distribution outcome variable \\(y\\) = life_exp Figure 5.6.\nFIGURE 5.6: Histogram life expectancy 2022.\nsee data left-skewed, also known negatively skewed: countries low life expectancy bringing mean life expectancy. However, median less sensitive effects outliers; hence, median greater mean case.Remember, however, want compare life expectancies continents within continents. words, visualizations need incorporate notion variable continent. can easily faceted histogram. Recall Section 2.6 facets allow us split visualization different values another variable. display resulting visualization Figure 5.7 adding facet_wrap(~ continent, nrow = 2) layer.\nFIGURE 5.7: Life expectancy 2022 continent (faceted).\nObserve unfortunately distribution African life expectancies much lower continents. Europe, life expectancies tend higher furthermore vary much. hand, Asia Africa variation life expectancies.Recall alternative method visualize distribution numerical variable split categorical variable using side--side boxplot. map categorical variable continent \\(x\\)-axis different life expectancies within continent \\(y\\)-axis Figure 5.8.\nFIGURE 5.8: Life expectancy 2022 continent (boxplot).\npeople prefer comparing distributions numerical variable different levels categorical variable using boxplot instead faceted histogram. can make quick comparisons categorical variable’s levels imaginary horizontal lines. example, observe Figure 5.8 can quickly convince Europe highest median life expectancies drawing imaginary horizontal line near \\(y = 81\\). Furthermore, observed faceted histogram Figure 5.7, Africa Asia largest variation life expectancy evidenced large interquartile ranges (size boxes).’s important remember, however, solid lines middle boxes correspond medians (middle value) rather mean (average). , example, look Asia, solid line denotes median life expectancy around 75 years. tells us half countries Asia life expectancy 75 years, whereas half life expectancy 75 years. compute median mean life expectancy continent little data wrangling display results Table 5.6.\nTABLE 5.6: Life expectancy continent\nObserve order second column median life expectancy: Africa lowest, Europe highest, others similar medians Africa Europe. ordering corresponds ordering solid black lines inside boxes side--side boxplot Figure 5.8.now turn attention values third column mean. Using Africa’s mean life expectancy 66.31 baseline comparison, start making comparisons mean life expectancies four continents put values Table 5.7, ’ll revisit later section.Asia, 74.95 - 66.31 = 8.64 years higher.Europe, 79.91 - 66.31 = 13.6 years higher.North America, 76.29 - 66.31 = 9.98 years higher.Oceania, 74.42 - 66.31 = 8.11 years higher.South America, 75.23 - 66.31 = 8.92 years higher.\nTABLE 5.7: Mean life expectancy continent relative differences mean Africa\n\nLearning check\n(LC5.11) Conduct new exploratory data analysis explanatory variable \\(x\\) continent gdp_per_capita new outcome variable \\(y\\). can say differences GDP per capita continents based exploration?(LC5.12) using categorical explanatory variable regression, baseline group represent?. group highest meanB. group chosen comparison groupsC. group data pointsD. group lowest standard deviation","code":"\ngapminder2022 <- un_member_states_2024 |>\n  select(country, life_exp = life_expectancy_2022, continent, gdp_per_capita) |> \n  na.omit()\nglimpse(gapminder2022)Rows: 188\nColumns: 4\n$ country        <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\", \"Argentina\", \"…\n$ life_exp       <dbl> 53.6, 79.5, 78.0, 83.4, 62.1, 77.8, 78.3, 76.1, 83.1, 82.3, 74.2, 76.1, 79.9, 74.7, 78.5, 74.3,…\n$ continent      <fct> Asia, Europe, Africa, Europe, Africa, North America, South America, Asia, Oceania, Europe, Asia…\n$ gdp_per_capita <dbl> 356, 6810, 4343, 41993, 3000, 19920, 13651, 7018, 65100, 52085, 7762, 31458, 30147, 2688, 20239…\ngapminder2022 |> sample_n(size = 3)\ngapminder2022 |> select(life_exp, continent) |> tidy_summary()\nggplot(gapminder2022, aes(x = life_exp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  labs(x = \"Life expectancy\", \n       y = \"Number of countries\",\n       title = \"Histogram of distribution of worldwide life expectancies\")\nggplot(gapminder2022, aes(x = life_exp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  labs(x = \"Life expectancy\", \n       y = \"Number of countries\",\n       title = \"Histogram of distribution of worldwide life expectancies\") +\n  facet_wrap(~ continent, nrow = 2)\nggplot(gapminder2022, aes(x = continent, y = life_exp)) +\n  geom_boxplot() +\n  labs(x = \"Continent\", y = \"Life expectancy\",\n       title = \"Life expectancy by continent\")\nlife_exp_by_continent <- gapminder2022 |>\n  group_by(continent) |>\n  summarize(median = median(life_exp), mean = mean(life_exp))\nlife_exp_by_continent"},{"path":"regression.html","id":"model2table","chapter":"5 Simple Linear Regression","heading":"5.2.2 Linear regression","text":"Subsection 5.1.2 introduced simple linear regression, involves modeling relationship numerical outcome variable \\(y\\) numerical explanatory variable \\(x\\). life expectancy example, now instead categorical explanatory variable continent. model yield “best-fitting” regression line like Figure 5.3, rather offsets relative baseline comparison.Subsection 5.1.2 studying relationship fertility rates life expectancy, output regression coefficients model. Recall done two steps:“fit” linear regression model using lm(y ~ x, data) save life_exp_model.get regression coefficients applying coef() function life_exp_model.focus values coefficient values. now 6 entries? break one one:intercept corresponds mean life expectancy countries Africa 66.31 years.intercept corresponds mean life expectancy countries Africa 66.31 years.continentAsia corresponds countries Asia value +8.64 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Asia \\(66.31 + 8.64 = 74.95\\).continentAsia corresponds countries Asia value +8.64 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Asia \\(66.31 + 8.64 = 74.95\\).continentEurope corresponds countries Europe value +13.6 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Europe \\(66.31 + 13.6 = 79.91\\).continentEurope corresponds countries Europe value +13.6 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Europe \\(66.31 + 13.6 = 79.91\\).continentNorth America corresponds countries North America value +9.98 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries North America \\(66.31 + 9.98 = 76.29\\).continentNorth America corresponds countries North America value +9.98 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries North America \\(66.31 + 9.98 = 76.29\\).continentOceania corresponds countries Oceania value +8.11 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Oceania \\(66.31 + 8.11 = 74.42\\).continentOceania corresponds countries Oceania value +8.11 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries Oceania \\(66.31 + 8.11 = 74.42\\).continentSouth America corresponds countries South America value +8.92 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries South America \\(66.31 + 8.92 = 75.23\\).continentSouth America corresponds countries South America value +8.92 difference mean life expectancy relative Africa displayed Table 5.7. words, mean life expectancy countries South America \\(66.31 + 8.92 = 75.23\\).summarize, 6 values regression coefficients correspond “baseline comparison” continent Africa (intercept) well five “offsets” baseline remaining 5 continents: Asia, Europe, North America, Oceania, South America.might asking point Africa chosen “baseline comparison” group. case reason comes first alphabetically six continents; default R arranges factors/categorical variables alphanumeric order. can change baseline group another continent manipulate variable continent’s factor “levels” using forcats package. See Chapter 15 R Data Science (Grolemund Wickham 2017) examples.now write equation fitted values \\(\\widehat{y} = \\widehat{\\text{life exp}}\\).\\[\n\\begin{aligned}\n\\widehat{y} = \\widehat{\\text{life exp}} &= b_0 + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + b_{\\text{Europe}}\\cdot\\mathbb{1}_{\\text{Europe}}(x) \\\\\n& \\qquad + b_{\\text{North America}}\\cdot\\mathbb{1}_{\\text{North America}}(x) + b_{\\text{Oceania}}\\cdot\\mathbb{1}_{\\text{Oceania }}(x) \\\\\n& \\qquad + b_{\\text{South America}}\\cdot\\mathbb{1}_{\\text{South America}}(x)\\\\\n&= 66.31 + 8.64\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 13.6\\cdot\\mathbb{1}_{\\text{Euro}}(x)  \\\\\n& \\qquad + 9.98\\cdot\\mathbb{1}_{\\text{North America}}(x) + 8.11\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + 8.92\\cdot\\mathbb{1}_{\\text{South America}}(x)\n\\end{aligned}\n\\]Whoa! looks daunting! Don’t fret, however, understand elements mean, things simplify greatly. First, \\(\\mathbb{1}_{}(x)\\) ’s known mathematics “indicator function.” returns one two possible values, 0 1, \\[\n\\mathbb{1}_{}(x) = \\left\\{\n\\begin{array}{ll}\n1 & \\text{} x \\text{ } \\\\\n0 & \\text{} \\text{otherwise} \\end{array}\n\\right.\n\\]statistical modeling context, also known dummy variable. case, consider first indicator variable \\(\\mathbb{1}_{\\text{Amer}}(x)\\). indicator function returns 1 country Asia, 0 otherwise:\\[\n\\mathbb{1}_{\\text{Amer}}(x) = \\left\\{\n\\begin{array}{ll}\n1 & \\text{} \\text{country } x \\text{ Asia} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\n\\]Second, \\(b_0\\) corresponds intercept ; case, mean life expectancy countries Africa. Third, \\(b_{\\text{Asia}}\\), \\(b_{\\text{Europe}}\\), \\(b_{\\text{North America}}\\), \\(b_{\\text{Oceania}}\\), \\(b_{\\text{South America}}\\) represent 5 “offsets relative baseline comparison” regression coefficients.put together compute fitted value \\(\\widehat{y} = \\widehat{\\text{life exp}}\\) country Africa. Since country Africa, five indicator functions \\(\\mathbb{1}_{\\text{Asia}}(x)\\), \\(\\mathbb{1}_{\\text{Europe}}(x)\\), \\(\\mathbb{1}_{\\text{North America}}(x)\\), \\(\\mathbb{1}_{\\text{Oceania}}(x)\\), \\(\\mathbb{1}_{\\text{South America}}(x)\\) equal 0, thus:\\[\n\\begin{aligned}\n\\widehat{\\text{life exp}} &= b_0 + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ b_{\\text{Europe}}\\cdot\\mathbb{1}_{\\text{Europe}}(x) \\\\\n& \\qquad  + b_{\\text{North America}}\\cdot\\mathbb{1}_{\\text{North America}}(x) +  b_{\\text{Oceania}}\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + b_{\\text{South America}}\\cdot\\mathbb{1}_{\\text{South America}}(x) \\\\\n&= 66.31 + 8.64\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ 13.6\\cdot\\mathbb{1}_{\\text{Europe}}(x)\\\\\n& \\qquad + 9.98\\cdot\\mathbb{1}_{\\text{North America}}(x) + 8.11\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + 8.92\\cdot\\mathbb{1}_{\\text{South America}}(x)\\\\\n&= 66.31 + 8.64\\cdot 0 + 13.6\\cdot 0 + 9.98\\cdot 0 + 8.11\\cdot 0 + 8.92\\cdot 0\\\\\n&= 66.31\n\\end{aligned}\n\\]words, left intercept \\(b_0\\), corresponding average life expectancy African countries 66.31 years. Next, say considering country Asia. case, indicator function \\(\\mathbb{1}_{\\text{Asia}}(x)\\) Asia equal 1, others equal 0, thus:\\[\n\\begin{aligned}\n\\widehat{\\text{life exp}} &= b_0 + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ b_{\\text{Europe}}\\cdot\\mathbb{1}_{\\text{Europe}}(x)\\\\\n& \\qquad + b_{\\text{North America}}\\cdot\\mathbb{1}_{\\text{North America}}(x) +  b_{\\text{Oceania}}\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + b_{\\text{South America}}\\cdot\\mathbb{1}_{\\text{South America}}(x) \\\\\n&= 66.31 + 8.64\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ 13.6\\cdot\\mathbb{1}_{\\text{Europe}}(x) \\\\\n& \\qquad + 9.98\\cdot\\mathbb{1}_{\\text{North America}}(x) + 8.11\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + 8.92\\cdot\\mathbb{1}_{\\text{South America}}(x)\\\\\n&= 66.31 + 8.64\\cdot 1 + 13.6\\cdot 0 + 9.98\\cdot 0 + 8.11\\cdot 0 + 8.92\\cdot 0\\\\\n&= 66.31 + 8.64 \\\\\n& = 74.95\n\\end{aligned}\n\\]mean life expectancy countries Asia 74.95 years Table 5.7. Note “offset baseline comparison” +8.64 years.one . Say considering country South America. case, indicator function \\(\\mathbb{1}_{\\text{South America}}(x)\\) South America equal 1, others equal 0, thus:\\[\n\\begin{aligned}\n\\widehat{\\text{life exp}} &= b_0 + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ b_{\\text{Europe}}\\cdot\\mathbb{1}_{\\text{Europe}}(x) \\\\\n& \\qquad + b_{\\text{North America}}\\cdot\\mathbb{1}_{\\text{North America}}(x) +  b_{\\text{Oceania}}\\cdot\\mathbb{1}_{\\text{Oceania}}(x) \\\\\n& \\qquad + b_{\\text{South America}}\\cdot\\mathbb{1}_{\\text{South America}}(x) \\\\\n&= 66.31 + 8.64\\cdot\\mathbb{1}_{\\text{Asia}}(x)\n+ 13.6\\cdot\\mathbb{1}_{\\text{Europe}}(x)\\\\\n& \\qquad + 9.98\\cdot\\mathbb{1}_{\\text{North America}}(x) + 8.11\\cdot\\mathbb{1}_{\\text{Oceania}}(x) + 8.92\\cdot\\mathbb{1}_{\\text{South America}}(x)\\\\\n&= 66.31 + 8.64\\cdot 0 + 13.6\\cdot 0 + 9.98\\cdot 0 + 8.11\\cdot 0 + 8.92\\cdot 1\\\\\n&= 66.31 + 8.92 \\\\\n& = 75.23\n\\end{aligned}\n\\]mean life expectancy South American countries 75.23 years Table 5.7. “offset baseline comparison” +8.64 years.generalize idea bit. fit linear regression model using categorical explanatory variable \\(x\\) \\(k\\) possible categories, regression table return intercept \\(k - 1\\) “offsets.” case, since \\(k = 6\\) continents, regression model returns intercept corresponding baseline comparison group Africa \\(k - 1 = 5\\) offsets corresponding Asia, Europe, North America, Oceania, South America.Understanding regression table output using categorical explanatory variable topic new regression often struggle . real remedy struggles practice, practice, practice. However, equip understanding create regression models using categorical explanatory variables, ’ll able incorporate many new variables models, given large amount world’s data categorical.\nLearning check\n(LC5.13) Fit linear regression using lm(gdp_per_capita ~ continent, data = gapminder2022) gdp_per_capita new outcome variable. Get information “best-fitting” line regression coefficients. regression results match results previous exploratory data analysis?(LC5.14) many “offsets” differences baseline regression model output categorical variable 4 levels?. 1B. 2C. 3D. 4","code":"\nlife_exp_model <- lm(life_exp ~ continent, data = gapminder2022)\ncoef(life_exp_model)           (Intercept)          continentAsia        continentEurope continentNorth America       continentOceania \n                 66.31                   8.64                  13.60                   9.99                   8.11 \ncontinentSouth America \n                  8.92 "},{"path":"regression.html","id":"model2points","chapter":"5 Simple Linear Regression","heading":"5.2.3 Observed/fitted values and residuals","text":"Recall Subsection 5.1.3, defined following three concepts:Observed values \\(y\\), observed value outcome variable Fitted values \\(\\widehat{y}\\), value regression line given \\(x\\) valueResiduals \\(y - \\widehat{y}\\), error observed value fitted valueWe obtained values values using get_regression_points() function moderndive package. time, however, add argument setting ID = \"country\", uses variable country gapminder2022 identification variable output. help contextualize analysis matching values countries.\nTABLE 5.8: Regression points (Sample 6 142 countries)\nObserve Table 5.8 life_exp_hat contains fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{life exp}}\\). look closely, 5 possible values life_exp_hat. correspond five mean life expectancies 5 continents displayed Table 5.7 computed using regression coefficient values.residual column simply \\(y - \\widehat{y}\\) = life_exp - life_exp_hat. values can interpreted deviation country’s life expectancy continent’s average life expectancy. example, observe first row Table 5.8 corresponding Afghanistan. residual \\(y - \\widehat{y} = 53.6 - 74.95 = -21.4\\) refers Afghanistan’s life expectancy 21.4 years lower mean life expectancy Asian countries. partly explained years war country suffered.\nLearning check\n(LC5.15) interpretation correct positive coefficient regression model categorical explanatory variable?. indicates baseline group.B. represents mean value baseline group.C. corresponding group higher response mean baseline’s.D. corresponding group lower response mean baseline’s.(LC5.16) following statements residuals regression true?. Residuals differences fitted observed response values.B. Residuals always positive.C. Residuals important model evaluation.D. Residuals predicted values model.(LC5.17) Using either sorting functionality RStudio’s spreadsheet viewer using data wrangling tools learned Chapter 3, identify five countries five smallest (negative) residuals? negative residuals say life expectancy relative continents’ life expectancy?(LC5.18) Repeat process, identify five countries five largest (positive) residuals. positive residuals say life expectancy relative continents’ life expectancy?","code":"\nget_regression_points(life_exp_model, ID = \"country\")"},{"path":"regression.html","id":"reg-related-topics","chapter":"5 Simple Linear Regression","heading":"5.3 Related topics","text":"","code":""},{"path":"regression.html","id":"correlation-is-not-causation","chapter":"5 Simple Linear Regression","heading":"5.3.1 Correlation is not necessarily causation","text":"Throughout chapter cautious interpreting regression slope coefficients. always discussed “associated” effect explanatory variable \\(x\\) outcome variable \\(y\\). example, statement Subsection 5.1.2 “every increase 1 unit life_exp, associated decrease average 0.137 units fert_rate.” include term “associated” extra careful suggest making causal statement. life_exp negatively correlated fert_rate, can’t necessarily make statements life expectancy’s direct causal effect fertility rates without information.another example depicted Figure 5.9: --great medical doctor goes medical records finds patients slept shoes tended wake headaches. doctor declares, “Sleeping shoes causes headaches!”\nFIGURE 5.9: sleeping shoes cause headaches?\nHowever, good chance someone sleeping shoes , potentially intoxicated alcohol. Furthermore, higher levels drinking leads hangovers, hence headaches. amount alcohol consumption ’s known confounding/lurking variable. “lurks” behind scenes, confounding causal relationship () “sleeping shoes ” “waking headache.” can summarize Figure 5.10 causal graph Y response variable X treatment variable whose causal effect interested . \nFIGURE 5.10: Causal graph.\nstudy relationship Y X, use regression model outcome variable set Y explanatory variable set X, ’ve throughout chapter. However, Figure 5.10 also includes third variable arrows pointing X Y:Z confounding variable affects X Y, thereby “confounding” relationship. confounding variable alcohol.Alcohol cause people likely sleep shoes well likely wake headache. Thus regression model relationship X Y also use Z explanatory variable. words, doctor needs take account drinking night . next chapter, ’ll start covering multiple regression models allow us incorporate one variable regression models.Establishing causation tricky problem frequently takes either carefully designed experiments methods control effects confounding variables. approaches attempt, best can, either take possible confounding variables account negate impact. allows researchers focus relationship interest: relationship outcome variable Y treatment variable X.read news stories, careful fall trap thinking correlation necessarily implies causation. Check Spurious Correlations website comical examples variables correlated, causally related.Coming back UN member states data, confounding variable level economic development country. affect life expectancy fertility rates. proxy looking level economic development Human Development Index (HDI). measures country’s average achievements three basic aspects human development: health (life expectancy), education (mean expected years schooling), standard living (gross national income per capita). stored hdi_2022 column un_member_states_2024. explore relationship life expectancy fertility rates:Looking scatterplots , see strong positive linear relationship Human Development Index (HDI) life expectancy, well strong negative correlation Human Development Index (HDI) fertility rates. correlation coefficients Human Development Index (HDI) life expectancy, well Human Development Index (HDI) fertility rates, also given. findings well additional understanding socioeconomic factors suggest Human Development Index (HDI) confounding variable relationship life expectancy fertility rates.","code":"\nggplot(data = un_member_states_2024, \n       aes(x = hdi_2022, y = life_expectancy_2022)) +\n  geom_point() +\n  labs(x = \"Human Development Index (HDI)\", y = \"Life Expectancy\")\nggplot(data = un_member_states_2024, \n       aes(x = hdi_2022, y = fertility_rate_2022)) +\n  geom_point() +\n  labs(x = \"Human Development Index (HDI)\", y = \"Fertility Rate\")\nun_member_states_2024 |> \n  get_correlation(life_expectancy_2022 ~ hdi_2022, na.rm = TRUE)# A tibble: 1 × 1\n    cor\n  <dbl>\n1 0.889\nun_member_states_2024 |> \n  get_correlation(fertility_rate_2022 ~ hdi_2022, na.rm = TRUE)# A tibble: 1 × 1\n     cor\n   <dbl>\n1 -0.849"},{"path":"regression.html","id":"leastsquares","chapter":"5 Simple Linear Regression","heading":"5.3.2 Best-fitting line","text":"Regression lines also known “best-fitting” lines. mean “best”? unpack criteria used regression determine “best.” Recall Figure 5.4, Bosnia Herzegovina marked observed value \\(y\\) circle, fitted value \\(\\widehat{y}\\) square, residual \\(y - \\widehat{y}\\) arrow. re-display Figure 5.4 top-left plot Figure 5.11 addition three arbitrarily chosen countries:\nFIGURE 5.11: Example observed value, fitted value, residual.\nfour plots refer :country Bosnia Herzegovina life expectancy \\(x = 77.98\\) fertility rate \\(y = 1.3\\). residual case \\(1.3 - 1.894 = -0.594\\), mark blue arrow top-left plot.addition Chad, life expectancy \\(x = 59.15\\) fertility rate \\(y = 6\\). residual case \\(6 - 4.479 = 1.521\\), mark new blue arrow top-right plot.addition India, life expectancy \\(x = 67.22\\) fertility rate \\(y = 2\\). residual case \\(2 - 3.371 = -1.371\\), mark new blue arrow bottom-left plot.addition Solomon Islands, life expectancy \\(x = 76.7\\) fertility rate \\(y = 3.8\\). residual case \\(3.8 - 2.069 = 1.731\\), mark new blue arrow bottom-right plot.Now say repeated process computing residuals 181 countries complete information, squared residuals, summed . call quantity sum squared residuals; measure lack fit model. Larger values sum squared residuals indicate bigger lack fit. corresponds worse-fitting model.regression line fits points perfectly, sum squared residuals 0. regression line fits points perfectly, fitted value \\(\\widehat{y}\\) equals observed value \\(y\\) cases, hence residual \\(y - \\widehat{y}\\) = 0 cases, sum even large number 0’s still 0.Furthermore, possible lines can draw cloud 181 points, regression line minimizes value. words, regression corresponding fitted values \\(\\widehat{y}\\) minimizes sum squared residuals:\\[\n\\sum_{=1}^{n}(y_i - \\widehat{y}_i)^2\n\\]use data-wrangling tools compute sum squared residuals exactly:straight line drawn figure yield sum squared residuals greater 81.265. mathematically guaranteed fact can prove using calculus linear algebra. ’s alternative names linear regression line best-fitting line least-squares line.square residuals (.e., arrow lengths)? positive negative deviations amount treated equally. (said, taking absolute value residuals also treat positive negative deviations amount equally, squaring residuals used reasons related calculus: taking derivatives minimizing functions. learn suggest consult textbook mathematical statistics.)\nLearning check\n(LC5.19) Note Figure 5.12 3 points marked dots :“best” fitting solid regression line blueAn arbitrarily chosen dotted red lineAnother arbitrarily chosen dashed green line\nFIGURE 5.12: Regression line two others.\nCompute sum squared residuals hand line. Show regression line blue smallest value three lines.","code":"\n# Fit regression model and regression points\ndemographics_model <- lm(fert_rate ~ life_exp, data = UN_data_ch5)\nregression_points <- get_regression_points(demographics_model)\n\n# Compute sum of squared residuals\nregression_points |>\n  mutate(squared_residuals = residual^2) |>\n  summarize(sum_of_squared_residuals = sum(squared_residuals))[1] 81.3"},{"path":"regression.html","id":"underthehood","chapter":"5 Simple Linear Regression","heading":"5.3.3 get_regression_x() functions","text":"Recall chapter introduced wrapper function moderndive package:get_regression_points() returns point--point information regression model Subsection 5.1.3.going behind scenes  get_regression_points() function? mentioned Subsection 5.1.2 example wrapper function. functions take pre-existing functions “wrap” single functions hide user inner workings. way user needs worry inputs look like outputs look like. subsection, ’ll “get hood” functions see “engine” wrapper functions works.get_regression_points() function wrapper function, returning information individual points involved regression model like fitted values, observed values, residuals. get_regression_points() uses augment() function broom package  produce data shown Table 5.9. Additionally, uses clean_names() janitor package (Firke 2024) clean variable names.\nTABLE 5.9: Regression points using augment() broom package\ncase, outputs variables interest students learning regression: outcome variable \\(y\\) (fert_rate), explanatory/predictor variables (life_exp), resulting fitted values \\(\\hat{y}\\) used applying equation regression line life_exp, residual \\(y - \\hat{y}\\).even curious wrapper functions work, take look source code functions GitHub.","code":"\nlibrary(broom)\nlibrary(janitor)\ndemographics_model |>\n  augment() |>\n  mutate_if(is.numeric, round, digits = 3) |>\n  clean_names() |>\n  select(-c(\"std_resid\", \"hat\", \"sigma\", \"cooksd\", \"std_resid\"))"},{"path":"regression.html","id":"reg-conclusion","chapter":"5 Simple Linear Regression","heading":"5.4 Conclusion","text":"","code":""},{"path":"regression.html","id":"additional-resources-basic-regression","chapter":"5 Simple Linear Regression","heading":"5.4.1 Additional resources","text":"R script file R code used chapter available .suggested Subsection 5.1.1, interpreting coefficients close extreme values -1, 0, 1 can somewhat subjective. help develop sense correlation coefficients, suggest play 80s-style video game called, “Guess Correlation,” http://guessthecorrelation.com/ previewed Figure 5.13.\nFIGURE 5.13: Preview “Guess Correlation” game.\n","code":""},{"path":"regression.html","id":"whats-to-come-4","chapter":"5 Simple Linear Regression","heading":"5.4.2 What’s to come?","text":"chapter, ’ve studied term simple linear regression, fit models one explanatory variable. Chapter 6, ’ll study multiple regression, regression models can now one explanatory variable moving little bit advanced basic form simple linear regression! particular, ’ll consider two scenarios: regression models one numerical one categorical explanatory variable regression models two numerical explanatory variables. allow construct sophisticated powerful models, hopes better explaining outcome variable \\(y\\).","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"6 Multiple Regression","heading":"6 Multiple Regression","text":"Chapter 5, studied simple linear regression \nmodel represents relationship two variables: outcome\nvariable response \\(y\\) explanatory variable regressor \\(x\\).\nFurthermore, keep things simple, considered models one\nexplanatory \\(x\\) variable either numerical Section\n5.1 categorical Section 5.2.chapter, introduce multiple linear regression, direct\nextension simple linear regression one explanatory\nvariable taken account explain changes outcome\nvariable. show next sections, much material\ndeveloped simple linear regression translates directly multiple\nlinear regression, interpretation associated effect \none explanatory variable must made taking account \nexplanatory variables included model.","code":""},{"path":"multiple-regression.html","id":"mult-reg-packages","chapter":"6 Multiple Regression","heading":"Needed packages","text":"needed, read Section 1.3 information \ninstall load R packages.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(ISLR2)"},{"path":"multiple-regression.html","id":"model4","chapter":"6 Multiple Regression","heading":"6.1 One numerical and one categorical explanatory variable","text":"continue using UN member states dataset introduced Section\n5.1. Recall studied relationship \noutcome variable fertility rate, \\(y\\), regressor life expectancy,\n\\(x\\).section, introduce one additional regressor model:\ncategorical variable income group four categories:\nLow income, Lower middle income, Upper middle income, \nHigh income. now want study fertility rate changes due \nchanges life expectancy different income levels. , \nuse multiple regression. Observe\nnow :numerical outcome variable \\(y\\), fertility rate given\ncountry state, andTwo explanatory variables:\nnumerical explanatory variable \\(x_1\\), life expectancy.\ncategorical explanatory variable \\(x_2\\), income group.\nnumerical explanatory variable \\(x_1\\), life expectancy.categorical explanatory variable \\(x_2\\), income group.","code":""},{"path":"multiple-regression.html","id":"model4EDA","chapter":"6 Multiple Regression","heading":"6.1.1 Exploratory data analysis","text":"UN member states data frame included moderndive package.\nkeep things simple, select() subset variables\nneeded , save data new data frame called\nUN_data_ch6. Note variables used different ones\nchosen Chapter 5. also set income variable\nfactor levels show expected order.Recall three common steps exploratory data analysis saw \nSubsection 5.1.1:Inspecting sample raw values.Computing summary statistics.Creating data visualizations.first look raw data values either looking UN_data_ch6\nusing RStudio’s spreadsheet viewer using glimpse() function\ndplyr package:variable country contains UN member states. R reads \nvariable character, <chr>, beyond country identification \nneeded analysis. variables life expectancy,\nlife_exp, fertility rate, fert_rate, numerical, \nvariable income, income, categorical. R, categorical variables\ncalled factors categories factor levels.also display random sample 10 rows 182 rows\ncorresponding different countries Table\n6.1. Remember due random nature \nsampling, likely end different subset 10 rows.\nTABLE 6.1: random sample 10 182 UN member states\nLife expectancy, life_exp, estimate many years, \naverage, person given country expected live. Fertility\nrate, fert_rate, average number live births per woman \nchildbearing age country. exploratory data\nanalyses Sections 5.1.1 5.2.1 Chapter\n5, find summary statistics:Recall row UN_data_ch6 represents particular country \nUN member state. tidy_summary() function shows summary \nnumerical variables life expectancy (life_exp), fertility rate\n(fert_rate), categorical variable income group (income).\nvariable numerical, tidy_summary() function provides\ntotal number observations data frame, five-number\nsummary, mean, standard deviation.example, first\nrow summary refers life expectancy life_exp. \n182 observations variable, numerical\nvariable, first quartile, Q1, 69.4; means \nlife expectancy 25% UN member states less 69.4 years.\nvariable dataset categorical, also called factor,\nsummary shows categories factor levels number \nobservations level. example, income group (income) \nfactor four levels: Low Income, Lower middle income,\nUpper middle income, High income. summary also provides \nnumber states factor level; observe, example, \ndataset 56 UN member states considered High Income\nstates.Furthermore, can compute correlation coefficient two\nnumerical variables: life_exp fert_rate. Recall Subsection\n5.1.1 correlation coefficients exist \nnumerical variables. observe “strongly negatively”\ncorrelated.ready create data visualizations, last exploratory\ndata analysis. Given outcome variable fert_rate \nexplanatory variable life_exp numerical, can create \nscatterplot display relationship, Figure\n5.2. time, incorporate categorical\nvariable income mapping variable color aesthetic,\nthereby creating colored scatterplot.\nFIGURE 6.1: Colored scatterplot life expectancy fertility rate.\nresulting Figure 6.1, observe \nggplot() assigns default color scheme points lines\nassociated four levels income: Low income,\nLower middle income, Upper middle income, High income.\nFurthermore, geom_smooth(method = \"lm\", se = FALSE) layer\nautomatically fits different regression line group.can see interesting trends. First, observe get \ndifferent line income group. Second, slopes \nincome groups negative. Third, slope High income group\nclearly less steep slopes three groups. ,\nchanges fertility rate due changes life expectancy \ndependent level income given country. Fourth, observe\nhigh-income countries , general, high life expectancy \nlow fertility rates.","code":"\nUN_data_ch6 <- un_member_states_2024 |>\n  select(country, \n         life_expectancy_2022, \n         fertility_rate_2022, \n         income_group_2024)|>\n  na.omit()|>\n  rename(life_exp = life_expectancy_2022, \n         fert_rate = fertility_rate_2022, \n         income = income_group_2024)|>\n  mutate(income = factor(income, \n                         levels = c(\"Low income\", \"Lower middle income\", \n                                    \"Upper middle income\", \"High income\")))\nglimpse(UN_data_ch6)Rows: 182\nColumns: 4\n$ country   <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\", \"Argentina\", \"Armenia\", \"Austr…\n$ life_exp  <dbl> 53.6, 79.5, 78.0, 62.1, 77.8, 78.3, 76.1, 83.1, 82.3, 74.2, 76.1, 79.9, 74.7, 78.5, 74.3, 81.9, 75.8…\n$ fert_rate <dbl> 4.3, 1.4, 2.7, 5.0, 1.6, 1.9, 1.6, 1.6, 1.5, 1.6, 1.4, 1.8, 1.9, 1.6, 1.5, 1.6, 2.0, 4.7, 1.4, 2.5, …\n$ income    <fct> Low income, Upper middle income, Lower middle income, Lower middle income, High income, Upper middle…\nUN_data_ch6 |> sample_n(size = 10)\nUN_data_ch6 |> \n  select(life_exp, fert_rate, income) |> \n  tidy_summary()\nUN_data_ch6 |> \n  get_correlation(formula = fert_rate ~ life_exp)# A tibble: 1 × 1\n     cor\n   <dbl>\n1 -0.815\nggplot(UN_data_ch6, aes(x = life_exp, y = fert_rate, color = income)) +\n  geom_point() +\n  labs(x = \"Life Expectancy\", y = \"Fertility Rate\", color = \"Income group\") +\n  geom_smooth(method = \"lm\", se = FALSE)"},{"path":"multiple-regression.html","id":"model4interactiontable","chapter":"6 Multiple Regression","heading":"6.1.2 Model with interactions","text":"can represent four regression lines Figure\n6.1 multiple regression model \ninteractions. , however, review linear regression one\ncategorical explanatory variable. Recall Subsection\n5.2.2 fit regression model country life\nexpectancy function corresponding continent. produce \ncorresponding analysis , now using fertility rate \nresponse variable income group categorical explanatory\nvariable. ’ll use slightly different notation done\npreviously make model general.linear model categorical explanatory variable called \none-factor model factor refers categorical explanatory\nvariable categories also called factor levels. represent\ncategories using indicator functions dummy variables. UN\ndata example, variable income four categories levels:\nLow income, Lower middle income, Upper middle income, \nHigh income. corresponding dummy variables needed :\\[\nD_1 = \\left\\{\n\\begin{array}{ll}\n1 & \\text{UN member state low income} \\phantom{asfdasfd} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\n\\]\n\\[\nD_2 = \\left\\{\n\\begin{array}{ll}\n1 & \\text{UN member state lower middle income} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\n\\]\n\\[\nD_3 = \\left\\{\n\\begin{array}{ll}\n1 & \\text{UN member state high middle income}\\phantom{} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\n\\]\n\\[\nD_4 = \\left\\{\n\\begin{array}{ll}\n1 & \\text{UN member state high income} \\phantom{asfdafd}\\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\\\\\n\\], example, given UN member state Low income, dummy\nvariables \\(D_1 = 1\\) \\(D_2 = D_3 = D_4 = 0\\). Similarly, another\nUN member state High  middle income, dummy variables\n\\(D_1 = D_2 = D_4 = 0\\) \\(D_3 = 1\\). Using dummy variables, \nmathematical formulation linear regression example :\\[\\hat y = \\widehat{\\text{fert rate}} = b_0 + b_2 D_2 + b_3 D_3 + b_4 D_4\\]want express terms \\(\\)th observation \ndataset, can include \\(\\)th subscript:\\[\\hat y_i = \\widehat{\\text{fert rate}} = b_0 + b_2 D_{2i} + b_3 D_{3i} + b_4 D_{4i}\\]Recall coefficient \\(b_0\\) represents intercept \ncoefficients \\(b_2\\), \\(b_3\\), \\(b_4\\) offsets based \nappropriate category. dummy variables, \\(D_2\\), \\(D_3\\), \\(D_4\\), take\nvalues zero one depending corresponding category \ngiven country. Observe also \\(D_1\\) appear model. \nreason entirely mathematical: model contain \nintercept dummy variables, model \n-specified, , contain one redundant explanatory\nvariable. solution drop one variables. keep \nintercept provides flexibility interpreting \ncomplicated models, drop one dummy variables , \ndefault R, first dummy variable, \\(D_1\\). mean\nlosing information first level \\(D_1\\). country \npart Low income level, \\(D_1 = 1\\), \\(D_2 = D_3 = D_4 = 0\\), \nterms regression zero linear regression\nbecomes:\\[\\hat y = \\widehat{\\text{fert rate}} = b_0\\] intercept\nrepresents average fertility rate country Low income\ncountry. Similarly, another country part \nHigh middle income level, \\(D_1 = D_2 = D_4 = 0\\) \\(D_3 = 1\\) \nlinear regression becomes:\\[\\hat y = \\widehat{\\text{fert rate}} = b_0 + b_3\\] average\nfertility rate High middle income country \\(b_0 + b_3\\).\nObserve \\(b_3\\) offset life expectancy \nbaseline level High middle income level. logic\napplies model possible income category.calculate regression coefficients using lm() function \ncommand coef() retrieve coefficients linear regression:present results table mathematical notation used\n:first level, Low income, “baseline” group. average\nfertility rate Low income UN member states 4.28.\nSimilarly, average fertility rate Upper middle income member\nstates 4.28 + -2.25 = 2.03.now ready study multiple linear regression model \ninteractions shown Figure 6.1. figure \ncan identify three different effects. First, fixed level life\nexpectancy, observe four different fertility rates. \nrepresent effect categorical explanatory variable, income.\nSecond, given regression line, slope represents change\naverage fertility rate due changes life expectancy. \neffect numerical explanatory variable life_exp. Third, observe\nslope line depends income level; \nillustration, observe High income member states slope \nless steep Low income member states. slope changes\ndue changes explanatory variable, call \ninteraction effect.mathematical formulation linear regression model two\nexplanatory variables, one numerical one categorical, \ninteractions :\\[\\begin{aligned}\\widehat{y} = \\widehat{\\text{fert rate}} = b_0\n&+ b_{02}D_2 + b_{03}D_3 + b_{04}D_4 \\\\\n&+ b_1x \\\\\n&+ b_{12}xD_2 + b_{13}xD_3 + b_{14}xD_4\\end{aligned}\\]linear regression shows average life expectancy affected\ncategorical variable, numerical variable, interaction\neffects. eight coefficients model separated\ncoefficients three lines highlight different roles.\nfirst line shows intercept effects categorical\nexplanatory variables. Recall \\(D_2\\), \\(D_3\\), \\(D_4\\) dummy\nvariables model equal one zero depending \ncategory country hand; correspondingly, coefficients\n\\(b_{02}\\), \\(b_{03}\\), \\(b_{04}\\) offsets respect \nbaseline level intercept, \\(b_0\\). Recall first dummy\nvariable dropped intercept captures effect. \nsecond line equation represents effect numerical\nvariable, \\(x\\). example \\(x\\) value life expectancy. \ncoefficient \\(b_1\\) slope line represents change \nfertility rate due one unit change life expectancy. third line\nequation represents interaction effects slopes.\nObserve combination life expectancy, \\(x\\), income\nlevel, \\(D_2\\), \\(D_3\\), \\(D_4\\). interaction effects \nmodify slope different levels income. \nLow income member state, dummy variables \\(D_1 = 1\\),\n\\(D_2 = D_3 = D_4 = 0\\) linear regression :\\[\\begin{aligned}\\widehat{y} = \\widehat{\\text{fert rate}} &= b_0 + b_{02}\\cdot 0 + b_{03}\\cdot 0 + b_{04}\\cdot 0 + b_1x + b_{12}x\\cdot 0 + b_{13}x\\cdot 0 + b_{14}x\\cdot 0\\\\\n& = b_0 + b_1x \\end{aligned}\\]Similarly, High income member state, dummy variables \n\\(D_1 = D_2 =D_3 = 0\\), \\(D_4 = 1\\). take account offsets\nintercept, \\(b_{04}\\), slope, \\(b_{14}\\), linear\nregression becomes:\\[\\begin{aligned}\\widehat{y} = \\widehat{\\text{fert rate}} &= b_0 + b_{02}\\cdot 0 + b_{03}\\cdot 0 + b_{04}\\cdot 1 + b_1 x  + b_{12}x\\cdot 0 + b_{13}x\\cdot 0 + b_{14}x\\cdot 1\\\\\n& = b_0 + b_{04} +  b_1x + b_{14}x\\\\\n& = (b_0 + b_{04}) + (b_1 + b_{14})\\cdot x\\end{aligned}\\]Observe intercept slope different \nHigh income member state compared baseline Low income\nmember state. illustration, construct multiple linear\nregression UN member state dataset R. first “fit” \nmodel using lm() “linear model” function find \ncoefficients using function coef(). R, formula used \ny ~ x1 + x2 + x1:x2 x1 x2 variable names \ndataset represent main effects x1:x2 interaction\nterm. simplicity, can also write y ~ x1 * x2 * sign\naccounts , main effects interaction effects. R let\nx1 x2 either explanatory numerical, need \nmake sure dataset format appropriate regression want\nrun. code example:\nTABLE 6.2: Regression table interaction model\ncan match coefficients values computed Table 6.2: fitted\nfertility rate \\(\\widehat{y} = \\widehat{\\text{fert rate}}\\) \nLow income countries \\[\\widehat{\\text{fert rate}} = b_0 + b_1\\cdot x = 11.92 + (-0.12)\\cdot x,\\]equation regression line Figure\n6.1 low income countries. regression \nintercept 11.92 slope -0.12. Since life\nexpectancy greater zero countries, intercept \npractical interpretation need produce \nappropriate line. interpretation slope : \nLow income countries, every additional year life expectancy reduces\naverage fertility rate 0.12 units.discussed earlier, intercept slope income\ngroups determined taking account appropriate offsets. \nexample, High income countries \\(D_4 = 1\\) \ndummy variables equal zero. regression line becomes\\[\\widehat{y} = \\widehat{\\text{fert rate}} = b_0 + b_1x + b_{04} + b_{14}x = (b_0 + b_{04} ) + (b_1+b_{14})x \\]\\(x\\) life expectancy, life_exp. intercept \n(Intercept) + incomeHigh income:\\[b_0 + b_{04} = 11.92 +(-6.58) = 5.34,\\]slope High income countries \nlife_exp + life_exp:incomeHigh income corresponding \\[b_1 + b_{14}=  -0.12 + 0.07 = -0.05.\\]High income countries, every additional year life expectancy\nreduces average fertility rate 0.05\nunits. intercepts slopes income levels calculated\nsimilarly.Since life expectancy Low income countries steeper slope\nHigh income countries, one additional year life expectancy\ndecrease fertility rates low-income group \nhigh-income group. consistent observation Figure\n6.1. associated effect one variable\ndepends value another variable say \ninteraction effect. reason regression slopes \ndifferent different income groups.\nLearning check\n(LC6.1) goal including interaction term multiple regression model?. create variables analysis.B. account effect one explanatory variable response considering influence another explanatory variable.C. make model complex without real benefit.D. automatically improve fit regression line.(LC6.2) inclusion main effects interaction terms regression model affect interpretation individual coefficients?. represent simple marginal effects.B. become meaningless.C. conditional effects, depending level interacting variables.D. interpreted way models without interactions.(LC6.3) statement use dummy variables regression models correct?. Dummy variables used represent numerical variables.B. Dummy variables used represent categorical variables least two levels.C. Dummy variables always decrease R-squared value.D. Dummy variables unnecessary regression models.","code":"\none_factor_model <- lm(fert_rate ~ income, data = UN_data_ch6)\ncoef(one_factor_model)\n# Fit regression model and get the coefficients of the model\nmodel_int <- lm(fert_rate ~ life_exp * income, data = UN_data_ch6)\ncoef(model_int)"},{"path":"multiple-regression.html","id":"model4table","chapter":"6 Multiple Regression","heading":"6.1.3 Model without interactions","text":"can simplify previous model removing interaction effects.\nmodel still represents different income groups different\nregression lines allowing different intercepts lines \nslope: parallel shown\nFigure 6.2.plot parallel slopes use function\ngeom_parallel_slopes()\nincluded moderndive package. use function need\nload ggplot2 moderndive packages. Observe \ncode identical one used model interactions \nFigure 6.1, now \ngeom_smooth(method = \"lm\", se = FALSE) layer replaced \ngeom_parallel_slopes(se = FALSE).\nFIGURE 6.2: Parallel slopes model fertility rate life expectancy income.\nregression lines income group shown Figure\n6.2. Observe lines now parallel:\nnegative slope. interpretation result\nchange fertility rate due changes life expectancy\ngiven country regardless income group \ncountry.hand, two regression lines Figure\n6.2 different intercepts representing \nincome group; particular, observe fixed level life\nexpectancy fertility rate greater Low income \nLower middle income countries Upper middle income \nHigh income countries.mathematical formulation linear regression model two\nexplanatory variables, one numerical one categorical, without\ninteractions :\\[\\widehat{y} = b_0 + b_{02}D_2 + b_{03}D_3 + b_{04}D_4+ b_1x.\\] Observe\ndummy variables affect intercept now, slope \nfully described \\(b_1\\) income group. UN data example, \nHigh income country, \\(D_4 = 1\\) dummy variables\nequal zero, represented \\[\\widehat{y} = (b_0 + b_{04})+ b_1x.\\]find coefficients regression R, formula used \ny ~ x1 + x2 x1 x2 variable names dataset\nrepresent main effects. Observe term x1:x2\nrepresenting interaction longer included. R let \nx1 x2 either explanatory numerical; therefore, \nalways check variable format appropriate \nregression want run. code UN data example:\nTABLE 6.3: Regression table model without interactions\nmodel without interactions presented Table\n6.3, slope \nregression lines, \\(b_1 = -0.101\\). Assuming model \ncorrect, UN member state, every additional year life\nexpectancy reduces average fertility rate 0.101\nunits, regardless income level member state. intercept\nregression line Low income member states \n10.768 High income member states \n\\(10.768 + (-1.067) = 9.701\\).\nintercepts income levels can determined similarly. \ncompare visualizations models side--side Figure\n6.3.\nFIGURE 6.3: Comparison interaction parallel slopes models.\none preferred model? Looking scatterplot \nclusters points Figure 6.3, \nappear lines different slopes capture better behavior \ndifferent groups points. lines appear parallel \ninteraction model seems appropriate.\nLearning check\n(LC6.4) model one categorical regressor one numerical regressor, interactions, interpreted?. slope model category different.B. slope model category .C. relationship categorical regressor response.D. relationship numerical regressor response.","code":"\nggplot(UN_data_ch6, aes(x = life_exp, y = fert_rate, color = income)) +\n  geom_point() +\n  labs(x = \"Life expectancy\", y = \"Fertility rate\", color = \"Income group\") +\n  geom_parallel_slopes(se = FALSE)\n# Fit regression model:\nmodel_no_int <- lm(fert_rate ~ life_exp + income, data = UN_data_ch6)\n\n# Get the coefficients of the model\ncoef(model_no_int)"},{"path":"multiple-regression.html","id":"model4points","chapter":"6 Multiple Regression","heading":"6.1.4 Observed responses, fitted values, and residuals","text":"subsection, work regression model interactions.\ncoefficients model found earlier, saved \nmodel_int, displayed Table 6.4:\nTABLE 6.4: Regression table interaction model\ncan use coefficients find fitted values residuals\ngiven observation. illustration, chose two observations\nUN member states dataset, provided values \nexplanatory variables response, well fitted values \nresiduals:first observation High income country life expectancy\n79.74 years observed fertility rate equal \n1.3. second observation Low income country\nlife expectancy 62.41 years observed\nfertility rate equal 5.7 fitted value, \\(\\hat y\\),\ncalled fert_rate_hat table, estimated value \nresponse determined regression line. value computed \nusing values explanatory variables coefficients \nlinear regression. addition, recall difference \nobserved response value fitted value, \\(y - \\hat y\\), called\nresidual.illustrate Figure 6.4. vertical line\nleft represents life expectancy value Low income\ncountry. y-value large dot regression line \nintersects vertical line fitted value fertility rate,\n\\(\\widehat y\\), y-value large dot line \nobserved fertility rate, \\(y\\). difference values,\n\\(y - \\widehat y\\), called residual case positive.\nSimilarly, vertical line right represents life expectancy\nvalue High income country, y-value large dot \nregression line fitted fertility rate. observed y-value\nfertility rate regression line making residual\nnegative.\nFIGURE 6.4: Fitted values two new countries.\ncan generalize study fitted values residuals \ncountries UN_data_ch6 dataset, shown Table\n6.5.\nTABLE 6.5: Regression points (First 10 182 countries)\n\nLearning check\n(LC6.5) Compute \nobserved response values, fitted values, residuals model\nwithout interactions.(LC6.6) main benefit visualizing fitted values residuals multiple regression model?. find errors dataset.B. check assumptions regression model, linearity homoscedasticity.C. always improve model’s accuracy.D. increase complexity model.","code":"\nregression_points <- get_regression_points(model_int)\nregression_points"},{"path":"multiple-regression.html","id":"model3","chapter":"6 Multiple Regression","heading":"6.2 Two numerical explanatory variables","text":"now consider regression models two numerical explanatory\nvariables. illustrate situation explore ISLR2 R\npackage first time book using\nCredit dataset. dataset contains\nsimulated information 400 customers. regression model use \ncredit card balance (Balance) response variable; credit limit\n(Limit), income (Income) numerical explanatory\nvariables.","code":""},{"path":"multiple-regression.html","id":"model3EDA","chapter":"6 Multiple Regression","heading":"6.2.1 Exploratory data analysis","text":"load Credit data\nframe ensure type \nbehavior become accustomed using tidyverse, also\nconvert data frame tibble using as_tibble(). construct new\ndata frame credit_ch6 variables needed.\nusing select() verb Subsection 3.8.1\n, addition, save selecting variables different names:\nBalance becomes debt, Limit becomes credit_limit, Income\nbecomes income:can observe effect use select() looking \nraw values either RStudio’s spreadsheet viewer using\nglimpse().Furthermore, present random sample five \n400 credit card holders Table\n6.6. observed , time run\ncode, different subset five rows given.\nTABLE 6.6: Random sample 5 credit card holders\nNote income thousands dollars debt credit limit\ndollars. can also compute summary statistics using \ntidy_summary() function. select() columns interest\nmodel shown Table 6.7:\nTABLE 6.7: Summary credit data\nmean median credit card debt $520.0 $459.5,\nrespectively. first quartile debt 68.8; means 25%\ncard holders debts $68.80 less. Correspondingly, mean\nmedian credit card limit, credit_limit, around $4,736 \n$4,622, respectively. Note also third quartile income \n57.5; 75% card holders incomes $57,500.visualize relationship response variable \ntwo explanatory variables using R code. plots shown\nFigure 6.5.\nFIGURE 6.5: Relationship credit card debt credit limit/income.\nleft plot Figure 6.5 shows positive \nlinear association credit limit credit card debt: credit\nlimit increases credit card debt. Observe also many\ncustomers credit card debt cluster points \ncredit card debt value zero. right plot Figure\n6.5 shows also positive somewhat linear\nassociation income credit card debt, association\nseems weaker actually appears positive incomes larger \n$50,000. lower income values clear \nassociation .Since variables debt, credit_limit, income numerical, \nimportantly, associations response explanatory\nvariables appear linear close linear, can also calculate \ncorrelation coefficient two variables. Recall \ncorrelation coefficient appropriate association \nvariables linear. One way using \nget_correlation() command seen Subsection 5.1.1, \nexplanatory variable response debt:Alternatively, using select() verb command cor() can\nfind correlations simultaneously returning correlation\nmatrix shown Table 6.8.\nmatrix shows correlation\ncoefficient pair variables appropriate row/column\ncombination.\nTABLE 6.8: Correlation coefficients credit card debt, credit limit, income\nLet’s look findings presented correlation matrix:diagonal values 1 , based definition \ncorrelation coefficient, correlation variable \nalways 1.correlation debt credit_limit 0.862. \nindicates strong positive linear relationship: greater \ncredit limit , larger credit card debt, average.correlation debt income 0.464. linear\nrelationship positive albeit somewhat weak. words,\nhigher income weakly associated higher debt.Observe also correlation coefficient two\nexplanatory variables, credit_limit income, 0.792.useful property correlation\ncoefficient invariant \nlinear transformations; means correlation two\nvariables, \\(x\\) \\(y\\), correlation \n\\((\\cdot x + b)\\) \\(y\\) constants \\(\\) \\(b\\). illustrate\n, observe correlation coefficient income \nthousands dollars credit card debt 0.464. now find\ncorrelation income dollars, multiplying income 1000,\ncredit card debt get:correlation exactly .return exploratory data analysis multiple regression.\nplots Figure 6.5 correspond \nresponse explanatory variables separately. \nFigure 6.6 show 3-dimensional (3D)\nscatterplot representing joint relationship three variables\nsimultaneously. 400 observations \ncredit_ch6 data frame marked \nblue point whereThe response variable \\(y\\), debt, vertical axis.regressors \\(x_1\\), income, \\(x_2\\), credit_limit, \ntwo axes form bottom plane.\nFIGURE 6.6: 3D scatterplot regression plane.\naddition, Figure 6.6 includes regression\nplane. Recall Subsection\n5.3.2 linear regression one numerical\nexplanatory variable selects “best-fitting” line: line \nminimizes sum squared\nresiduals. linear regression \nperformed two numerical explanatory variables, solution \n“best-fitting” plane: plane minimizes sum squared\nresiduals. Visit \nwebsite open \ninteractive version plot browser.\nLearning check\n(LC6.7) Conduct new\nexploratory data analysis outcome variable \\(y\\) debt \ncredit_rating age new explanatory variables \\(x_1\\)\n\\(x_2\\). can say relationship credit card\nholder’s debt credit rating age?","code":"\nlibrary(ISLR2)\ncredit_ch6 <- Credit |> as_tibble() |> \n  select(debt = Balance, credit_limit = Limit, \n         income = Income, credit_rating = Rating, age = Age)\nglimpse(credit_ch6)Rows: 400\nColumns: 5\n$ debt          <dbl> 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0, 204, 1081, 148, 0, 0, 368, 891, 104…\n$ credit_limit  <dbl> 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, 8117, 1311, 5308, 6922, 3291, 2525, …\n$ income        <dbl> 14.9, 106.0, 104.6, 148.9, 55.9, 80.2, 21.0, 71.4, 15.1, 71.1, 63.1, 15.0, 80.6, 43.7, 19.1, 20.…\n$ credit_rating <dbl> 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, 394, 511, 269, 200, 286, 339, 448, 4…\n$ age           <dbl> 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, 57, 73, 69, 28, 44, 63, 72, 61, 48, …\ncredit_ch6 |> sample_n(size = 5)\ncredit_ch6 |> select(debt, credit_limit, income) |> tidy_summary()\nggplot(credit_ch6, aes(x = credit_limit, y = debt)) +\n  geom_point() +\n  labs(x = \"Credit limit (in $)\", y = \"Credit card debt (in $)\", \n       title = \"Debt and credit limit\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\nggplot(credit_ch6, aes(x = income, y = debt)) +\n  geom_point() +\n  labs(x = \"Income (in $1000)\", y = \"Credit card debt (in $)\", \n       title = \"Debt and income\") +\n  geom_smooth(method = \"lm\", se = FALSE)\ncredit_ch6 |> get_correlation(debt ~ credit_limit)\ncredit_ch6 |> get_correlation(debt ~ income)\ncredit_ch6 |> select(debt, credit_limit, income) |> cor()\ncredit_ch6 |> get_correlation(debt ~ 1000 * income)# A tibble: 1 × 1\n    cor\n  <dbl>\n1 0.464"},{"path":"multiple-regression.html","id":"model3table","chapter":"6 Multiple Regression","heading":"6.2.2 Multiple regression with two numerical regressors","text":"shown Figure 6.6, linear regression \ntwo numerical regressors produces “best-fitting” plane. start\nmodel interactions two numerical explanatory\nvariables income credit_limit. R consider model fit \nformula form y ~ x1 + x2. retrieve regression\ncoefficients using lm() function command coef() \nget coefficients linear regression. regression\ncoefficients shown follows.present results table mathematical notation used\n:determine linear regression coefficients using\nlm(y ~ x1 + x2, data) x1 x2 two numerical\nexplanatory variables used.extract coefficients output using coef()\ncommand.Let’s interpret coefficients. intercept value \n-$385.179. range values regressors take\ninclude credit_limit $0 income $0, intercept\nrepresent average credit card debt individual \nlevels credit_limit income. case \ndata intercept practical interpretation; mainly\nused determine plane cut \\(y\\)-intercept \nproduce smallest sum squared residuals.slope multiple linear regression considered partial slope\nrepresents marginal additional contribution regressor\nadded model already contains regressors. \npartial slope typically different slope may find \nsimple linear regression regressor. reason ,\ntypically, regressors correlated, one regressor part \nmodel, indirectly ’s also explaining part regressor. \nsecond regressor added model, helps explain \nchanges response already accounted first\nregressor. example, slope credit_limit $0.264. Keeping\nincome fixed value, additional increase credit limit\none dollar credit debt increases, average, $0.264.\nSimilarly, slope income -$7.663. Keeping credit_limit\nfixed level, one unit increase income ($1000 \nactual income), associated decrease $7.66 credit card\ndebt, average.Putting results together, equation regression plane\ngives us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{debt}}\\) :\\[\n\\begin{aligned}\n\\widehat{y} = \\widehat{\\text{debt}}  &= b_0 + b_1 \\cdot x_1 +  b_2 \\cdot x_2\\\\\n&= -385.179 + 0.263 \\cdot x_1 - 7.663 \\cdot x_2\n\\end{aligned}\n\\] \\(x_1\\) represents credit limit \\(x_2\\) income.illustrate role partial slopes , observe right\nplot Figure 6.5 shows relationship \ndebt income isolation, positive relationship, \nslope income positive. can determine value slope \nconstructing simple linear regression using income \nregressor:regression line given following coefficients\ndenoted using prime (\\('\\)) designation since different values\nsaw previously\\[\n\\begin{aligned}\n\\widehat{y} = \\widehat{\\text{debt}}  &= b_0' + b_2' \\cdot x_2 = 246.515 + 6.048 \\cdot x_2\n\\end{aligned}\n\\]\n\\(x_2\\) income. contrast, credit_limit \nincome considered jointly explain changes debt, \nequation multiple linear regression :\\[\n\\begin{aligned}\n\\widehat{y} = \\widehat{\\text{debt}}  &= b_0 + b_1 \\cdot x_1 +  b_2 \\cdot x_2\\\\\n&= -385.179 + 0.263 \\cdot x_1 - 7.663 \\cdot x_2\n\\end{aligned}\n\\]slope income simple linear regression 6.048, \nslope income multiple linear regression \\(-7.663\\). \nsurprising results may appear first, perfectly\nvalid consistent slope simple linear regression \ndifferent role partial slope multiple linear regression.\nlatter additional effect income debt \ncredit_limit already taken account.\nLearning check\n(LC6.8) Fit new simple\nlinear regression using\nlm(debt ~ credit_rating + age, data = credit_ch6) \ncredit_rating age new numerical explanatory variables\n\\(x_1\\) \\(x_2\\). Get information “best-fitting” regression\nplane regression table finding coefficient model.\nregression results match results \nprevious exploratory data analysis?(LC6.9) following statements best describes interpretation regression coefficient multiple regression model?. additional effect regressor response regressors already taken account.B. average response variable value explanatory variables zero.C. always positive correlation strong.D. interpreted two explanatory variables.(LC6.10) characteristic “best-fitting” plane multiple regression model two numerical explanatory variables?. represents line best fit explanatory variable separately.B. minimizes product residuals.C. minimizes sum squared residuals combinations regressors.D. shows exact predictions every data point.(LC6.11) intercept represent multiple regression model two explanatory variables?. effect one explanatory variable, keeping constant.B. change response variable per unit change explanatory variable.C. correlation two explanatory variables.D. expected value response variable regressors zero.(LC6.12) term “partial slope” refer multiple regression model?. additional effect regressor response variable, regressors taken account.B. total slope variables combined.C. slope variables zero.D. average slopes model.","code":"\ndebt_model <- lm(debt ~ credit_limit + income, data = credit_ch6)\ncoef(debt_model)\n# Fit regression model and get the coefficients of the model\nsimple_model <- lm(debt ~ income, data = credit_ch6)\ncoef(simple_model)"},{"path":"multiple-regression.html","id":"model3points","chapter":"6 Multiple Regression","heading":"6.2.3 Observed/fitted values and residuals","text":"shown Subsection 6.1.4 UN member states\nexample, find fitted values residuals credit card\ndebt regression model. fitted values credit card debt\n(\\(\\widehat{\\text{debt}}\\)) computed using equation \nregression plane:\\[\n\\begin{aligned}\n\\widehat{y} = \\widehat{\\text{debt}} &= -385.179 + 0.263 \\cdot x_1 - 7.663 \\cdot x_2\n\\end{aligned}\n\\] \\(x_1\\) credit_limit \\(x_2\\) income. residuals\ndifference observed credit card debt fitted\ncredit card debt, \\(y - \\widehat y\\), observation data\nset. R, find fitted values, debt_hat, residuals,\nresidual, using get_regression_points() function. Table\n6.9 present first 10 rows output.\nRemember coordinates points 3D\nscatterplot Figure 6.6 can found \nincome, credit_limit, debt columns.\nTABLE 6.9: Regression points (First 10 credit card holders 400)\n","code":"\nget_regression_points(debt_model)"},{"path":"multiple-regression.html","id":"mult-reg-conclusion","chapter":"6 Multiple Regression","heading":"6.3 Conclusion","text":"","code":""},{"path":"multiple-regression.html","id":"additional-resources-4","chapter":"6 Multiple Regression","heading":"6.3.1 Additional resources","text":"R script file R code used chapter available .","code":""},{"path":"multiple-regression.html","id":"whats-to-come-5","chapter":"6 Multiple Regression","heading":"6.3.2 What’s to come?","text":"chapter concludes “Statistical/Data Modeling moderndive”\nportion book. ready proceed Part III: “Statistical\nInference infer.” Statistical inference science \ninferring unknown quantity using sampling. far, \nstudied regression coefficients interpretation. \nlater chapters learn can use information \nsample make inferences entire population.covered Chapters 7 sampling,\n8 confidence intervals, \n9 hypothesis testing, revisit \nregression models Chapter 10 \ninference regression. complete topics book, shown\nFigure 6.7!Also Chapter 10, revisit \nconcept residuals \\(y - \\widehat{y}\\) discuss importance \ninterpreting results regression model. perform known\nresidual analysis residual variable \nget_regression_points() outputs. Residual analyses enable us verify\nknown conditions inference regression.\nFIGURE 6.7: ModernDive flowchart – Part III!\n","code":""},{"path":"sampling.html","id":"sampling","chapter":"7 Sampling","heading":"7 Sampling","text":"third portion book introduces statistical inference. chapter sampling. Sampling involves drawing repeated random samples population. Section 7.1, illustrate sampling working samples white red balls proportion red balls samples. Section 7.2, present theoretical framework define sampling distribution. introduce one fundamental theoretical results Statistics: Central Limit Theorem Section 7.3. Section 7.4, present second sampling activity, time working samples chocolate-covered almonds average weight samples. Section 7.5, present sampling distribution scenarios. concepts behind sampling form basis inferential methods, particular confidence intervals hypothesis tests; methods studied Chapters 8 9.","code":""},{"path":"sampling.html","id":"sampling-packages","chapter":"7 Sampling","heading":"Needed packages","text":"needed, read Section 1.3 information install load R packages.Recall loading tidyverse package loads many packages encountered earlier. details refer Section 4.4. packages moderndive infer contain functions data frames used chapter.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)"},{"path":"sampling.html","id":"sampling-activity","chapter":"7 Sampling","heading":"7.1 First activity: red balls","text":"Take look bowl Figure 7.1. red white balls equal size. balls mixed beforehand seem particular pattern location red white balls inside bowl.\nFIGURE 7.1: bowl red white balls.\n","code":""},{"path":"sampling.html","id":"population-proportion","chapter":"7 Sampling","heading":"7.1.1 The proportion of red balls in the bowl","text":"interested finding proportion red balls bowl. find proportion, count number red balls divide number total number balls. bowl seen Figure 7.1 represented virtually data frame bowl included moderndive package. first ten rows shown illustration purposes:bowl 2400 rows representing 2400 balls bowl shown Figure 7.1. can view scroll entire contents bowl RStudio’s data viewer running View(bowl). first variable ball_ID used identification variable discussed Subsection 1.4.4; none balls actual bowl marked numbers.second variable color indicates whether particular virtual ball red white. compute proportion red balls bowl using dplyr data-wrangling verbs presented Chapter 3. steps needed order determine proportion. present steps separately remind work later introduce steps together simplify code. First, balls, identify red using test equality logical operator ==. using mutate() function Section 3.5 allows us create new Boolean variable called is_red.variable is_red returns Boolean (logical) value TRUE row color == \"red\" FALSE every row color equal \"red\". Since R treats TRUE like number 1 FALSE like number 0, accounting TRUEs FALSEs equivalent working 1’s 0’s. particular, adding 1’s 0’s equivalent counting many red balls bowl.compute using sum() function inside summarize() function. Recall Section 3.3 summarize() takes data frame many rows returns data frame single row containing summary statistics sum():sum() added 1’s 0’s effectively counted number red balls. 900 red balls bowl. Since bowl contains 2400 balls, proportion red balls 900/2400 = 0.375. ask R find proportion directly replacing sum() mean() function inside summarize(). average 1’s 0’s precisely proportion red balls bowl:code works well can simplified . Instead creating new Boolean variable is_red finding proportion, write steps simultaneously single line code:type calculation used often next subsections.","code":"\nbowl# A tibble: 2,400 × 2\n   ball_ID color\n     <int> <chr>\n 1       1 white\n 2       2 white\n 3       3 white\n 4       4 red  \n 5       5 white\n 6       6 white\n 7       7 red  \n 8       8 white\n 9       9 red  \n10      10 white\n# ℹ 2,390 more rows\nbowl |> \n  mutate(is_red = (color == \"red\"))# A tibble: 2,400 × 3\n   ball_ID color is_red\n     <int> <chr> <lgl> \n 1       1 white FALSE \n 2       2 white FALSE \n 3       3 white FALSE \n 4       4 red   TRUE  \n 5       5 white FALSE \n 6       6 white FALSE \n 7       7 red   TRUE  \n 8       8 white FALSE \n 9       9 red   TRUE  \n10      10 white FALSE \n# ℹ 2,390 more rows\nbowl |> \n  mutate(is_red = (color == \"red\")) |> \n  summarize(num_red = sum(is_red))# A tibble: 1 × 1\n  num_red\n    <int>\n1     900\nbowl |> \n  mutate(is_red = (color == \"red\")) |> \n  summarize(prop_red = mean(is_red))# A tibble: 1 × 1\n  prop_red\n     <dbl>\n1    0.375\nbowl |> \n  summarize(prop_red = mean(color == \"red\"))# A tibble: 1 × 1\n  prop_red\n     <dbl>\n1    0.375"},{"path":"sampling.html","id":"sampling-manual","chapter":"7 Sampling","heading":"7.1.2 Manual sampling","text":"previous subsection able find proportion red balls bowl using R information entire bowl data frame. Otherwise, retrieve manually. bowl contained large number balls, long tedious process. long think take manually bowl tens thousands balls? millions? even ?real-life situations, often interested finding proportion large number objects, subjects, performing exhaustive count tedious, costly, impractical, even impossible. limitations, typically perform exhaustive counts. Rather, balls example, randomly select sample balls bowl, find proportion red balls sample, use proportion learn proportion red balls entire bowl.","code":""},{"path":"sampling.html","id":"one-sample","chapter":"7 Sampling","heading":"One sample","text":"start inserting shovel bowl seen Figure 7.2 collect \\(5 \\cdot 10 = 50\\) balls shown Figure 7.3. set balls retrieved called sample.\nFIGURE 7.2: Inserting shovel bowl.\n\nFIGURE 7.3: Taking sample 50 balls bowl.\nObserve 17 balls red, thus proportion red balls sample 17/50 = 0.34 34%. Compare proportion red balls entire bowl, 0.375, found Subsection 7.1.1. proportion sample seems actually pretty good, take much time energy get. , approximate proportion just lucky outcome? lucky next time take sample bowl? Next take samples bowl calculate proportions red balls.","code":""},{"path":"sampling.html","id":"thirty-three-samples","chapter":"7 Sampling","heading":"Thirty-three samples","text":"now take many random samples shown Figure 7.4. time following:Return 50 balls used earlier back bowl mix contents bowl ensure new sample influenced previous sample.Take new sample shovel determine new proportion red balls.\nFIGURE 7.4: Repeating sampling activity.\nperform activity many times, observe different samples may produce different proportions red balls. proportion red balls sample called sample proportion. group 33 students performed activity previously drew histogram using blocks represent sample proportions red balls. Figure 7.5 shows students working histogram two blocks drawn already representing first two sample proportions found third added.\nFIGURE 7.5: Students drawing histogram sample proportions.\nRecall Section 2.5 histograms help us visualize distribution numerical variable. particular, center values falls values vary. histogram first 10 sample proportions can seen Figure 7.6.\nFIGURE 7.6: Hand-drawn histogram 10 sample proportions.\nlooking histogram, observe lowest proportion red balls 0.20 0.25 highest 0.45 0.5. importantly, frequently occurring proportions 0.30 0.35.activity performed 33 students results stored tactile_prop_red data frame included moderndive package. first 10 rows given:Observe student group data frame provides names, number red_balls observed sample, calculated proportion red balls sample, prop_red. also replicate variable enumerating 33 groups. chose name row can viewed one instance replicated (words “repeated”) activity.Using R data visualization techniques introduced Chapter 2, construct histogram 33 sample proportions shown Figure 7.7. Recall student sample 50 balls using procedure calculated proportion red balls sample. histogram built using sample proportions. need individual information student number red balls found. constructed histogram using ggplot() geom_histogram(). align bins computerized histogram version matches hand-drawn histogram shown Figure 7.6, arguments boundary = 0.4 binwidth = 0.05 used. former indicates want binning scheme, , one bins’ boundaries 0.4; latter fixes width bin 0.05 units.\nFIGURE 7.7: distribution sample proportions based 33 random samples size 50.\nstudying histogram can see proportions lower 25% others greater 45%, sample proportions 30% 45%.can also use activity introduce statistical terminology. process taking repeated samples 50 balls finding corresponding sample proportions called sampling. Since returned observed balls bowl getting another sample, say performed sampling replacement mixed balls taking new sample, samples randomly drawn called random samples.shown Figure 7.7, different random samples produce different sample proportions. phenomenon called sampling variation. Furthermore, histogram graphical representation distribution sample proportions; describes sample proportions determined often appear. distribution possible sample proportions can found random samples called, appropriately, sampling distribution sample proportion. sampling distribution central ideas develop chapter.\nLearning check\n(LC7.1) important mix balls bowl take new sample?(LC7.2) students sample proportion red balls?","code":"\ntactile_prop_red# A tibble: 33 × 4\n   group            replicate red_balls prop_red\n   <chr>                <int>     <int>    <dbl>\n 1 Ilyas, Yohan             1        21     0.42\n 2 Morgan, Terrance         2        17     0.34\n 3 Martin, Thomas           3        21     0.42\n 4 Clark, Frank             4        21     0.42\n 5 Riddhi, Karina           5        18     0.36\n 6 Andrew, Tyler            6        19     0.38\n 7 Julia                    7        19     0.38\n 8 Rachel, Lauren           8        11     0.22\n 9 Daniel, Caroline         9        15     0.3 \n10 Josh, Maeve             10        17     0.34\n# ℹ 23 more rows\nggplot(tactile_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of red balls in each sample\", \n       title = \"Histogram of 33 proportions\") "},{"path":"sampling.html","id":"sampling-simulation","chapter":"7 Sampling","heading":"7.1.3 Virtual sampling","text":"previous Subsection 7.1.2, performed tactile sampling activity: students took physical samples using real shovel bowl white red balls hand. now extend entire process using simulations computer, sort virtual sampling activity.use simulations permits us study 33 random samples thousands, tens thousands, even samples. large number random samples retrieved, can gain better understanding sampling distribution sampling variation sample proportions. addition, limited samples 50 balls, can simulate sampling desired sample size. going subsection. start mimicking manual activity.","code":""},{"path":"sampling.html","id":"one-virtual-sample","chapter":"7 Sampling","heading":"One virtual sample","text":"Recall bowl seen Figure 7.1 represented data frame bowl included moderndive package. virtual analog 50-ball shovel seen Figure 7.2 can achieved using rep_slice_sample() function included moderndive package. function allows us take repeated (replicated) random samples size n. start taking single sample 50 balls:Observe virtual_shovel 50 rows corresponding virtual sample size 50. ball_ID variable identifies 2400 balls bowl included sample 50 balls color denotes whether white red. replicate variable equal 1 50 rows decided take one sample right now. Later , take samples, replicate take values.compute proportion red balls virtual sample. code use similar one used finding proportion red balls entire bowl Subsection 7.1.1:Based random sample, 24% virtual_shovel’s 50 balls red! proceed finding sample proportion random samples.","code":"\nvirtual_shovel <- bowl |> \n  rep_slice_sample(n = 50)\nvirtual_shovel# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       <int>   <int> <chr>\n 1         1    1970 white\n 2         1     842 red  \n 3         1    2287 white\n 4         1     599 white\n 5         1     108 white\n 6         1     846 red  \n 7         1     390 red  \n 8         1     344 white\n 9         1     910 white\n10         1    1485 white\n# ℹ 40 more rows\nvirtual_shovel |> \n summarize(prop_red = mean(color == \"red\"))# A tibble: 1 × 2\n  replicate prop_red\n      <int>    <dbl>\n1         1     0.24"},{"path":"sampling.html","id":"thirty-three-virtual-samples","chapter":"7 Sampling","heading":"Thirty-three virtual samples","text":"Section 7.1, students got 33 samples sample proportions. repeated/replicated sampling process 33 times. virtually using function rep_slice_sample() time adding reps = 33 argument want retrieve 33 random samples. save samples data frame virtual_samples, shown, provide preview first 10 rows. want inspect entire virtual_samples data frame, use RStudio’s data viewer running View(virtual_samples).Observe data viewer first 50 rows replicate equal 1, next 50 rows replicate equal 2, . first 50 rows correspond first sample 50 balls next 50 rows correspond second sample 50 balls. pattern continues reps = 33 replicates, thus virtual_samples 33 \\(\\cdot\\) 50 = 1650 rows.Using virtual_samples find proportion red balls replicate. use dplyr verbs . particular, add group_by() replicate variable. Recall Section 3.4 assigning grouping variable “meta-data” summarize(), perform calculations needed replicate separately. line code, explained case one sample, calculates sample proportion red balls. preview first 10 rows presented:Actually, function rep_slice_sample() already groups data replicate, necessary include group_by() code. Moreover, using dplyr pipes R simplify work write everything :using rep_slice_sample(), 33 replicates (random sample 50 balls) andusing summarize() mean() Boolean values, determine proportion red balls sample.store proportions data frame virtual_prop_red print first 10 sample proportions (first 10 samples) illustration:case tactile activity, sampling variation resulting 33 proportions virtual samples. manually Subsection 7.1.3, construct histogram sample proportions shown Figure 7.8. histogram helps us visualize sampling distribution sample proportion. Observe histogram constructed using ggplot(), geom_histogram(), including arguments binwidth = 0.05 boundary = 0.4.\nFIGURE 7.8: distribution 33 proportions based 33 virtual samples size 50.\nobserving histogram can see proportions lower 25% others greater 45%. Also, sample proportions observed frequently 35% 40% (11 33 samples). found similar results sampling done hand Subsection 7.1.2, histogram presented Figure 7.7. present histograms side side Figure 7.9 easy comparison. Note somewhat similar center variation, although identical. differences also due sampling variation.\nFIGURE 7.9: sampling distribution sample proportion sampling variation: showing histogram virtual sample proportions (left) another histogram tactile sample proportions (right).\n\nLearning check\n(LC7.3) couldn’t study effects sampling variation used virtual shovel ? need take one virtual sample (case, 33 virtual samples)?","code":"\nvirtual_samples <- bowl |> \n  rep_slice_sample(n = 50, reps = 33)\nvirtual_samples# A tibble: 1,650 × 3\n# Groups:   replicate [33]\n   replicate ball_ID color\n       <int>   <int> <chr>\n 1         1    1970 white\n 2         1     842 red  \n 3         1    2287 white\n 4         1     599 white\n 5         1     108 white\n 6         1     846 red  \n 7         1     390 red  \n 8         1     344 white\n 9         1     910 white\n10         1    1485 white\n# ℹ 1,640 more rows\nvirtual_prop_red <- virtual_samples |> \n  group_by(replicate) |> \n  summarize(prop_red = mean(color == \"red\")) \nvirtual_prop_red# A tibble: 33 × 2\n   replicate prop_red\n       <int>    <dbl>\n 1         1     0.24\n 2         2     0.46\n 3         3     0.38\n 4         4     0.36\n 5         5     0.38\n 6         6     0.3 \n 7         7     0.42\n 8         8     0.42\n 9         9     0.32\n10        10     0.48\n# ℹ 23 more rows\nvirtual_prop_red <- bowl |> \n  rep_slice_sample(n = 50, reps = 33) |>\n  summarize(prop_red = mean(color == \"red\"))\nvirtual_prop_red# A tibble: 33 × 2\n   replicate prop_red\n       <int>    <dbl>\n 1         1     0.24\n 2         2     0.46\n 3         3     0.38\n 4         4     0.36\n 5         5     0.38\n 6         6     0.3 \n 7         7     0.42\n 8         8     0.42\n 9         9     0.32\n10        10     0.48\n# ℹ 23 more rows\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Sample proportion\", \n       title = \"Histogram of 33 sample proportions\") "},{"path":"sampling.html","id":"one-thousand-virtual-samples","chapter":"7 Sampling","heading":"One thousand virtual samples","text":"helpful observe sampling variation affects sample proportions 33 samples. also interesting note 33 virtual samples provide different sample proportions 33 physical samples, overall patterns fairly similar. samples taken random cases, set 33 samples, virtual physical, provide different set sample proportions due sampling variation, overall patterns still similar. Still, 33 samples enough fully understand patterns.now study sampling distribution effects sampling variation 1000 random samples. Trying manually impractical getting virtual samples can done quickly efficiently. Additionally, already developed tools . repeat steps performed earlier using rep_slice_sample() function sample size set 50. time, however, set number replicates (reps) 1000, use summarize() mean() Boolean values calculate sample proportions. compute virtual_prop_red count red balls corresponding sample proportion 1000 random samples. proportions first 10 samples shown:done previously, histogram 1000 sample proportions given Figure 7.10.\nFIGURE 7.10: distribution 1000 proportions based 1000 random samples size 50.\nsample proportions represented histogram low 15% high 60%, extreme proportions rare. frequent proportions determined 35% 40%. Furthermore, histogram now shows symmetric bell-shaped distribution can approximated well normal distribution.\nPlease read “Normal distribution” section (Appendix online) brief discussion distribution properties.\nLearning check\n(LC7.4) take 1000 samples 50 balls hand?(LC7.5) Looking Figure 7.10, say sampling 50 balls 30% red likely ? sampling 50 balls 10% red?","code":"\nvirtual_prop_red <- bowl |> \n  rep_slice_sample(n = 50, reps = 1000) |> \n  summarize(prop_red = mean(color == \"red\"))\nvirtual_prop_red# A tibble: 1,000 × 2\n   replicate prop_red\n       <int>    <dbl>\n 1         1     0.24\n 2         2     0.46\n 3         3     0.38\n 4         4     0.36\n 5         5     0.38\n 6         6     0.3 \n 7         7     0.42\n 8         8     0.42\n 9         9     0.32\n10        10     0.48\n# ℹ 990 more rows\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = \"Sample proportion\", title = \"Histogram of 1000 sample proportions\") "},{"path":"sampling.html","id":"different-sample-sizes","chapter":"7 Sampling","heading":"Different sample sizes","text":"Another advantage using simulations can also study sampling distribution sample proportion changes find sample proportions samples smaller larger 50 balls. need careful mix results though: build sampling distribution using sample proportions samples size, size chosen 50 balls.must first decide sample size want use, take samples using size. illustration, can perform sampling activity three times, activity using different sample size, think three shovels sizes 25, 50, 100 shown Figure 7.11. course, virtually: shovel size gather many random samples, calculate corresponding sample proportions, plot proportions histogram. Therefore create three histograms, one describing sampling distribution sample proportions samples size 25, 50, 100, respectively. show later subsection, size sample direct effect sampling distribution magnitude sampling variation.\nFIGURE 7.11: Three shovels extract three different sample sizes.\nfollow process performed previously: generate 1000 samples, find sample proportions, use draw histogram. follow process three different times, setting size argument code equal 25, 50, 100, respectively. run following code segments individually compare resulting histograms.easy comparison, present three resulting histograms single row matching \\(x\\) \\(y\\) axes Figure 7.12.\nFIGURE 7.12: Histograms sample proportions different sample sizes.\nObserve three histograms :centered around middle value, appears value slightly 0.4,somewhat bell-shaped, andexhibit sampling variation different sample size. particular, sample size increases 25 50 100, sample proportions vary much seem get closer middle value.important characteristics sampling distribution sample proportion: first observation relates shape distribution, second center distribution, last one sampling variation affected sample size. results coincidental isolated example sample proportions red balls bowl. next subsection, theoretical framework introduced helps explain precise mathematical equations behavior sample proportions coming random samples.\nLearning check\n(LC7.6) shown Figure 7.12 histograms sample proportions somewhat bell-shaped. can say center histograms?. smaller sample size concentrated center histogram.B. larger sample size smaller center histogram.C. center histogram seems , regardless sample size.(LC7.7) shown Figure 7.12 sample size increases, histogram gets narrower. happens sample proportions?. vary less.B. vary amount.C. vary .(LC7.8) use random sampling constructing sampling distributions?. always get sampleB. minimize bias make inferences populationC. make process easierD. reduce number samples needed(LC7.9) important construct histogram sample means proportions simulation study?. visualize distribution assess normality patternsB. increase accuracy sample meansC. ensure sample means exactly sameD. remove outliers data","code":"\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Compute sample proportions for 1000 samples, each sample of size 25\nvirtual_prop_red_25 <- bowl |> \n  rep_slice_sample(n = 25, reps = 1000) |> \n  summarize(prop_red = mean(color == \"red\"))\n\n# 1.b) Plot a histogram to represent the distribution of the sample proportions\nggplot(virtual_prop_red_25, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 balls that were red\", title = \"25\") \n\n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Compute sample proportions for 1000 samples, each sample of size 50\nvirtual_prop_red_50 <- bowl |> \n  rep_slice_sample(n = 50, reps = 1000) |> \n  summarize(prop_red = mean(color == \"red\"))\n\n# 2.b) Plot a histogram to represent the distribution of the sample proportions\nggplot(virtual_prop_red_50, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", title = \"50\")  \n\n\n# Segment 3: sample size = 100 ------------------------------\n# 2.a) Compute sample proportions for 1000 samples, each sample of size 100\nvirtual_prop_red_100 <- bowl |> \n  rep_slice_sample(n = 100, reps = 1000) |> \n  summarize(prop_red = mean(color == \"red\"))\n\n# 3.b) Plot a histogram to represent the distribution of the sample proportions\nggplot(virtual_prop_red_100, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 balls that were red\", title = \"100\") "},{"path":"sampling.html","id":"sampling-framework","chapter":"7 Sampling","heading":"7.2 Sampling framework","text":"Section 7.1, gained intuition sampling characteristics. section, introduce statistical definitions terminology related sampling. conclude introducing key characteristics formally studied rest chapter.","code":""},{"path":"sampling.html","id":"terminology-and-notation","chapter":"7 Sampling","heading":"7.2.1 Population, sample, and the sampling distribution","text":"population study population collection individuals observations interest. bowl activities population collection balls bowl. sample subset population. Sampling act collecting samples population. Simple random sampling sampling member population chance selected, example, using shovel select balls bowl. random sample sample found using simple random sampling. bowl activities, physical virtual, use simple random sampling get random samples bowl.population parameter (simply parameter) numerical summary (number) represents characteristic population. sample statistic (simply statistic) numerical summary computed sample. bowl activities parameter interest population proportion \\(p=\\) 0.375. Similarly, previously sample 50 balls taken 17 red. statistic sample proportion example equal \\(\\widehat{p}= 0.34\\). Observe use \\(p\\) represent population proportion (parameter) \\(\\widehat{p}\\) sample proportion (statistic).distribution list numbers set possible values list often occur. sampling distribution sample proportion distribution sample proportions possible random samples given size. illustrate concept recall Subsection 7.1.3 drew three histograms shown Figure 7.12. histogram left, example, constructed taking 1000 random samples size \\(n=25\\), finding sample proportion sample using proportions draw histogram. histogram good visual approximation sampling distribution sample proportion.sampling distribution can difficult concept grasp right away:sampling distribution sample proportion distribution sample proportions; constructed using exclusively sample proportions.careful people learning terminology sometimes confuse term sampling distribution sample’s distribution. latter can understood distribution values given sample.histogram simulation sample proportions visual approximation sampling distribution. exact distribution. Still, simulations produce large number sample proportions, resulting histogram provides good approximation sampling distribution. case Subsection 7.1.3 three histograms shown Figure 7.12.lessons learned performing activities Section 7.1 contribute gaining insights key characteristics sampling distribution sample proportion, namely:center sampling distributionThe effect sampling variation sampling distribution effect sample size sampling variationThe shape sampling distributionThe first two points relate measures central tendency dispersion, respectively. last one provides connection one important theorems statistics: Central Limit Theorem. next section, formally study characteristics.\nLearning check\n(LC7.10) case bowl activity, population parameter? know value? can know value exactly?(LC7.11) ensure samples collected shovel random?","code":""},{"path":"sampling.html","id":"central-limit-theorem","chapter":"7 Sampling","heading":"7.3 The Central Limit Theorem","text":"fascinating result statistics , retrieving random samples population, corresponding sample means follow typical behavior: histogram bell-shaped unique features. true regardless distribution population values forms basis know Central Limit Theorem. fully describing , introduce theoretical framework construct characteristics related sampling.","code":""},{"path":"sampling.html","id":"random-variables","chapter":"7 Sampling","heading":"7.3.1 Random variables","text":"simple theoretical framework can help us formalize important properties sampling distribution sample proportion. modify bowl activity slightly. Instead using shovel select 25 balls , randomly select one ball time, 25 times. ball red call success record 1 (one); red call failure record 0 (zero). , return ball bowl proportion red balls bowl doesn’t change.\nprocess called trial Bernoulli trial honor Jacob Bernoulli, 17th-century mathematician among first ones work trials.\nGetting sample 25 balls running 25 trials getting 25 numbers, ones zeros, representing whether observed red balls trial, respectively.\naverage 25 numbers (zeros ones) represents precisely proportion red balls sample 25 balls.useful represent trial random variable. use uppercase letter \\(X\\) subscript \\(1\\) \\(X_1\\) denote random variable first trial. first trial completed, color first ball observed, value \\(X_1\\) realized 1 ball red 0 ball white. example, first ball red, write \\(X_1 = 1\\). Similarly, use \\(X_2\\) represent second trial. example, second ball white, \\(X_2\\) realized \\(X_2=0\\), . \\(X_1\\), \\(X_2\\), \\(\\dots\\) random variables trials performed. trials, just ones zeros representing red white balls, respectively.Moreover, since experiment perform 25 trials find average , average mean, trials carried , can also expressed random variable:\\[\\overline X = \\frac{X_1+X_2+\\dots+X_{25}}{25}.\\]\\(\\overline X\\) random variable represents average, mean, 25 trials. call \\(\\overline X\\) sample mean. , \\(\\overline X\\) random variable 25 trials performed. trials, \\(\\overline X\\) realized average 25 zeros ones.\nexample, results trials \\[\\{0,0,0,1,0,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1 \\},\\]observed value \\(\\overline X\\) \\[\\overline X = \\frac{0+0+0+1+0+1+\\dots+1+0+0+0+1}{25} = \\frac{10}{25}=0.4.\\], particular example, sample mean \\(\\overline X = 0.4\\) happens sample proportion red balls sample 25 balls. context Bernoulli trials, finding averages zeros ones, sample means sample proportions! Connecting notation used earlier, observe trials completed, \\(\\overline X = \\widehat{p}\\).","code":""},{"path":"sampling.html","id":"the-sampling-distribution-using-random-variables","chapter":"7 Sampling","heading":"7.3.2 The sampling distribution using random variables","text":"Suppose want calculate sample proportion another random sample 25 balls. terms random variable \\(\\overline X\\), performing 25 trials finding another 25 values, ones zeros, \\(X_1\\), $X_2, \\(\\dots\\), \\(X_{25}\\) finding average. example might get:\\[\\{1,0,0,1,0,0,0,1,0,0,1,0,1,0,1,1,0,0,0,1,0,1,0,0,0,0\\}\\], realization \\(\\overline X\\) \\(\\overline X = 9/25 = 0.36\\). sample proportion different one found earlier, 0.4. possible values \\(\\overline X\\) possible proportions red balls sample 25 balls. words, value \\(\\overline X\\) takes trials completed sample proportion observed sample red white balls.Moreover, given trial can result choosing red ball (1 0), chances getting red ball influenced proportion red balls bowl. example, bowl red balls white, chances getting red ball given trial higher getting white ball. 1 realization trial red ball observed, sample proportion also tend higher.Sampling variation produces different sample proportions different random samples, influenced proportion red white balls bowl. understanding sampling distribution sample proportion learning sample proportions possible proportions less likely observed. Since realization \\(\\overline X\\) observed sample proportion, sampling distribution sample proportion precisely distribution \\(\\overline X\\). rest section, use expressions interchangeably. Recall key characteristics sampling distribution sample proportion, now given terms \\(\\overline X\\):center distribution \\(\\overline X\\)effect sampling variation distribution \\(\\overline X\\) effect sample size sampling variationThe shape distribution \\(\\overline X\\)address points, use simulations. Simulations seldom provide exact structure distribution, infinite number samples may needed . large number replications often produces really good approximation distribution though can used understand well distribution’s characteristics. Let’s use output found Subsection 7.1.3; namely, sample proportions samples size 25, 50, 100. focus size 25, think sample proportion samples size 25 possible value \\(\\overline X\\). now use sample proportions illustrate properties distribution \\(\\overline X\\), sampling distribution sample proportion.","code":""},{"path":"sampling.html","id":"the-center-of-the-distribution-the-expected-value","chapter":"7 Sampling","heading":"7.3.3 The center of the distribution: the expected value","text":"Since distribution \\(\\overline X\\) composed sample proportions can calculated given sample size, center distribution can understood average proportions. value expect get, average, sample proportions. center value sampling distribution called expected value sample proportion, write \\(E(\\overline X)\\). Based probability theory, mean \\(\\overline X\\) happens equal population proportion red balls bowl. Subsection 7.1.1 determined population proportion 900/2400 = 0.375, therefore\\[E(\\overline X) = p = 0.375.\\]illustration, noted Subsection 7.1.3 looking histograms Figure 7.12 three histograms centered value 0.35 0.4 (35% 40%). established now, centered exactly expected value \\(\\overline X\\), population proportion. Figure 7.13 displays histograms , time adds vertical red line location population proportion value, \\(p\\) = 0.375.\nFIGURE 7.13: Three sampling distributions population proportion \\(p\\) marked vertical line.\nresults shown seem agree theory. can check, using simulation results, finding average 1000 sample proportions. start histogram left:variable prop_red data frame virtual_prop_red_25 contains sample proportions 1000 samples taken. average sample proportions presented object E_Xbar_25 represents estimated expected value \\(\\overline X\\), using average 1000 sample proportions. sample proportions calculated random samples 25 balls bowl. average happens precisely population proportion.worth spending moment understanding result. take one random sample given size, know sample proportion sample somewhat different population proportion due sampling variation; however, take many random samples size, average sample proportions expected population proportion.present equivalent results samples size 50 100:Indeed, results population proportion. Note average 1000 sample proportions samples size 50 actually 0.379 close 0.375. happens simulations approximate sampling distribution expected value. using simulations expect achieve exact theoretical results, rather values close enough support understanding theoretical results.\nLearning check\n(LC7.12) expected value sample mean context sampling distributions?. observed value sample meanB. population meanC. median sample distributionD. midpoint range","code":"\nvirtual_prop_red_25# A tibble: 1,000 × 3\n   replicate prop_red     n\n       <int>    <dbl> <int>\n 1         1     0.32    25\n 2         2     0.24    25\n 3         3     0.16    25\n 4         4     0.32    25\n 5         5     0.44    25\n 6         6     0.36    25\n 7         7     0.32    25\n 8         8     0.36    25\n 9         9     0.2     25\n10        10     0.32    25\n# ℹ 990 more rows\nvirtual_prop_red_25 |> \n  summarize(E_Xbar_25 = mean(prop_red))# A tibble: 1 × 1\n  E_Xbar_25\n      <dbl>\n1     0.377\nvirtual_prop_red_50 |> \n  summarize(E_Xbar_50 = mean(prop_red))\nvirtual_prop_red_100 |> \n  summarize(E_Xbar_100 = mean(prop_red))# A tibble: 1 × 1\n  E_Xbar_50\n      <dbl>\n1     0.379# A tibble: 1 × 1\n  E_Xbar_100\n       <dbl>\n1      0.377"},{"path":"sampling.html","id":"sampling-variation","chapter":"7 Sampling","heading":"7.3.4 Sampling variation: standard deviation and standard error","text":"Another relevant characteristic observed Figure 7.13 amount dispersion sampling variation changes sample size changes. histograms similar bell-shaped configuration centered value, observe …sample size \\(n=25\\) (left histogram) observed sample proportions low 0.1 high 0.65.sample size \\(n=50\\) (middle histogram) observed sample proportions low 0.15 high 0.55.sample size \\(n=100\\) (right histogram) observed sample proportions low 0.20 high 0.5.sample size \\(n\\) increases 25 50 100, variation sampling distribution decreases. Thus, values clustered tightly around center distribution. words, histogram left Figure 7.13 spread one middle, turn spread one right.know center distribution expected value \\(\\overline X\\), population proportion. , can quantify variation calculating far sample proportions , average, population proportion. well-known statistical measurement quantify dispersion standard deviation. discuss works continue sampling variation problem.","code":""},{"path":"sampling.html","id":"the-standard-deviation","chapter":"7 Sampling","heading":"The standard deviation","text":"start example introduce special notation. illustration, given four values \\(y_1=3\\), \\(y_2=-1\\), \\(y_3=5\\), \\(y_4= 9\\), average given \\[\\bar y = \\frac14\\sum_{=1}^4y_i =\\frac14 (y_1 + y_2 + y_3 + y_4)=  \\frac{3-1+5+9}{4}= 2.\\]capital Greek letter \\(\\Sigma\\) represents summation values, useful large number values need added. letter \\(\\) underneath \\(\\Sigma\\) index summation. starts \\(=1\\), first value adding \\(y_{\\bf 1} = 3\\). Afterwards \\(=2\\), add \\(y_{\\bf 2}=-1\\) previous result, , shown equation . summation symbol can useful adding many numbers making complicated operations, defining standard deviation.construct standard deviation list values, wefirst find deviations value average,square deviations,find average squared deviations, andtake square root average finish.example, standard deviation given \\[\\begin{aligned}\nSD &= \\sqrt{\\frac14\\sum_{=1}^4(y_i - \\bar y)^2} = \\sqrt{\\frac{(3-2)^2+(-1-2)^2+(5-2)^2+(9-2)^2}{4}} \\\\\n   &= \\sqrt{\\frac{1+9+9+49}{4}}=\\sqrt{17} = 4.12\n\\end{aligned}\\]present another example, time using R. use bowl activity red white balls bowl. create Boolean variable is_red corresponds TRUEs 1s red balls FALSEs 0s white balls using numbers, compute proportion (average 1s 0s) using mean() function standard deviation using sd() function2 inside summarize():, proportion red balls 0.375 standard deviation 0.484. intuition behind standard deviation can expressed follows: select many balls, replacement, bowl, expect proportion red balls 0.375 give take 0.484.addition, dealing proportions, formula standard deviation can expressed directly terms population proportion, \\(p\\), using formula:\\[SD = \\sqrt{p(1-p)}.\\]value standard deviation using alternative formula R:value using general formula. Now gained better understanding standard deviation, can discuss standard deviation context sampling variation sample proportion.","code":"\nbowl |> \n  mutate(is_red = color == \"red\") |> \n  summarize(p = mean(is_red), st_dev = sd(is_red))# A tibble: 1 × 2\n      p st_dev\n  <dbl>  <dbl>\n1 0.375  0.484\np <- 0.375\nsqrt(p * (1 - p))[1] 0.484"},{"path":"sampling.html","id":"the-standard-error","chapter":"7 Sampling","heading":"The standard error","text":"Recall want measure magnitude sampling variation distribution \\(\\overline X\\) (sampling distribution sample proportion) want use standard deviation purpose. shown earlier center distribution \\(\\overline X\\) expected value \\(\\overline X\\). case, population proportion \\(p = 0.375\\). standard deviation indicate far, average, possible sample proportion roughly population proportion. consider using sample proportion estimate population proportion, deviation considered error estimation. particular relationship, standard deviation sampling distribution receives special name: standard error. Note standard errors standard deviations standard deviations standard errors.work simulations bowl red white balls. take 10,000 random samples size \\(n=100\\), find sample proportion sample, calculate average standard deviation sample proportions. simulation produces histogram similar one presented right Figure 7.13. produce data use rep_slice_sample() function mean() sd() function inside summarize() produce desired results:Observe p estimated expected value SE_Xbar estimated standard error based simulation taking sample proportions random samples size \\(n=100\\). Compare value standard deviation entire bowl, discovered earlier. one-tenth size! coincidence: standard error \\(\\overline X\\) equal standard deviation population (bowl) divided square root sample size. case sample proportions, standard error \\(\\overline X\\) can also determined using formula:\\[SE(\\overline X) = \\sqrt{\\frac{p(1-p)}{n}}\\]\n\\(p\\) population proportion \\(n\\) size sample. formula shows standard error inversely proportional square root sample size: sample size increases, standard error decreases. example, standard error \\[SE(\\overline X) = \\sqrt{\\frac{0.375\\cdot(1-0.375)}{100}} = 0.0484\\]value nearly identical result found simulation . repeat exercise, time finding estimated standard error \\(\\overline X\\) simulations done earlier. simulations stored data frames virtual_prop_red_25 virtual_prop_red_50, sample sizes used \\(n=25\\) \\(n=50\\), respectively:standard errors examples, based proportion red balls bowl sample sizes, given:simulations support standard errors derived using mathematical formulas. simulations used check fact results achieved agree theory. Observe also theoretical results constructed based knowledge population proportion, \\(p\\); contrast, simulations produce samples based population interest produce results based information found samples sample proportions.formula standard error sample proportion given can actually derived using facts probability theory, development goes beyond scope book. learn , please consult advanced treatments probability statistics one.","code":"\nbowl |>\n  rep_slice_sample(n = 100, replace = TRUE, reps = 10000) |>\n  summarize(prop_red = mean(color == \"red\")) |>\n  summarize(p = mean(prop_red), SE_Xbar = sd(prop_red))# A tibble: 1 × 2\n      p SE_Xbar\n  <dbl>   <dbl>\n1 0.375  0.0479\np <- 0.375\nsqrt(p * (1 - p) / 100)[1] 0.0484\nvirtual_prop_red_25 |> \n  summarize(SE_Xbar_50 = sd(prop_red))# A tibble: 1 × 1\n  SE_Xbar_50\n       <dbl>\n1     0.0971\nvirtual_prop_red_50 |> \n  summarize(SE_Xbar_100 = sd(prop_red))# A tibble: 1 × 1\n  SE_Xbar_100\n        <dbl>\n1      0.0667\nsqrt(p * (1 - p) / 25)[1] 0.0968\nsqrt(p * (1 - p) / 50)[1] 0.0685"},{"path":"sampling.html","id":"the-sampling-distribution-of-the-sample-proportion","chapter":"7 Sampling","heading":"The sampling distribution of the sample proportion","text":"far shown properties sampling distribution sampling proportion; namely, expected value standard error \\(\\overline X\\). now turn attention shape sampling distribution.mentioned , histograms (seen earlier) provide good approximation sampling distribution sample proportion, distribution \\(\\overline X\\). Since interested shape distribution, redraw histograms using sample proportions random samples size \\(n=25\\), \\(n=50\\), \\(n=100\\), time add smooth curve appears connect top parts bar histogram. histograms presented Figures 7.14, 7.15, 7.16. figures represent density histograms area bar represents percentage proportion observations corresponding bin total area histogram 1 (100%). ranges \\(x-\\) \\(y-\\)axis plots kept constant appropriate comparisons among .\nFIGURE 7.14: Histogram distribution sample proportion normal curve (n=25).\n\nFIGURE 7.15: Histogram distribution sample proportion normal curve (n=50).\n\nFIGURE 7.16: Histogram sampling distribution sample proportion normal curve (n=100).\ncurves red seem fairly good representation top bars histograms. However, used simulated data draw curves, bell-shaped curves extracted normal distribution mean equal \\(p=0.375\\) standard deviation equal \\(\\sqrt{{p(1-p)/n}}\\) \\(n\\) changes histogram. fascinating result due application one important results Statistics: Central Limit Theorem (CLT).CLT states sample size, \\(n\\), tends infinity, distribution \\(\\overline X\\) tends normal distribution (appropriate mean standard deviation).\nMoreover, depend population distribution; population can bowl red white balls anything else.observant reader might noticed , practice, take samples size equal infinity.\nmakes CLT even relevant practical purposes distribution \\(\\overline X\\) approximates normality even sample size used fairly small. can see Figure 7.14, even random samples size \\(n=25\\) used, distribution \\(\\overline X\\) already seems follow normal distribution.Observe also curves follow bell-shaped form normal curve spread greater smaller sample size used consistent standard error \\(\\overline X\\) found earlier case.\nLearning check\n(LC7.13) role Central Limit Theorem (CLT) statistical inference?. provides formula calculating standard deviation given sample, allowing understanding sample’s spread variability.B. states sampling distribution sample mean approach normal distribution, regardless population’s distribution, sample size becomes large.C. determines actual mean population directly calculating randomly selected sample, without needing additional data assumptions.D. principle applies strictly exclusively populations normally distributed, ensuring cases sample means follow normal distribution pattern.(LC7.14) term “sampling variation” refer ?. Variability population data.B. Differences sample statistics due random sampling.C. Changes population parameter time.D. Variation caused errors data collection.","code":""},{"path":"sampling.html","id":"summary-5","chapter":"7 Sampling","heading":"7.3.5 Summary","text":"Let’s look learned sampling distribution sample proportion:mean sample proportions exactly population proportion.standard deviation sample proportions, also called standard error, inversely proportional square root sample size: larger sample size used calculate sample proportions, closer sample proportions population proportion, average.long random samples used large enough, sampling distribution sample proportion, simply distribution \\(\\overline X\\), approximate normal distribution. true sample proportions regardless structure underlying population distribution; , regardless many red white balls bowl, whether performing experiment deals sample proportions.case want reinforce ideas little , Shuyi Chiou, Casey Dunn, Pathikrit Bhattacharyya created 3-minute 38-second video https://youtu./jvoxEYmQHNM explaining crucial statistical theorem using average weight wild bunny rabbits average wingspan dragons examples. Figure 7.17 shows preview video.\nFIGURE 7.17: Preview Central Limit Theorem video.\n","code":""},{"path":"sampling.html","id":"sampling-activity-mean","chapter":"7 Sampling","heading":"7.4 Second activity: chocolate-covered almonds","text":"want extend results achieve sample proportion general case: sample mean. section show \nresults sample proportions extend directly sample means, also highlight important differences working sampling distribution sample mean.sample proportions, start illustrating results another activity: sampling bowl chocolate-covered almonds, seen Figure 7.18.\nFIGURE 7.18: bowl chocolate-covered almonds.\nease exposition refer chocolate-covered almond simply almond. now interested average weight grams almonds bowl; population average weight population mean weight.","code":""},{"path":"sampling.html","id":"population-mean","chapter":"7 Sampling","heading":"7.4.1 The population mean weight of almonds in the bowl","text":"population interest given almonds bowl. bowl represented virtually data frame almonds_bowl included moderndive package. first ten rows shown illustration purposes:first variable ID represents virtual ID number given almond, variable weight contains weight grams almond bowl. population mean weight almonds, population parameter, can calculated R using dplyr data-wrangling verbs presented Chapter 3. Observe, particular, inside function summarize() use mean(), sd(), n() functions mean weight, weight’s standard deviation3, number almonds bowl:5,000 almonds bowl, population mean weight 3.64 grams, weight’s standard deviation 0.392 grams. used R compute mean standard deviation, used formulas instead. call \\(x_1\\) first almond bowl, \\(x_2\\) second, , mean given \\[\\mu = \\sum_{=1}^{5000}\\frac{x_i}{5000}=3.64.\\]standard deviation given \\[\\sigma = \\sum_{=1}^{5000} \\frac{(x_i - \\mu)^2}{5000}=0.392.\\]Greek letters \\(\\mu\\) \\(\\sigma\\) used represent population mean population standard deviation (parameters interest). addition, since know information entire bowl, can draw distribution weights entire population (bowl) using histogram:\nFIGURE 7.19: Distribution weights entire bowl almonds.\ncan see weight almonds ranges 2.6 4.6 grams common weights observed 3.6 4.0 grams, distribution symmetric follow typical pattern.Now clear understanding population interest parameters interest, can continue exploration sampling distribution sample mean weights almonds constructing samples.","code":"\nalmonds_bowl# A tibble: 5,000 × 2\n      ID weight\n   <int>  <dbl>\n 1     1    3.8\n 2     2    4.2\n 3     3    3.2\n 4     4    3.1\n 5     5    4.1\n 6     6    3.9\n 7     7    3.4\n 8     8    4.2\n 9     9    3.5\n10    10    3.4\n# ℹ 4,990 more rows\nalmonds_bowl |> \n  summarize(mean_weight = mean(weight), \n            sd_weight = sd(weight), \n            length = n())# A tibble: 1 × 3\n  mean_weight sd_weight length\n        <dbl>     <dbl>  <int>\n1        3.64     0.392   5000\nggplot(almonds_bowl, aes(x = weight)) +\n  geom_histogram(binwidth = 0.1, color = \"white\")"},{"path":"sampling.html","id":"resampling-tactile-bowl","chapter":"7 Sampling","heading":"7.4.2 Manual sampling and sample means","text":"randomly select one almond bowl, determine weight using scale, shown Figure 7.20:\nFIGURE 7.20: One almond scale.\nLet’s now take random sample 25 almonds, shown Figure 7.21, determine sample average weight, sample mean weight, grams.\nFIGURE 7.21: random sample 25 almonds scale.\nSince total weight 88.6 grams, shown Figure 7.21, sample mean weight \\(88.6/25 = 3.544\\). moderndive package contains information sample almonds_sample data frame. , present weight first 10 almonds sample:almonds_sample data frame moderndive package \\(n=\\) 25 rows corresponding almond sample shown Figure 7.21.\nfirst variable replicate indicates first replicate since single sample. second variable ID gives identification particular almond. third column weight gives corresponding weight almond grams numeric variable, also known double (dbl). distribution weights 25 shown histogram Figure 7.22.\nFIGURE 7.22: Distribution weight sample 25 almonds.\nweights almonds sample range 2.9 4.4 grams. obvious pattern distribution sample. now compute sample mean using data-wrangling tools Chapter 3.sample mean weight far population mean weight 3.64 grams. difference statistic (sample mean weight) parameter (population mean weight) due sampling variation.","code":"\nalmonds_sample# A tibble: 25 × 3\n# Groups:   replicate [1]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1  4645    3  \n 2         1  3629    3.9\n 3         1  4796    4  \n 4         1  2669    3.8\n 5         1  3488    4.3\n 6         1   105    4.1\n 7         1  1762    3.6\n 8         1  1035    4.2\n 9         1  4880    3.2\n10         1   398    4  \n# ℹ 15 more rows\nggplot(almonds_sample, aes(x = weight)) +\n  geom_histogram(binwidth = 0.1, color = \"white\")\nalmonds_sample |> summarize(sample_mean_weight = mean(weight))# A tibble: 1 × 2\n  replicate sample_mean_weight\n      <int>              <dbl>\n1         1               3.67"},{"path":"sampling.html","id":"virtual-samples-mean-bowl","chapter":"7 Sampling","heading":"7.4.3 Virtual sampling","text":"now perform sampling virtually. data frame almonds_bowl 5000 rows, representing almond bowl. Section 7.1.3 use rep_slice_sample() function retrieve 1000 random samples sample size set 25, number replicates reps set 1000. sure scroll contents virtual_samples RStudio’s viewer.Observe now virtual_samples_almonds 1000 \\(\\cdot\\) 25 = 25,000 rows. Using appropriate data wrangling code, virtual_mean_weight data frame produces sample mean almond weight random sample, total 1000 sample means.Figure 7.23 presents histogram sample means:\nFIGURE 7.23: distribution 1000 means based 1000 random samples size 25.\nsample mean weights observed histogram appear go 3.4 grams 3.85 grams, extreme sample means rare. frequent sample means found seem 3.5 3.8 grams.\nFurthermore, histogram almost symmetric showing bell-shaped form, although left tail histogram appears slightly longer right tail. dealing sample means now, conclusions strikingly similar presented Subsection 7.4.2 discussing sampling distribution sample proportion.","code":"\nvirtual_samples_almonds <- almonds_bowl |> \n  rep_slice_sample(n = 25, reps = 1000)\nvirtual_samples_almonds# A tibble: 25,000 × 3\n# Groups:   replicate [1,000]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1  3467    3.7\n 2         1  3784    4.2\n 3         1  4653    4.2\n 4         1  2216    4.1\n 5         1    98    3.5\n 6         1  2286    3.6\n 7         1  4597    3.6\n 8         1  2385    4.3\n 9         1  3959    3.7\n10         1  1497    3.9\n# ℹ 24,990 more rows\nvirtual_mean_weight <- virtual_samples_almonds |> \n  summarize(mean_weight = mean(weight))\nvirtual_mean_weight# A tibble: 1,000 × 2\n   replicate mean_weight\n       <int>       <dbl>\n 1         1        3.79\n 2         2        3.45\n 3         3        3.67\n 4         4        3.5 \n 5         5        3.67\n 6         6        3.63\n 7         7        3.62\n 8         8        3.59\n 9         9        3.56\n10        10        3.78\n# ℹ 990 more rows\nggplot(virtual_mean_weight, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.04, boundary = 3.5, color = \"white\") +\n  labs(x = \"Sample mean\", title = \"Histogram of 1000 sample means\") "},{"path":"sampling.html","id":"the-sampling-distribution-of-the-sample-mean","chapter":"7 Sampling","heading":"7.4.4 The sampling distribution of the sample mean","text":"case sample proportion, interested learning key characteristics sampling distribution sample mean, namely:center sampling distributionThe effect sampling variation sampling distribution effect sample size sampling variationThe shape sampling distribution","code":""},{"path":"sampling.html","id":"random-variable-sample-mean","chapter":"7 Sampling","heading":"7.4.5 Random variables","text":", use random variable formalize understanding sample distribution sample mean. Instead using Bernoulli trials case sample proportions, trials record almond weights. modify bowl activity slightly. lieu selecting sample almonds , randomly select one almond time, record weight almond return bowl selecting another almond, configuration weights chances almonds selected every time choose one.\nGetting sample 25 almonds performing trials 25 times get 25 weights. , average 25 numbers average weight mean weight sample 25 almonds. call sample mean.Using random variables, now let uppercase \\(X_1\\) random variable represents weight first almond selected, \\(X_2\\) weight second almond, . random variables can take possible almond weight value bowl.\nfirst trial completed, value \\(X_1\\) realized weight grams first almond selected. can represent value lowercase \\(x_1\\) longer random variable number can write \\(X_1 = x_1\\). second trial completed, \\(X_2 = x_2\\), lowercase \\(x_2\\) observed almond weight, .\nSince experiment perform 25 trials find average , average mean, trials carried , can also expressed random variable:\\[\\overline X = \\frac{X_1+X_2+\\dots+X_{25}}{25}.\\]Observe \\(\\overline X\\) average, mean, 25 trials. \\(\\overline X\\) called sample mean. Recall dealing proportions, trials Bernoulli trials, represented zeros ones. context, sample proportions special case sample means. trials use now restricted zeros ones, sample means longer sample proportions.example, let’s focus sample 25 almond weights used earlier sample mean:looking weights data frame almonds_sample, observe \\(X_1 = 3.0\\) grams, \\(X_2 = 3.9\\) grams, \\(X_3 = 4.0\\) grams, . view entire data frame, example running View(almonds_sample) R, check \\(X_{23} = 3.3\\), \\(X_{24} = 4.4\\), \\(X_{25} = 3.6\\), sample mean \\[\\overline X = \\frac{3.0+3.9+4.0+\\dots+3.3+4.4+3.6}{25} = \\frac{91.8}{25}=3.67.\\]R:, sample observed, random variable \\(\\overline X\\) realized \\(\\overline X = 3.67\\), sample mean 3.67 grams.\nNote possible values \\(\\overline X\\) can take possible sample means samples 25 almonds bowl. chances getting sample means determined configuration almond weights bowl.\\(\\overline X\\) constructed sample mean given random sample, sampling distribution sample mean precisely distribution \\(\\overline X\\). context, recall interested determining:center distribution \\(\\overline X\\)effect sampling variation distribution \\(\\overline X\\) effect sample size sampling variationThe shape distribution \\(\\overline X\\)dealing sample proportions, use simulations produce good approximations distribution \\(\\overline X\\), sample mean weight almonds. also work samples size 25, 50, 100 learn changes sample variation sample size changes.process follow:generate 1000 samples,calculate sample means almond weights, anduse draw histograms.three times size argument set 25, 50, 100, respectively. run following code segments individually compare resulting histograms.present three resulting histograms single row matching x y axes Figure 7.24 comparison among clear.\nFIGURE 7.24: Comparing histograms sample means using different sample sizes.\nObserve three histograms bell-shaped appear center around middle value highlighted line middle. addition, magnitude sampling variation decreases sample size increases. happened sampling distribution sample proportion, measures center dispersion distributions directly related parameters population: population mean, \\(\\mu\\), population standard deviation, \\(\\sigma\\). print parameters one time :simulations next. Recall expected value \\(\\overline X\\) value expect observe, average, take many sample means random samples given size. located center distribution \\(\\overline X\\). Similarly, standard error \\(\\overline X\\) measure dispersion magnitude sampling variation. standard deviation sample means calculated possible random samples given size. Using data wrangling code mean() sd() functions inside summarize() applied simulation values, can estimate expected value standard error \\(\\overline X\\). Three sets values found, one corresponding sample sizes presented Table 7.1.\nTABLE 7.1: Comparing expected values standard errors three different sample sizes\nsummary:estimated expected value either 3.65 3.64. either near equal \\(\\mu = 3.64\\), population mean weight almonds entire bowl.standard error decreases sample size increases. focus result \\(n=100\\), standard error 0.038. compared population standard deviation \\(\\sigma = 0.392\\) standard error one-tenth value \\(\\sigma\\). Similarly, \\(n=25\\) standard error 0.077 one-fifth value \\(\\sigma\\) pattern also holds \\(n=50\\). case sample proportion, standard error inversely proportional squared sample size used construct distribution \\(\\overline X\\). also theoretical result can expressed :\\[SE_{\\overline X} = \\frac{\\sigma}{\\sqrt {n}}\\]\n\\(n\\) sample size \\(\\sigma\\) population standard deviation.\nLearning check\n(LC7.15) increasing sample size affect standard error sample mean?. increases standard errorB. decreases standard errorC. effect standard errorD. affects standard deviation","code":"\nalmonds_sample# A tibble: 25 × 3\n# Groups:   replicate [1]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1  4645    3  \n 2         1  3629    3.9\n 3         1  4796    4  \n 4         1  2669    3.8\n 5         1  3488    4.3\n 6         1   105    4.1\n 7         1  1762    3.6\n 8         1  1035    4.2\n 9         1  4880    3.2\n10         1   398    4  \n# ℹ 15 more rows\nalmonds_sample |>\n  summarize(sample_mean_weight = mean(weight))# A tibble: 1 × 2\n  replicate sample_mean_weight\n      <int>              <dbl>\n1         1               3.67\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Calculating the 1000 sample means, each from random samples of size 25\nvirtual_mean_weight_25 <- almonds_bowl |> \n  rep_slice_sample(n = 25, reps = 1000)|>\n  summarize(mean_weight = mean(weight), n = n())\n\n# 1.b) Plot distribution via a histogram\nggplot(virtual_mean_weight_25, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.02, boundary = 3.6, color = \"white\") +\n  labs(x = \"Sample mean weights for random samples of 25 almonds\", title = \"25\") \n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Calculating the 1000 sample means, each from random samples of size 50\nvirtual_mean_weight_50 <- almonds_bowl |> \n  rep_slice_sample(n = 50, reps = 1000)|>\n  summarize(mean_weight = mean(weight), n = n())\n\n# 2.b) Plot distribution via a histogram\nggplot(virtual_mean_weight_50, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.02, boundary = 3.6, color = \"white\") +\n  labs(x = \"Sample mean weights for random samples of 50 almonds\", title = \"50\") \n\n# Segment 3: sample size = 100 ------------------------------\n# 3.a) Calculating the 1000 sample means, each from random samples of size 100\nvirtual_mean_weight_100 <- almonds_bowl |> \n  rep_slice_sample(n = 100, reps = 1000)|>\n  summarize(mean_weight = mean(weight), n = n())\n\n# 3.b) Plot distribution via a histogram\nggplot(virtual_mean_weight_100, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.02, boundary = 3.6, color = \"white\") +\n  labs(x = \"Sample mean weights for random samples of 100 almonds\", title = \"100\") \nalmonds_bowl |>\n  summarize(mu = mean(weight), sigma = sd(weight))# A tibble: 1 × 2\n     mu sigma\n  <dbl> <dbl>\n1  3.64 0.392\n# n = 25\nvirtual_mean_weight_25 |> \n  summarize(E_Xbar_25 = mean(mean_weight), sd = sd(mean_weight))\n\n# n = 50\nvirtual_mean_weight_50 |> \n  summarize(E_Xbar_50 = mean(mean_weight), sd = sd(mean_weight))\n\n# n = 100\nvirtual_mean_weight_100 |> \n  summarize(E_Xbar_100 = mean(mean_weight), sd = sd(mean_weight))"},{"path":"sampling.html","id":"CLT-mean","chapter":"7 Sampling","heading":"7.4.6 The Central Limit Theorem revisited","text":"Finally, observe shapes histograms Figure 7.24 bell-shaped seem approximate normal distribution. proportions, Figures 7.25 7.26 compare histograms theoretical curve normal distribution.\nFIGURE 7.25: distribution sample mean (n=25).\n\nFIGURE 7.26: distribution sample mean (n=100).\nconclude distribution \\(\\overline X\\), , sampling distribution sample mean, sample size large enough, follows approximately normal distribution mean equal population mean, \\(\\mu\\), standard deviation equal population standard deviation divided square root sample size, \\(\\sigma/\\sqrt{n}\\). can write \\[\\overline X \\sim Normal \\left(\\mu, \\frac{\\sigma}{\\sqrt n}\\right)\\]\nLearning check\n(LC7.16) following cases, explain whether sampling distribution sample mean approximates normal distribution.population distribution normal.sample size large.sample size sufficiently large, regardless population distribution.population distribution uniform.","code":""},{"path":"sampling.html","id":"sampling-other-scenarios","chapter":"7 Sampling","heading":"7.5 The sampling distribution in other scenarios","text":"Sections 7.1, 7.3, 7.4, provided information expected value, standard error, shape sampling distribution statistic interest sample proportions sample means. possible study sampling distribution statistics. section explore .","code":""},{"path":"sampling.html","id":"sampling-distribution-for-two-samples","chapter":"7 Sampling","heading":"7.5.1 Sampling distribution for two samples","text":"Assume like compare parameters two populations, example, means proportion populations. random sample taken first population another random sample, independent first, retrieved second population. can use statistic sample, sample mean sample proportion, use produce sampling distributions depend two independent samples. provide two examples illustrate sampling distributions affected.","code":""},{"path":"sampling.html","id":"difference-in-sample-means","chapter":"7 Sampling","heading":"Difference in sample means","text":"problem hand chocolate-covered almonds’ weight almonds bowl needs compared chocolate-covered coffee beans’ weight coffee beans different bowl. statistic now consider difference sample means samples taken two bowls. happens, properties presented single sample mean sample proportion can extended directly two-sample problems.now provide mathematical details problem. assume chocolate-covered almonds’ weight population mean standard deviation given \\(\\mu_1\\) \\(\\sigma_1\\) chocolate-covered coffee beans’ weight population mean standard deviation given \\(\\mu_2\\) \\(\\sigma_2\\).sampling exercise two components. First, take random sample size \\(n_1\\) almonds’ bowl find sample mean. let \\(\\overline X_1\\) represent possible sample mean values possible almond sample. , let \\(n_2\\) \\(\\overline X_2\\) represent similar quantities coffee beans’ bowl. compare two sample means, look difference, \\(\\overline X_1 - \\overline X_2\\). distribution \\(\\overline X_1 - \\overline X_2\\) sampling distribution difference sample means.expected value standard error \\(\\overline X_1 - \\overline X_2\\) given \\(\\mu_1 - \\mu_2\\) \\[\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}},\\]respectively. distributions \\(X_1\\) \\(X_2\\) approximately normal due CLT, distribution \\(\\overline X_1 - \\overline X_2\\). can write properties :\\[\\overline X - \\overline Y \\sim Normal \\left(\\mu_1 - \\mu_2, \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}} \\right)\\]Observe standard deviation difference sum squared standard deviations sample mean. reason add standard deviations instead subtract whether add subtract statistics, effectively adding uncertainty results dispersion increases .\nLearning check\n(LC7.17) context comparing two samples, add variances (squared standard deviations) instead subtracting finding standard error difference?. variances always cancel outB. adding variances reflects total uncertainty samplesC. subtracting variances always gives negative resultsD. variances related standard error","code":""},{"path":"sampling.html","id":"difference-in-sample-proportions","chapter":"7 Sampling","heading":"Difference in sample proportions","text":"Comparing two sample proportions can useful. may interested comparing proportion patients improve using one treatment versus proportion patients improve using different treatment, proportion winter accidents highway using one type tire versus another.illustrate sampling distribution works difference sample proportions, modify examples used earlier.\nAssume want compare proportion red balls first bowl proportion almonds heavier 3.8 grams second bowl. example shows one way convert numeric data like almond weights Boolean result instead.\nstatistic now consider difference sample proportions samples retrieved two bowls.\nsamples bowl need size; example, can take samples size \\(n_1 = 50\\) first bowl samples size \\(n_2 = 60\\) second bowl.proceed bytaking random sample first bowl,calculating sample proportion red balls,getting random sample second bowl,calculating proportion almonds heavier 3.8 grams, andfinding difference sample proportion red balls minus sample proportion almonds greater 3.8 grams (resulting statistic).can use R produce required virtual samples differences. use approximate sampling distribution difference sample proportions.sampling exercise two components. First, take random sample \\(n_1 = 50\\) balls bowl red balls calculate sample proportion red balls. , let \\(\\overline X_1\\) represent possible values sample proportion can take possible sample. Recall \\(\\overline X_1\\) sample proportion context, also sample mean Bernoulli trials. Second, let \\(n_2 = 60\\) represent sample size used samples almonds’ bowl random variable \\(\\overline X_2\\) represent possible values sample proportion almonds greater 3.8 grams can take possible sample.compare two sample proportions, find difference, \\(\\overline X_1 - \\overline X_2\\). distribution \\(\\overline X_1 - \\overline X_2\\) sampling distribution difference sample proportions. use virtual sampling approximate distribution. use rep_slice_sample summarize functions produce random samples necessary sample proportions, respectively. total 1000 random samples sample proportions acquired bowl appropriate sample sizes, 50 60, respectively. Moreover, inner_join function introduced Section 3.7 used merge sample proportions single data frame difference sample proportions calculated replication.results stored data frame prop_joined. variable prop_diff data frame represents difference sample proportions. present first 10 values data frame:, build histogram 1000 differences Figure 7.27.\nFIGURE 7.27: distribution 1000 differences sample proportions based 1000 random samples size 50 first bowl 1000 random samples size 60 second bowl.\nsampling distribution difference sample proportions also looks bell-shaped appears centered negative value somewhere around -0.05. happened single sample proportion sample mean, sampling distribution difference sample proportions also follows normal distribution expected difference well standard error rely information population sample size.mathematical details: \\(\\overline X_1\\) random variable represents sample proportion red balls size \\(n_1 = 50\\), \\(\\overline X_2\\) random variable represents sample proportion almonds heavier 3.8 grams, taken samples size \\(n_2 = 60\\). proportion red balls population proportion standard deviation given \\(p_1\\) \\(\\sigma_1 = \\sqrt{p_1(1-p_1)}\\) almonds’ weight population proportion standard deviation given \\(p_2\\) \\(\\sigma_2 = \\sqrt{p_2(1-p_2)}\\).expected value standard error difference, \\(\\overline X_1 - \\overline X_2\\), given \\(p_1 - p_2\\) \\[\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}},\\]respectively. distributions \\(X_1\\) \\(X_2\\) approximately normal due CLT, distribution \\(\\overline X_1 - \\overline X_2\\). can write properties :\nLearning check\n(LC7.18) sampling distribution difference sample proportions expected look like samples large enough?. Uniform.B. Bell-shaped, approximating normal distribution.C. Bimodal.D. Skewed right.","code":"\nvirtual_prop_red <- bowl |> \n  rep_slice_sample(n = 50, reps = 1000) |> \n  summarize(prop_red = mean(color == \"red\"))\nvirtual_prop_almond <- almonds_bowl |>\n  rep_slice_sample(n = 60, reps = 1000) |>\n  summarize(prop_almond = mean(weight > 3.8))\nprop_joined <- virtual_prop_red |>\n  inner_join(virtual_prop_almond, by = \"replicate\") |>\n  mutate(prop_diff = prop_red - prop_almond)\nprop_joined# A tibble: 1,000 × 4\n   replicate prop_red prop_almond prop_diff\n       <int>    <dbl>       <dbl>     <dbl>\n 1         1     0.24       0.45    -0.21  \n 2         2     0.46       0.5     -0.0400\n 3         3     0.38       0.35     0.0300\n 4         4     0.36       0.433   -0.0733\n 5         5     0.38       0.367    0.0133\n 6         6     0.3        0.317   -0.0167\n 7         7     0.42       0.383    0.0367\n 8         8     0.42       0.483   -0.0633\n 9         9     0.32       0.5     -0.18  \n10        10     0.48       0.433    0.0467\n# ℹ 990 more rows\nggplot(prop_joined, aes(x = prop_diff)) +\n  geom_histogram(binwidth = 0.04, boundary = 0, color = \"white\") +\n  labs(x = \"Difference in sample proportions\", \n       title = \"Histogram of 1000 differences in sample proportions\") "},{"path":"sampling.html","id":"sampling-final-remarks","chapter":"7 Sampling","heading":"7.6 Summary and final remarks","text":"","code":""},{"path":"sampling.html","id":"summary-of-scenarios","chapter":"7 Sampling","heading":"7.6.1 Summary of scenarios","text":"cases sampling distribution can determined. example, performing linear regression, can find sampling distribution slope regression line behavior similar described . discuss inference context linear regression Chapter 10. now, present summary different scenarios presented chapter.\nTABLE 7.2: Scenarios sampling inference\n","code":""},{"path":"sampling.html","id":"additional-resources-5","chapter":"7 Sampling","heading":"7.6.2 Additional resources","text":"R script file R code used chapter available .","code":""},{"path":"sampling.html","id":"whats-to-come-6","chapter":"7 Sampling","heading":"7.6.3 What’s to come?","text":"upcoming Chapter 8 delve deeper concept statistical inference, building upon foundations already established chapter sampling. chapter introduces us idea estimating population parameters using sample data, key aspect inferential statistics.explore construct interpret confidence intervals, particularly focusing understanding imply population parameters. involves grasping concept confidence level recognizing limitations proper usage confidence intervals. chapter designed enhance practical understanding examples applications, enabling us apply concepts real-world scenarios.","code":""},{"path":"confidence-intervals.html","id":"confidence-intervals","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8 Estimation, Confidence Intervals, and Bootstrapping","text":"studied sampling Chapter 7. Recall, example, getting many random samples red white balls bowl, finding sample proportions red balls samples, studying distribution sample proportions. can summarize findings follows:sampling distribution sample proportion follows, approximately, normal distribution,expected value sample proportion, located center distribution, exactly equal population proportion, andthe sampling variation, measured standard error sample proportion, equal standard deviation population divided square root sample size used collect samples.Similarly, sampling chocolate-covered almonds getting sample mean weight sample, characteristics described also encountered sampling distribution sample mean; namely,sampling distribution sample mean follows, approximately, normal distribution;expected value sample mean population mean, andthe standard error sample mean standard deviation population divided square root sample size.Moreover, characteristics also apply sampling distributions difference sample means, difference sample proportions, others. Recall sampling distribution restricted distribution population. long samples taken fairly large use appropriate standard error, can generalize results appropriately.study sampling distribution motivated another question yet answered: can determine average weight almonds access entire bowl? seen using simulations Chapter 7 average sample means, derived many random samples, fairly close expected value sample mean, precisely population mean weight.However, real-life situations, access many random samples, single random sample. chapter introduces methods techniques can help us approximate information entire population, population mean weight, using single random sample population. undertaking called estimation, central Statistics Data Science.introduce statistical jargon estimation. using sample statistic estimate population parameter, e.g., using sample mean random sample estimate population mean, call statistic point estimate make emphasis single value used estimate parameter interest.\nNow, may recall , due sampling variation, sample mean typically match population mean exactly, even sample large.\naccount variation, use interval estimate parameter instead single value, appropriately call interval estimate , given level accuracy, confidence interval population parameter. chapter, explain find confidence intervals, advantages using , different methods can used determine .Section 8.1 introduce method build confidence interval population mean uses random sample taken theoretical characteristics sampling distribution discussed Chapter 7.\ncall theory-based approach constructing intervals.\nSection 8.2 introduce another method, called bootstrap, produces confidence intervals resampling large number times original sample. Since resampling done via simulations, call simulation-based approach constructing confidence intervals. \nFinally, Section 8.5 summarize present extensions methods.","code":""},{"path":"confidence-intervals.html","id":"CI-packages","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Needed packages","text":"needed, read Section 1.3 information install load R packages.Recall loading tidyverse package loads many packages encountered earlier. details refer Section 4.4. packages moderndive infer contain functions data frames used chapter.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)"},{"path":"confidence-intervals.html","id":"theory-based-CI","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1 Tying the sampling distribution to estimation","text":"section revisit chocolate-covered almonds example introduced Chapter 7 results sampling distribution sample mean weight almonds, time use information context estimation.start introducing reviewing terminology using almonds example. bowl chocolate-covered almonds population interest. parameter interest population mean weight almonds bowl, \\(\\mu\\). quantity want estimate.want use sample mean estimate parameter. call sample mean estimator estimate \\(\\mu\\), population mean weight.\ndifference estimator estimate worth discussing.illustration, decide take random sample 100 almonds bowl use sample mean weight estimate population mean weight. words, intend sum 100 almonds’ weights, divide sum 100, use value estimate population mean weight. refer sample mean describe process via equation, sample mean weight called estimator population mean weight.\nSince different samples produce different sample means, sample mean estimator random variable \\(\\overline  X\\) described Section 7.4.5. learned studying sampling distribution Chapter 7, know estimator follows, approximately, normal distribution; expected value equal population mean weight. standard deviation, also called standard error, \\[SE(\\bar x) = \\frac{\\sigma}{\\sqrt{n}}\\]\\(n = 100\\) example \\(\\sigma\\) population standard deviation almonds’ weights.took random sample 100 almonds’ weights one shown stored moderndive package name:can use calculate sample mean weight:\\(\\overline{x} = 3.682\\) grams estimate population mean weight.\nsummary, estimator procedure, equation, method used sample estimate parameter sample retrieved many useful properties discussed Chapter 7. moment sample taken equation sample mean applied sample, resulting number estimate.sample mean, estimator estimate population mean, central component material developed chapter.\n, note quantity interest.\nexample, population standard deviation almonds’ weight, denoted Greek letter \\(\\sigma\\), parameter sample standard deviation can estimator estimate parameter.Furthermore, shown Chapter 7 expected value sample mean equal population mean. happens, call sample mean unbiased estimator population mean. mean sample mean equal population mean; sample means greater others smaller , average, equal population mean. general, expected value estimator equal parameter trying estimate, call estimator unbiased. , estimator biased.now revisit almond activity study sampling distribution sample mean can help us build interval estimates population mean.\nLearning check\n(LC8.1) expected value sample mean weight almonds large sample according sampling distribution theory?. always larger population mean.B. always smaller population mean.C. exactly equal population mean.D. equal population mean average may vary single sample.(LC8.2) point estimate differ interval estimate context statistical estimation?. point estimate uses multiple values estimate parameter; interval estimate uses single value.B. point estimate single value used estimate parameter; interval estimate provides range values within parameter likely falls.C. point estimate mean multiple samples; interval estimate median.D. point estimate interval estimate can used interchangeably.","code":"\nalmonds_sample_100# A tibble: 100 × 3\n# Groups:   replicate [1]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1   166    4.2\n 2         1  1215    4.2\n 3         1  1899    3.9\n 4         1  1912    3.8\n 5         1  4637    3.3\n 6         1   511    3.5\n 7         1   127    4  \n 8         1  4419    3.5\n 9         1  4729    4.2\n10         1  2574    4.1\n# ℹ 90 more rows\nalmonds_sample_100 |>\n  summarize(sample_mean = mean(weight))# A tibble: 1 × 2\n  replicate sample_mean\n      <int>       <dbl>\n1         1       3.682"},{"path":"confidence-intervals.html","id":"revisit-almond","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1.1 Revisiting the almond activity for estimation","text":"Chapter 7, one activities take many random samples size 100 bowl 5,000 chocolate-covered almonds. Since access contents entire bowl, can compute population parameters:total number almonds bowl 5,000. population mean \\[\\mu = \\sum_{=1}^{5000}\\frac{x_i}{5000}=3.645,\\]population standard deviation, pop_sd(), moderndive, defined \\[\\sigma = \\sum_{=1}^{5000} \\frac{(x_i - \\mu)^2}{5000}=0.392.\\]keep numbers future reference determine well methods estimation , recall real-life situations access population values population mean \\(\\mu\\) unknown. information one random sample. example, assume know almonds_sample_100 object stored moderndive.\nID variable shows almond chosen bowl corresponding weight. Using sample calculate sample statistics:one activities performed Chapter 7 took many random samples, calculated sample means, constructed histogram using sample means, showed histogram good approximation sampling distribution sample mean.\nredraw Figure 7.26 Figure 8.1.\nFIGURE 8.1: distribution sample mean.\nhistogram Figure 8.1 drawn using many sample mean weights random samples size \\(n=100\\).\nadded red smooth curve density curve normal distribution appropriate expected value standard error calculated sample distribution.\nred dot represents population mean \\(\\mu\\), unknown parameter trying estimate. blue right sample mean \\(\\overline{x} = 3.682\\) random sample stored almonds_sample_100.real-life applications, sample mean taken sample, distribution population population mean unknown, location blue dot respect red dot also unknown. However, construct interval centered blue dot, long wide enough interval contain red dot.\nunderstand better, need learn additional properties normal distribution.","code":"\nalmonds_bowl |> \n  summarize(population_mean = mean(weight), \n            population_sd = pop_sd(weight))# A tibble: 1 × 2\n  population_mean population_sd\n            <dbl>         <dbl>\n1         3.64496      0.392070\nalmonds_sample_100 |> \n  summarize(mean_weight = mean(weight), \n            sd_weight = sd(weight), \n            sample_size = n())# A tibble: 1 × 4\n  replicate mean_weight sd_weight sample_size\n      <int>       <dbl>     <dbl>       <int>\n1         1       3.682  0.362199         100"},{"path":"confidence-intervals.html","id":"the-normal-distribution","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1.2 The normal distribution","text":"random variable can take different values. values can represented one intervals, likelihood values can expressed graphically density curve Cartesian coordinate system two dimensions. horizontal axis (X-axis) represents values random variable can take height density curve (Y-axis) provides graphical representation likelihood values; higher curve likely values . addition, total area density curve always equal 1. set values random variable can take alongside likelihood call distribution random variable.normal distribution distribution special type random variable. density curve distinctive bell shape, fully defined two values: (1) mean expected value random variable, \\(\\mu\\), located X-axis center density curve (highest point), (2) standard deviation, \\(\\sigma\\), reflects dispersion random variable; greater standard deviation wider curve appears.\nFigure 8.2, plot density curves three random variables, following normal distributions:solid line represents normal distribution \\(\\mu = 5\\) & \\(\\sigma = 2\\).dotted line represents normal distribution \\(\\mu = 5\\) & \\(\\sigma = 5\\).dashed line represents normal distribution \\(\\mu = 15\\) & \\(\\sigma = 2\\).\nFIGURE 8.2: Three normal distributions.\nrandom variable follows normal distribution can take values real line, values (X-axis) correspond peak density curve likely corresponding tails.\ndensity curve drawn solid line mean one drawn dotted line, \\(\\mu = 5\\), former exhibits less dispersion, measured standard deviation \\(\\sigma =2\\), latter, \\(\\sigma = 5\\). Since total area density curve equal 1, wider curve shorter height preserve property. hand, density curve drawn solid line standard deviation one drawn dashed line, \\(\\sigma = 2\\), latter greater mean, \\(\\mu = 15\\), former, \\(\\mu = 5\\), look latter centered farther right X-axis former.","code":""},{"path":"confidence-intervals.html","id":"the-standard-normal-distribution","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"The standard normal distribution","text":"special normal distribution mean \\(\\mu\\) = 0 standard deviation \\(\\sigma\\) = 1. called standard normal distribution, represented density curve called \\(z\\)-curve. random variable \\(Z\\) follows standard normal distribution, realization random variable called standard value \\(z\\)-value. \\(z\\)-value also represents number standard deviations mean, positive, mean, negative. example, \\(z=5\\), value observed represents realization random variable \\(Z\\) five standard deviations mean, \\(\\mu = 0\\).","code":""},{"path":"confidence-intervals.html","id":"linear-transformations-of-random-variables-that-follow-the-normal-distribution","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Linear transformations of random variables that follow the normal distribution","text":"linear transformation random variable transforms original variable new random variable adding, subtracting, multiplying, dividing constants original values. resulting random variable different mean standard deviation. interesting transformation turning random variable another \\(\\mu = 0\\) \\(\\sigma = 1\\). happens say random variable standardized.property normal distribution linear transformation random variable follows normal distribution results new random variable also follows normal distribution, potentially different mean standard deviation. particular, can turn random variable follows normal distribution random variable follows standard normal distribution. example, value \\(x = 11\\) comes normal distribution mean \\(\\mu =5\\) standard deviation \\(\\sigma = 2\\), \\(z\\)-value\\[z = \\frac{x - \\mu}{\\sigma} = \\frac{11 - 5}{2} = 3\\]\ncorresponding value standard normal curve. Moreover, determined \\(x = 11\\) example precisely \\(3\\) standard deviations mean.","code":""},{"path":"confidence-intervals.html","id":"finding-probabilities-under-a-density-curve","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Finding probabilities under a density curve","text":"random variable can represented density curve, probability random variable takes value given interval (X-axis) equal area density curve interval. know equation represents density curve, use mathematical technique calculus known integration determine area. case normal curve, integral interval close-form solution, solution calculated using numerical approximations.Please review Appendix online provide R code work different areas, probabilities, values normal density curve. , place focus insights specific values areas without dedicating time calculations.assume random variable \\(Z\\) follows standard normal distribution. like know likely random variable take value within one standard deviation mean. Equivalently, probability observed value \\(z\\) (X-axis) -1 1 shown Figure 8.3?\nFIGURE 8.3: Normal area within one standard deviation.\nCalculations show area 0.6827 (68.27%) total area curve. equivalent saying probability getting value \\(-1\\) 1 standard normal 68.27%. also means random variable representing experiment follows normal distribution, probability outcome experiment within one standard deviation mean 68.27%. Similarly, area standard normal density curve -2 2 shown Figure 8.4.\nFIGURE 8.4: Normal area within two standard deviations.\nCalculations show area equal 0.9545 95.45%. random variable representing experiment follows normal distribution, probability outcome experiment within two standard deviations mean 95.45%. also common practice use exact number standard deviations correspond area around mean exactly equal 95% (instead 95.45%).\nPlease see Appendix online produce calculations R.\nresult area density curve around mean exactly equal 0.95, 95%, area within 1.96 standard deviation mean. Remember number used times later sections.summary, possible outcomes experiment can expressed random variable follows normal distribution, probability getting result within one standard deviation mean 68.27%, within 2 standard deviations form mean 95.45%, within 1.96 standard deviations mean 95%, within 3 standard deviations mean 99.73%, name .\nSpend moments grasping idea; observe, example, almost impossible observe outcome represented number five standard deviations mean chances happening near zero. now ready return main goal: find interval estimate population mean based single sample.\nLearning check\n(LC8.3) population mean (\\(\\mu\\)) represent context almond activity?. average weight 100 randomly sampled almonds.B. weight heaviest almond bowl.C. average weight 5,000 almonds bowl.D. total weight almonds bowl.(LC8.4) following statements best describes population standard deviation (\\(\\sigma\\)) almond activity?. measures average difference almond’s weight sample mean weight.B. measures average difference almond’s weight population mean weight.C. equal square root sample variance.D. always smaller population mean.(LC8.5) use sample mean estimate population mean almond activity?. sample mean always larger population mean.B. sample mean good estimator population mean due unbiasedness.C. sample mean requires less computational effort population mean.D. sample mean eliminates sampling variation.","code":""},{"path":"confidence-intervals.html","id":"CI-general","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1.3 The confidence interval","text":"continue using example try estimate population mean weight almonds random sample 100 almonds. showed Chapter 7 sampling distribution sample mean weight almonds approximates normal distribution expected value equal population mean weight almonds standard error equal \\[SE(\\bar x) = \\frac{\\sigma}{\\sqrt {100}}.\\]\nSubsection 8.1.1 showed population almonds, \\(\\mu =3.645\\) grams \\(\\sigma = 0.392\\), standard error sampling distribution \\[SE(\\bar x) = \\frac{\\sigma}{\\sqrt{100}} = \\frac{0.392}{\\sqrt{100}} = 0.039\\]\ngrams. Figure 8.5 plot density curve distribution using values.\nFIGURE 8.5: Normal density curve sample mean weight almonds.\nhorizontal axis (X-axis) represents sample means can determine possible random samples 100 almonds. red dot represents expected value sampling distribution, \\(\\mu = 3.64\\), located X-axis center distribution. density curve’s height can thought likely sample means observed. example, likely get random sample sample mean around \\(3.645\\) grams (corresponds highest point curve) get sample sample mean around \\(3.5\\) grams, since curve’s height almost zero value. blue dot sample mean sample 100 almonds, \\(\\overline{x} = 3.682\\) grams. located 0.037 grams population mean weight. far 0.037 grams? helpful express distance standardized values:\\[\\frac{3.682 - 3.645}{0.039} = 0.945\\]0.037 grams 0.945 standard errors population mean.real-life situations, population mean, \\(\\mu\\), unknown distance sample mean \\(\\mu\\) also unknown.\nhand, sampling distribution sample mean follows normal distribution. Based earlier discussion areas normal curve, 95% chance value observed within 1.96 standard deviations mean. context problem, 95% chance sample mean weight within 1.96 standard errors population mean weight. shown earlier, sample mean calculated example 0.945 standard errors population mean, well within reasonable range.Think result. take different random sample 100 almonds, sample mean likely different, still 95% chance new sample mean within 1.96 standard errors population mean.can finally construct interval estimate takes advantage configuration. center interval sample mean observed extend side magnitude equivalent 1.96 standard errors. lower upper bounds interval :\\[\\begin{aligned}\\left(\\overline{x} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\\quad \\overline{x} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right) &= \\left(3.682 - 1.96 \\cdot \\frac{0.392}{\\sqrt{100}},\\quad 3.682 + 1.96 \\cdot \\frac{0.392}{\\sqrt{100}}\\right)\\\\\n&= (3.605, 3.759)\\end{aligned}\\]R code can used calculate lower upper bounds:functions mean() length() find sample mean weight sample size, respectively, sample almonds’ weights almonds_sample_100. number 1.96 corresponds number standard errors needed get 95% area normal distribution population standard deviation sigma 0.392 found Subsection 8.1.1. Figure 8.6 shows interval horizontal blue line. Observe population mean \\(\\mu\\) part interval.\nFIGURE 8.6: population mean interval?\nSince 1.96 standard errors used construction interval, call 95% confidence interval. confidence interval can viewed interval estimator population mean. Compare interval estimator sample mean point estimator. latter estimates parameter single number, former provides entire interval account location parameter. apt analogy involves fishing. Imagine single fish swimming murky water. fish visible movement produces ripples surface can provide limited information fish’s location. capture fish, one use spear net. information limited, throwing spear ripples may capture fish likely miss .Throwing net around ripples, hand, may give much higher likelihood capturing fish. Using sample mean estimate population mean like throwing spear ripples hopes capturing fish. Constructing confidence interval may include population mean like throwing net surround ripples. Keep analogy mind, revisit later sections.\nLearning check\n(LC8.6) standard error sample mean weight almonds calculated context example?. dividing sample mean population standard deviation.B. dividing population standard deviation square root sample size.C. multiplying sample mean square root sample size.D. dividing population mean sample size.(LC8.7) 95% confidence interval represent context almond weight estimation?. 95% chance sample mean within 1.96 standard deviations population mean.B. interval contain 95% almond weights sample.C. 95% chance population mean falls within 1.96 standard errors sample mean.D. sample mean exactly equal population mean 95% time.","code":"\nalmonds_sample_100 |>\n  summarize(\n    sample_mean = mean(weight),\n    lower_bound = mean(weight) - 1.96 * sigma / sqrt(length(weight)),\n    upper_bound = mean(weight) + 1.96 * sigma / sqrt(length(weight))\n  )# A tibble: 1 × 4\n  replicate sample_mean lower_bound upper_bound\n      <int>       <dbl>       <dbl>       <dbl>\n1         1       3.682     3.60515     3.75885"},{"path":"confidence-intervals.html","id":"t-distribution-CI","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1.4 The t distribution","text":"Recall due Central Limit Theorem, sampling distribution sample mean approximately normal mean equal population mean \\(\\mu\\) standard deviation given standard error \\(SE(\\overline X) = \\sigma/\\sqrt{n}\\). can standardize sample mean \\(\\overline{x}\\) \\[z = \\frac{\\overline{x} - \\mu}{\\sigma/\\sqrt{n}}\\]corresponding value standard normal distribution.construction interval Figure 8.6 assumed population standard deviation, \\(\\sigma\\), known, therefore used find confidence interval. Nevertheless, real-life applications, population standard deviation also unknown.\nInstead, use sample standard deviation, \\(s\\), sample , estimator population standard deviation \\(\\sigma\\). estimated standard error given \\[\\widehat{SE}(\\overline X) = \\frac{s}{\\sqrt n}.\\]using sample standard deviation estimate standard error, introducing additional uncertainty model. example, try standardize value, get\\[t = \\frac{\\overline{x} - \\mu}{s/\\sqrt{n}}.\\]using sample standard deviation equation since sample standard deviation changes sample sample, additional uncertainty makes values \\(t\\) longer normal. Instead follow new distribution called \\(t\\) distribution.\\(t\\) distribution similar standard normal; density curve also bell-shaped, symmetric around zero, tails \\(t\\) distribution little thicker standard normal.\naddition, \\(t\\) distribution requires one additional parameter, degrees freedom. sample mean problems, degrees freedom needed exactly \\(n-1\\), size sample minus one. Figure 8.7 shows density curves ofthe standard normal density curve, black,\\(t\\) density curve t distribution 2 degrees freedom, dotted blue, anda \\(t\\) density curve t distribution 10 degrees freedom, dashed red.\nFIGURE 8.7: standard normal two t-distributions.\nObserve \\(t\\) density curve dashed red (\\(t\\) 10 degrees freedom) gets closer standard normal density curve, \\(z\\)-curve, solid black, \\(t\\) curve dotted blue (\\(t\\) 2 degrees freedom). greater number degrees freedom, closer \\(t\\) density curve \\(z\\) curve. change makes calculations slightly different.Please see Appendix online calculations probabilities \\(t\\) density curves different degrees freedom.\nUsing knowledge, calculation specific example shows 95% sample means within 1.98 standard errors population mean weight. number standard errors needed different , 1.98 versus 1.96, degrees freedom fairly large.Using information, can construct 95% confidence interval based entirely sample information using sample mean sample standard deviation. calculate values almonds_sample_100:Observe sample standard deviation \\(s = 0.362\\) different population standard deviation \\(\\sigma = 0.392\\). center confidence interval observed sample mean now extend interval 1.98 standard errors side. lower upper bounds confidence interval :\\[\n\\begin{aligned}\n\\left(\\overline{x} - 1.98 \\frac{s}{\\sqrt{n}},\\quad \\overline{x} + 1.98 \\frac{s}{\\sqrt{n}}\\right) &= \\left(3.682 - 1.98 \\cdot \\frac{0.362}{\\sqrt{100}}, 3.682 + 1.98 \\cdot \\frac{0.362}{\\sqrt{100}}\\right) \\\\\n&= (3.498, 3.846))\n\\end{aligned}\n\\]can also compute lower upper bounds:confidence interval computed , using sample standard deviation \\(t\\) distribution, almost one attained using population standard deviation standard normal distribution, difference 0.005 units upper lower bound. happens sample size 100, \\(t\\)-curve \\(z\\)-curve almost identical also sample standard deviation similar population standard deviation. always case occasionally can observe greater differences; , general, results fairly similar.importantly, confidence interval constructed contains population mean \\(\\mu = 3.645\\), result needed. Recall confidence interval interval estimate parameter interest, population mean weight almonds.\ncan summarize results far:size used random sample large enough, sampling distribution sample mean follows, approximately, normal distribution.Using sample mean observed standard error sampling distribution, can construct 95% confidence intervals population mean. formula intervals (\\(n\\) sample size used) given \\[\\left(\\overline{x} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\\quad \\overline{x} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right).\\]population standard deviation unknown (almost always case), sample standard deviation used estimate standard error. produces additional variability standardized values follow \\(t\\) distribution \\(n-1\\) degrees freedom. formula 95% confidence intervals sample size \\(n=100\\) given \\[\\left(\\overline{x} - 1.98 \\frac{s}{\\sqrt{100}},\\quad \\overline{x} + 1.98 \\frac{s}{\\sqrt{100}}\\right).\\]method construct 95% confidence intervals guarantees long-run 95% possible samples, intervals determined include population mean. also guarantees 5% possible samples lead intervals include population mean.constructed intervals 95% level confidence, can construct intervals level confidence. change equations number standard errors needed.","code":"\nalmonds_sample_100 |>\n  summarize(sample_mean = mean(weight), sample_sd = sd(weight))# A tibble: 1 × 3\n  replicate sample_mean sample_sd\n      <int>       <dbl>     <dbl>\n1         1       3.682  0.362199\nalmonds_sample_100 |>\n  summarize(sample_mean = mean(weight), sample_sd = sd(weight),\n            lower_bound = mean(weight) - 1.98*sd(weight)/sqrt(length(weight)),\n            upper_bound = mean(weight) + 1.98*sd(weight)/sqrt(length(weight)))# A tibble: 1 × 5\n  replicate sample_mean sample_sd lower_bound upper_bound\n      <int>       <dbl>     <dbl>       <dbl>       <dbl>\n1         1       3.682  0.362199     3.61028     3.75372"},{"path":"confidence-intervals.html","id":"interpreting-confidence-intervals","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.1.5 Interpreting confidence intervals","text":"used sample almonds_sample_100, constructed 95% confidence interval population mean weight almonds, showed interval contained population. result surprising expect intervals include population mean 95% possible random samples. repeat interval construction many random samples. Figure 8.8 presents results one hundred 95% confidence intervals.\nFIGURE 8.8: One hundred 95% confidence intervals whether population mean captured .\nNote interval built using different random sample. red vertical line drawn location population mean weight, \\(\\mu = 3.645\\). horizontal lines represent one hundred 95% confidence intervals found. gray confidence intervals cross red vertical line contain population mean. black confidence intervals .result motivates meaning 95% confidence interval: construct intervals using procedure described earlier every possible random sample, 95% intervals include population mean 5% .course, situations, impractical impossible take every possible random sample. Still, large number random samples, result approximately correct. Figure 8.8, example, 5 100 confidence intervals include population mean, 95% . won’t always match perfectly like , proportions match pretty close confidence level chosen.term “95% confidence” invites us think talking probabilities chances. Indeed , subtle way. random sample procured, 95% chance confidence interval constructed using prospective random sample, interval contain population mean. moment random sample attained, interval constructed either contains population mean ; certainty, longer chance involved. true even know population mean .\n95% confidence refers method process used prospective sample. confident follow process construct interval, 95% time random sample attained lead us produce interval contains population mean.hand, improper say … “95% chance confidence interval contains population mean.” Looking Figure 8.8, confidence intervals either contain \\(\\mu\\). confidence interval determined, either population mean included .literature, explanation encapsulated short-hand version: 95% confident interval contains population parameter. example, Subsection 8.1.4 95% confidence interval population mean weight almonds (3.498, 3.846), say: “95% confident population mean weight almonds 3.498 3.846 grams.”perfectly acceptable use short-hand statement, always remember 95% confidence refers process, method, can thought chance probability random sample acquired. ensure probability-type language misused, quotation marks sometimes put around “95% confident” emphasize short-hand version accurate explanation.\nLearning check\n(LC8.8) \\(t\\) distribution thicker tails compared standard normal distribution?. sample mean considered likely match population mean closely.B. \\(t\\) distribution designed work data follow normal distribution.C. assumes sample size always smaller applying \\(t\\) distribution.D. accounts extra uncertainty comes using sample standard deviation instead population standard deviation.(LC8.9) effect increasing degrees freedom \\(t\\) distribution?. tails distribution become thicker.B. tails distribution become thinner.C. distribution change degrees freedom.D. distribution becomes skewed right.","code":""},{"path":"confidence-intervals.html","id":"ci-width","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Understanding the width of a confidence interval","text":"confidence interval estimator population parameter. case almonds’ bowl constructed confidence interval population mean. equation construct 95% confidence interval \\[\\left(\\overline{x} - 1.96 \\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\nObserve confidence interval centered sample mean extends side 1.96 standard errors \\(1.96\\cdot \\sigma / \\sqrt{n}\\). quantity exactly half width confidence interval, called margin error. value population standard deviation, \\(\\sigma\\), beyond control, determined distribution experiment phenomenon studied. sample mean, \\(\\overline{x}\\), result depends random sample exclusively. hand, number 1.96 sample size, \\(n\\), values can changed researcher practitioner. play important role width confidence interval. study separately.","code":""},{"path":"confidence-intervals.html","id":"the-confidence-level","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"The confidence level","text":"mentioned earlier number 1.96 relates 95% confidence process show determine value. level confidence decision practitioner. want confident, say 98% 99% confident, just need adjust appropriate number standard errors needed. show determine number, use Figure 8.9 illustrate process.confidence level 0.95 (95%), area middle standard normal distribution 0.95. area shaded Figure 8.9.construct \\(\\alpha = 1 - \\text{confidence level} = 1 - 0.95 = 0.05\\). Think \\(\\alpha\\) total area tails.Since normal distribution symmetric, area tail \\(\\alpha/2 = 0.05/2 = 0.025\\).need exact number standard deviations produces shaded area. Since center standard normal density curve zero, shown Figure 8.9, normal curve symmetric, number standard deviations can represented \\(-q\\) \\(q\\), magnitude one positive negative.\nFIGURE 8.9: Normal curve shaded middle area 0.95\nR, function qnorm() finds value \\(q\\) area curve left value \\(q\\) given. example area left \\(-q\\) \\(\\alpha/2 = 0.05/2 = 0.025\\), soor 1.96 standard deviation mean. Similarly, total area curve left \\(q\\) total shaded area, 0.95, plus small white area left tail, \\(0.025\\), \\(0.95 + 0.025 = 0.975\\), soThat reason use 1.96 standard deviation calculating 95% confidence intervals. want retrieve 90% confidence interval? follow procedure:confidence level 0.90.\\(\\alpha = 1 - \\text{confidence level} = 1 - 0.90 = 0.10\\).area tail \\(\\alpha/2 = 0.10/2 = 0.05\\).area needed find \\(q\\) \\(0.90+0.05 = 0.95\\).want determine 90% confidence interval, need use 1.645 standard errors calculations. can update R code calculate lower upper bounds 90% confidence interval:Let’s one . want 80% confidence interval, \\(1 - 0.8 = 0.2\\), \\(0.2/2 = 0.1\\), \\(0.8+0.1 = 0.9\\), soWhen want calculate 80%, 90%, 95% confidence interval, need construct interval using 1.282, 1.645, 1.96 standard errors, respectively. confident want , larger number standard errors need use, wider confidence interval becomes. confidence interval estimator population mean, narrower , useful practical reasons. trade-width confidence interval confidence want .","code":"\nqnorm(0.025)[1] -1.96\nqnorm(0.975)[1] 1.96\nqnorm(0.95)[1] 1.65\nalmonds_sample_100 |>\n  summarize(sample_mean = mean(weight),\n            lower_bound = mean(weight) - qnorm(0.95)*sigma/sqrt(length(weight)),\n            upper_bound = mean(weight) + qnorm(0.95)*sigma/sqrt(length(weight)))# A tibble: 1 × 4\n  replicate sample_mean lower_bound upper_bound\n      <int>       <dbl>       <dbl>       <dbl>\n1         1       3.682     3.61751     3.74649\nqnorm(0.9)[1] 1.28"},{"path":"confidence-intervals.html","id":"the-sample-size","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"The sample size","text":"studied changes confidence level, can determine big random sample used. margin error 95% confidence interval \\[1.96\\cdot \\frac{\\sigma}{\\sqrt{n}}.\\]sample size increases, margin error decreases proportional square root sample size. example, secure random sample size 25, \\(1/\\sqrt{25} = 0.2\\), draw sample size 100, \\(1/\\sqrt{100} = 0.1\\). choosing larger sample size, four times larger, produce confidence interval half width. result worth considering.confidence interval estimator parameter interest, population mean weight almonds. Ideally, like build confidence interval high level confidence, example, 95% confidence. also want interval narrow enough provide useful information. example, assume get following 95% confidence intervals population mean weight almonds:2 4 grams, orbetween 3.51 3.64 grams, orbetween 3.539 3.545 grams.first interval seem useful , second works better, third tremendously accurate, 95% confident population mean within 0.006 grams. Obviously, always prefer narrower intervals, trade-offs need consider. always prefer high levels confidence, confident want wider interval . addition, larger random sample used, narrower confidence interval . Using large sample always preferred choice, trade-offs often external; collecting large samples expensive time-consuming. construction confidence intervals needs take account considerations.concluded theory-based approach construct confidence intervals. next section explore completely different approach construct confidence intervals later sections make comparisons methods.","code":""},{"path":"confidence-intervals.html","id":"simulation-based-CI","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.2 Estimation with the bootstrap","text":"1979, Brad Efron published article introducing method called bootstrap (Efron 1979) next summarized. random sample size \\(n\\) taken population.\nsample used find another sample, replacement, also size \\(n\\). called resampling replacement resulting sample called bootstrap sample. example, original sample \\(\\{4,2,5,4,1,3,7,4,6,1\\}\\), one particular bootstrap sample \\(\\{6, 4, 7, 4, 2, 7, 2, 5, 4, 1\\}.\\)\nObserve number 7 appears original sample, twice bootstrap sample;\nsimilarly, number 3 original sample appear bootstrap sample. uncommon bootstrap sample, numbers original sample repeated others included.basic idea bootstrap gain large number bootstrap samples, drawn original sample. , use bootstrap samples find estimates population parameters, standard errors, even density curve population. Using can construct confidence intervals, perform hypothesis testing, inferential methods.method takes advantage large number bootstrap samples can determined. several respects, exercise different sampling distribution explained Chapter 7. difference, albeit important one, sampling population, sampling original sample.\nmany different bootstrap samples get single sample? large number, actually. original sample 10 numbers, one shown , possible bootstrap sample size 10 determined sampling 10 times replacement, total number bootstrap samples \\(10^{10}\\) 10 billion different bootstrap samples. original sample 20 numbers, number bootstrap samples \\(20^{20}\\), number greater total number stars universe.\nEven modern powerful computers, onerous task calculate every possible bootstrap sample. Instead, thousand bootstrap samples retrieved, similar simulations performed Chapter 7, number often large enough provide useful results.Since Efron (Efron 1979) proposed bootstrap, statistical community embraced method. 1980s 1990s, many theoretical empirical results presented showing strength bootstrap methods. illustration, Efron (Efron 1979), Hall (Hall 1986), Efron Tibshirani(Efron Tibshirani 1986), Hall (Hall 1988) showed bootstrapping least good better existent methods, goal estimate standard error estimator find confidence intervals parameter. Modifications proposed improve algorithm situations basic method producing accurate results. continuous improvement computing power speed, advantages ready--use statistical software implementation, use bootstrap become popular many fields.illustration, interested mean population, \\(\\mu\\), collected one random sample, can gain large number bootstrap samples original sample, use calculate sample means, order sample means smallest largest, choose interval contains middle 95% sample means. simplest way find confidence interval based bootstrap. next subsections, explore incorporate similar methods construct confidence intervals.","code":""},{"path":"confidence-intervals.html","id":"revisit-almond-bootstrap","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.2.1 Bootstrap samples: revisiting the almond activity","text":"study understand behavior bootstrap samples, return example chocolate-covered almonds bowl. Recall bowl considered population almonds, interested estimating population mean weight almonds., access single random sample. section, use data frame almonds_sample_100, random sample 100 almonds taken earlier. call original sample, used section create bootstrap samples.\nfirst 10 rows shown:","code":"\nalmonds_sample_100# A tibble: 100 × 3\n# Groups:   replicate [1]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1   166    4.2\n 2         1  1215    4.2\n 3         1  1899    3.9\n 4         1  1912    3.8\n 5         1  4637    3.3\n 6         1   511    3.5\n 7         1   127    4  \n 8         1  4419    3.5\n 9         1  4729    4.2\n10         1  2574    4.1\n# ℹ 90 more rows"},{"path":"confidence-intervals.html","id":"constructing-a-bootstrap-sample-resampling-once","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Constructing a bootstrap sample: resampling once","text":"start constructing one bootstrap sample 100 almonds original sample 100 almonds. steps needed perform task manually:Step 1: Place original sample 100 almonds bag hat.Step 2: Mix bag contents, draw one almond, weigh , record weight seen Figure 8.10.\nFIGURE 8.10: Step 2: Weighing one almond random.\nStep 3: Put almond back bag! words, replace seen Figure 8.11.\nFIGURE 8.11: Step 3: Replacing almond.\nStep 4: Repeat Steps 2 3 total 99 times, resulting 100 weights.steps describe resampling replacement, resulting sample called bootstrap sample. procedure results almonds chosen almonds chosen . Resampling replacement induces sampling variation, every bootstrap sample can different .activity can performed manually following steps described . can also take advantage R code introduced Chapter 7 virtually.\ndata frame almonds_sample_100 contains random sample almonds taken population. show selected rows sample.use ungroup() select eliminate variable replicate almonds_sample_100 variable may create clutter resampling. can now create bootstrap sample also size 100 resampling replacement .used type R syntax many times Chapter 7.\nfirst select data frame almonds_sample_100 contains almonds’ weights original sample.\nperform resampling replacement : resample using rep_sample_n(), sample size 100 setting size = 100, replacement adding argument replace = TRUE, one time setting reps = 1.\nobject boot_sample bootstrap sample 100 almonds’ weights gained original sample 100 almonds’ weights. show first ten rows boot_sample:can also study characteristics bootstrap sample, sample mean:using summarize() mean() bootstrap sample boot_sample, determine mean weight 3.702 grams. Recall sample mean original sample found previous subsection 3.682. , sample mean bootstrap sample different sample mean original sample. variation induced resampling replacement, method finding bootstrap sample. can also compare histogram weights bootstrap sample histogram weights original sample.\nFIGURE 8.12: Comparing weight resampled boot_sample original sample almonds_sample_100.\nObserve Figure 8.12 general shapes distributions weights roughly similar, identical.\ntypical behavior bootstrap samples. samples determined original sample, replacement used new observation attained, values often appear others often appear .","code":"\nalmonds_sample_100 <- almonds_sample_100 |> \n  ungroup() |> \n  select(-replicate)\nalmonds_sample_100# A tibble: 100 × 2\n      ID weight\n   <int>  <dbl>\n 1   166    4.2\n 2  1215    4.2\n 3  1899    3.9\n 4  1912    3.8\n 5  4637    3.3\n 6   511    3.5\n 7   127    4  \n 8  4419    3.5\n 9  4729    4.2\n10  2574    4.1\n# ℹ 90 more rows\nboot_sample <- almonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 1)\nboot_sample# A tibble: 100 × 3\n# Groups:   replicate [1]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1  2105    3.1\n 2         1  4529    3.8\n 3         1  1146    4.2\n 4         1  2993    3.2\n 5         1  1535    3.2\n 6         1  2294    3.7\n 7         1   438    3.8\n 8         1  4419    3.5\n 9         1  1674    3.5\n10         1  1146    4.2\n# ℹ 90 more rows\nboot_sample |> \n  summarize(mean_weight = mean(weight))# A tibble: 1 × 2\n  replicate mean_weight\n      <int>       <dbl>\n1         1       3.702\nggplot(boot_sample, aes(x = weight)) +\n  geom_histogram(binwidth = 0.1, color = \"white\") +\n  labs(title = \"Resample of 100 weights\")\nggplot(almonds_sample_100, aes(x = weight)) +\n  geom_histogram(binwidth = 0.1, color = \"white\") +\n  labs(title = \"Original sample of 100 weights\")"},{"path":"confidence-intervals.html","id":"replicates","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Many bootstrap samples: resampling multiple times","text":"subsection, take full advantage resampling replacement taking many bootstrap samples study relevant information, variability sample means. can start using R syntax used , time 35 replications.syntax , time set reps = 35 get 35 bootstrap samples.\nresulting data frame, bootstrap_samples, 35 \\(\\cdot\\) 100 = 3500 rows corresponding 35 resamples 100 almonds’ weights. Let’s now compute resulting 35 sample means using dplyr code previous section:Observe boot_means 35 rows, corresponding 35 bootstrap sample means. Furthermore, observe values mean_weight vary shown Figure 8.13.\nFIGURE 8.13: Distribution 35 sample means 35 bootstrap samples.\nhistogram highlights variation sample mean weights. Since used 35 bootstrap samples, histogram looks little coarse.\nimprove perception variation, find 1000 bootstrap samples sample means:can combine two operations single chain pipe (|>) operators:data frame boot_means contains 1000 sample mean weights. calculated different bootstrap sample visualized Figure 8.14.\nFIGURE 8.14: Histogram 1000 bootstrap sample mean weights almonds.\nhistogram graphical approximation bootstrap distribution sample mean. distribution constructed getting sample means every bootstrap sample constructed based original sample. Since total number possible bootstraps really large, used , 1000 already provides good visual approximation.Observe also bootstrap distribution can approximate sampling distribution sample mean, concept studied Chapter 7 took multiple samples population. key difference resample single sample, original sample, entire population.inspecting histogram Figure 8.14, bell-shape apparent. can also approximate center spread distribution computing mean standard deviation 1000 bootstrap sample means:Everything learned Chapter 7 studying sampling distribution sample mean applies . example, observe mean bootstrap sample means near 3.68 grams, close mean original sample: 3.682 grams. intention study distribution bootstrap samples, rather use estimate population values, population mean. next section, discuss can use bootstrap samples construct confidence intervals.\nLearning check\n(LC8.10) chief difference bootstrap distribution sampling distribution?(LC8.11) Looking bootstrap distribution sample mean Figure 8.14, two values say values lie?(LC8.12) following true confidence level constructing confidence interval?. confidence level determines width interval affects likely contain population parameter.B. confidence level always fixed 95% statistical analyses involving confidence intervals.C. higher confidence level always results narrower confidence interval, making useful practical purposes.D. confidence level relevant population standard deviation known.(LC8.13) increasing sample size affect width confidence interval given confidence level?. increases width confidence interval, making less precise.B. effect width confidence interval since confidence level fixed.C. decreases width confidence interval, making precise reducing margin error.D. changes confidence level directly, regardless factors.","code":"\nbootstrap_samples_35 <- almonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 35)\nbootstrap_samples_35# A tibble: 3,500 × 3\n# Groups:   replicate [35]\n   replicate    ID weight\n       <int> <int>  <dbl>\n 1         1  1459    3.6\n 2         1  2972    3.4\n 3         1  1215    4.2\n 4         1  1381    3.4\n 5         1  1264    3.5\n 6         1   199    3.4\n 7         1   476    3.8\n 8         1  4806    3.7\n 9         1  3169    4.1\n10         1  2265    3.4\n# ℹ 3,490 more rows\nboot_means <- bootstrap_samples_35 |> \n  summarize(mean_weight = mean(weight))\nboot_means# A tibble: 35 × 2\n   replicate mean_weight\n       <int>       <dbl>\n 1         1       3.68 \n 2         2       3.688\n 3         3       3.632\n 4         4       3.68 \n 5         5       3.679\n 6         6       3.675\n 7         7       3.678\n 8         8       3.706\n 9         9       3.643\n10        10       3.68 \n# ℹ 25 more rows\nggplot(boot_means, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.01, color = \"white\") +\n  labs(x = \"sample mean weight in grams\")\n# Retrieve 1000 bootstrap samples\nbootstrap_samples <- almonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 1000)\n\n# Compute sample means from the bootstrap samples\nboot_means <- bootstrap_samples |> \n  summarize(mean_weight = mean(weight))\nboot_means <- almonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 1000) |> \n  summarize(mean_weight = mean(weight))\nboot_means# A tibble: 1,000 × 2\n   replicate mean_weight\n       <int>       <dbl>\n 1         1       3.68 \n 2         2       3.688\n 3         3       3.632\n 4         4       3.68 \n 5         5       3.679\n 6         6       3.675\n 7         7       3.678\n 8         8       3.706\n 9         9       3.643\n10        10       3.68 \n# ℹ 990 more rows\nggplot(boot_means, aes(x = mean_weight)) +\n  geom_histogram(binwidth = 0.01, color = \"white\") +\n  labs(x = \"sample mean weight in grams\")\nboot_means |> \n  summarize(mean_of_means = mean(mean_weight),\n            sd_of_means = sd(mean_weight))# A tibble: 1 × 2\n  mean_of_means sd_of_means\n          <dbl>       <dbl>\n1       3.67998   0.0356615"},{"path":"confidence-intervals.html","id":"bootstrap-process","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.2.2 Confidence intervals and the bootstrap: original workflow","text":"process determining bootstrap samples using estimation called bootstrapping.\ncan estimate population parameters mean, median, standard deviation. can also construct confidence intervals.subsection, focus latter construct confidence intervals based bootstrap samples. , review R syntax workflow already used previous sections also introduce new package: infer package tidy transparent statistical inference.","code":""},{"path":"confidence-intervals.html","id":"original-workflow","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Original workflow","text":"Recall took bootstrap samples, calculated sample means samples. Let’s revisit original workflow using dplyr verbs |> operator.First, use rep_sample_n() resample original sample almonds_sample_100 5000 almonds. set size = 100 generate bootstrap samples size original sample, resample replacement setting replace = TRUE. create 1000 bootstrap samples setting reps = 1000:Second, add another pipe followed summarize() compute sample mean() weight replicate:simple case, needed use rep_sample_n() function dplyr verb. However, using dplyr verbs provides us limited set tools ideal working complicated situations. reason introduce infer package.","code":"\nalmonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 1000)\nalmonds_sample_100 |> \n  rep_sample_n(size = 100, replace = TRUE, reps = 1000) |> \n  summarize(mean_weight = mean(weight))"},{"path":"confidence-intervals.html","id":"infer-workflow","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.2.3 The infer package workflow:","text":"infer package R package statistical inference. makes efficient use |> pipe operator introduced Section 3.1 spell sequence steps necessary perform statistical inference “tidy” transparent fashion. Just dplyr package provides functions verb-like names perform data wrangling, infer package provides functions intuitive verb-like names perform statistical inference, constructing confidence intervals performing hypothesis testing. discussed theory-based implementation former section 8.1.3 introduce latter Chapter 9.Using example almonds’ weights, introduce infer first comparing implementation dplyr. Recall calculate sample statistic point estimate sample, sample mean, using dplyr use summarize() mean():want use infer instead, use functions specify() calculate() shown:new structure using infer seems slightly complicated one using dplyr simple calculation. functions provide three chief benefits moving forward.First, infer verb names better align overall simulation-based framework need understand construct confidence intervals conduct hypothesis tests (Chapter 9). see flowchart diagrams framework upcoming Figure 8.20 Chapter 9 Figure 9.11.First, infer verb names better align overall simulation-based framework need understand construct confidence intervals conduct hypothesis tests (Chapter 9). see flowchart diagrams framework upcoming Figure 8.20 Chapter 9 Figure 9.11.Second, can transition seamlessly confidence intervals hypothesis testing minimal changes code. becomes apparent Subsection 9.4.2 compare infer code inferential methods.Second, can transition seamlessly confidence intervals hypothesis testing minimal changes code. becomes apparent Subsection 9.4.2 compare infer code inferential methods.Third, infer workflow much simpler conducting inference one variable. introduce two-sample inference sample data collected two groups, Section 8.4 study contagiousness yawning Section 9.2 compare popularity music genres. Section 10.3, see situations inference regression using regression models fit Chapter 5.Third, infer workflow much simpler conducting inference one variable. introduce two-sample inference sample data collected two groups, Section 8.4 study contagiousness yawning Section 9.2 compare popularity music genres. Section 10.3, see situations inference regression using regression models fit Chapter 5.now illustrate sequence verbs necessary construct confidence interval \\(\\mu\\), population mean weight almonds.","code":"\nalmonds_sample_100 |> \n  summarize(stat = mean(weight))\nalmonds_sample_100 |> \n  specify(response = weight) |> \n  calculate(stat = \"mean\")"},{"path":"confidence-intervals.html","id":"specify-variables","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"1. specify variables","text":"\nFIGURE 8.15: Diagram specify() verb.\nshown Figure 8.15, specify() function used choose variables data frame focus statistical inference. specifying response argument. example, almonds_sample_100 data frame 100 almonds sampled bowl, variable interest weight:Notice data change, Response: weight (numeric) meta-data . similar group_by() verb dplyr doesn’t change data, adds “grouping” meta-data, saw Section 3.4.can also specify variables focus study introducing formula = y ~ x specify(). formula notation saw Chapters 5 6 regression models: response variable y separated explanatory variable x ~ (“tilde”). following use specify() formula argument yields result seen previously:case almonds response variable explanatory variable interest. Thus, set x right-hand side ~ NULL.cases inference focused single sample, almonds’ weights example, either specification works. examples present later sections, formula specification simpler flexible. particular, comes upcoming Section 8.4 comparing two proportions Section 10.3 inference regression.","code":"\nalmonds_sample_100 |> \n  specify(response = weight)Response: weight (numeric)\n# A tibble: 100 × 1\n   weight\n    <dbl>\n 1    4.2\n 2    4.2\n 3    3.9\n 4    3.8\n 5    3.3\n 6    3.5\n 7    4  \n 8    3.5\n 9    4.2\n10    4.1\n# ℹ 90 more rows\nalmonds_sample_100 |> \n  specify(formula = weight ~ NULL)"},{"path":"confidence-intervals.html","id":"generate-replicates","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"2. generate replicates","text":"\nFIGURE 8.16: Diagram generate() replicates.\nspecify() variables interest, pipe results generate() function generate replicates. function produces bootstrap samples performs similar resampling process large number times, based variable(s) specified previously, shown Figure 8.16. Recall 1000 times.generate() function’s first argument reps, sets number replicates like generate. Since want resample 100 almonds almonds_sample_100 replacement 1000 times, set reps = 1000.second argument type determines type computer simulation used. Setting type = \"bootstrap\" produces bootstrap samples using resampling replacement. present different options type Chapter 9.Observe resulting data frame 100,000 rows. found 1000 bootstrap samples, 100 rows.variable replicate indicates bootstrap sample row belongs , 1 1000, replicate repeated 100 times. default value type argument \"bootstrap\" scenario, inclusion made completeness. last line written simply generate(reps = 1000), result .Comparing original workflow: Note steps infer workflow far produce results original workflow using rep_sample_n() function saw earlier. words, following two code chunks produce similar results:","code":"\nalmonds_sample_100 |> \n  specify(response = weight) |> \n  generate(reps = 1000, type = \"bootstrap\")Response: weight (numeric)\n# A tibble: 100,000 × 2\n# Groups:   replicate [1,000]\n   replicate weight\n       <int>  <dbl>\n 1         1    3.6\n 2         1    3.4\n 3         1    4.2\n 4         1    3.4\n 5         1    3.5\n 6         1    3.4\n 7         1    3.8\n 8         1    3.7\n 9         1    4.1\n10         1    3.4\n# ℹ 99,990 more rows# infer workflow:                   # Original workflow:\nalmonds_sample_100 |>               almonds_sample_100 |> \n  specify(response = weight) |>        rep_sample_n(size = 100, replace = TRUE, \n  generate(reps = 1000)                             reps = 1000)              "},{"path":"confidence-intervals.html","id":"calculate-summary-statistics","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"3. calculate summary statistics","text":"\nFIGURE 8.17: Diagram calculate() summary statistics.\ngenerate() 1000 bootstrap samples, want summarize , example, calculating sample mean one . Figure 8.17 shows, calculate() function .example, calculate mean weight bootstrap sample setting stat argument equal \"mean\" inside calculate() function. stat argument can used common summary statistics \"median\", \"sum\", \"sd\" (standard deviation), \"prop\" (proportion). see list possible summary statistics can use, type ?calculate read help file.Let’s save result data frame called bootstrap_means explore contents:Observe resulting data frame 1000 rows 2 columns corresponding 1000 replicate values. also mean weight bootstrap sample saved variable stat.Comparing original workflow: may recognized point calculate() step infer workflow produces output summarize() step original workflow.","code":"\nbootstrap_means <- almonds_sample_100 |> \n  specify(response = weight) |> \n  generate(reps = 1000) |> \n  calculate(stat = \"mean\")Setting `type = \"bootstrap\"` in `generate()`.\nbootstrap_meansResponse: weight (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 3.68 \n 2         2 3.688\n 3         3 3.632\n 4         4 3.68 \n 5         5 3.679\n 6         6 3.675\n 7         7 3.678\n 8         8 3.706\n 9         9 3.643\n10        10 3.68 \n# ℹ 990 more rows# infer workflow:                   # Original workflow:\nalmonds_sample_100 |>                  almonds_sample_100 |> \n  specify(response = weight) |>        rep_sample_n(size = 100, replace = TRUE, \n  generate(reps = 1000) |>                          reps = 1000) |>              \n  calculate(stat = \"mean\")             summarize(stat = mean(weight))"},{"path":"confidence-intervals.html","id":"visualize-the-results","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"4. visualize the results","text":"\nFIGURE 8.18: Diagram visualize() results.\nvisualize() verb provides quick way visualize bootstrap distribution histogram numerical stat variable’s values shown Figure 8.19. pipeline main infer verbs used exploring bootstrap distribution results shown Figure 8.18.\nFIGURE 8.19: Bootstrap distribution.\nComparing original workflow: fact, visualize() wrapper function ggplot() function uses geom_histogram() layer. Recall illustrated concept wrapper function Figure 5.5 Subsection 5.1.2.visualize() function can take many arguments customize plot . later sections, take advantage flexibility. addition, works helper functions add shading histogram values corresponding confidence interval values.\nintroduced different elements infer workflow constructing bootstrap distribution visualizing . summary steps presented Figure 8.20.\nFIGURE 8.20: infer package workflow confidence intervals.\n","code":"\nvisualize(bootstrap_means)# infer workflow:                    # Original workflow:\nvisualize(bootstrap_means)           ggplot(bootstrap_means, aes(x = stat)) +\n                                        geom_histogram()"},{"path":"confidence-intervals.html","id":"conf-int-infer","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.2.4 Confidence intervals using bootstrap samples with infer","text":"ready introduce confidence intervals using bootstrap via infer. present two different methods constructing 95% confidence intervals interval estimates unknown population parameter: percentile method standard error method{Bootstrap!standard error method}. Let’s now check infer package code explicitly constructs . also additional neat functions visualize resulting confidence intervals built-infer package.","code":""},{"path":"confidence-intervals.html","id":"percentile-method-infer","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Percentile method","text":"Recall Subsection 8.2.3 generated 1000 bootstrap samples stored data frame bootstrap_means:sample means stored bootstrap_means represent good approximation bootstrap distribution possible bootstrap samples. percentile method constructing 95% confidence intervals sets lower endpoint confidence interval 2.5th percentile bootstrap_means similarly sets upper endpoint 97.5th percentile. resulting interval captures middle 95% values sample mean weights almonds bootstrap_means. interval estimate population mean weight almonds entire bowl.can compute 95% confidence interval piping bootstrap_means get_confidence_interval() function infer package, confidence level set 0.95 confidence interval type \"percentile\". save results percentile_ci.Alternatively, can visualize interval (3.61, 3.76) piping bootstrap_means data frame visualize() function adding shade_confidence_interval() layer. set endpoints argument percentile_ci.\nFIGURE 8.21: Percentile method 95% confidence interval shaded corresponding potential values.\nObserve Figure 8.21 95% sample means stored stat variable bootstrap_means fall two endpoints marked darker lines, 2.5% sample means left shaded area 2.5% sample means right. also option change colors shading using color fill arguments.infer package incorporated shorter named function shade_ci() produces results. Try following code:","code":"\nbootstrap_meansResponse: weight (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 3.68 \n 2         2 3.688\n 3         3 3.632\n 4         4 3.68 \n 5         5 3.679\n 6         6 3.675\n 7         7 3.678\n 8         8 3.706\n 9         9 3.643\n10        10 3.68 \n# ℹ 990 more rows\npercentile_ci <- bootstrap_means |> \n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1  3.61198    3.756\nvisualize(bootstrap_means) + \n  shade_confidence_interval(endpoints = percentile_ci)\nvisualize(bootstrap_means) + \n  shade_ci(endpoints = percentile_ci, color = \"hotpink\", fill = \"khaki\")"},{"path":"confidence-intervals.html","id":"se-infer","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"Standard error method","text":"Subsection 8.1.3 introduced theory-based confidence intervals. show 95% confidence interval can constructed \\[\\left(\\overline{x} - 1.96 \\cdot SE(\\bar x), \\quad \\overline{x} + 1.96 \\cdot SE(\\bar x)\\right)\\]\\(\\overline{x}\\) sample mean original sample, 1.96 number standard errors around mean needed account 95% area density curve (distribution normal), \\(SE(\\bar x)\\) standard error sample mean can computed \\(\\sigma /\\sqrt{n}\\) population standard deviation known, estimated \\(s/\\sqrt{n}\\) use sample standard deviation, \\(s\\), sample size, \\(n\\).use structure construct confidence intervals using bootstrap sample means estimate standard error \\(\\overline{x}\\).\nThus, 95% confidence interval population mean, \\(\\mu\\), using standard error estimated via bootstrapping, \\(SE_\\text{boot}\\), :\\[\\left(\\overline{x} - 1.96 \\cdot SE_{\\text{boot}}, \\quad \\overline{x} + 1.96 \\cdot SE_{\\text{boot}}\\right)\\]can compute confidence interval using dplyr. First, calculate estimated standard error:use original sample mean calculate 95% confidence interval:Alternatively, computation 95% confidence interval can done via infer. find sample mean original sample store variable x_barNow, pipe bootstrap_means data frame created get_confidence_interval() function. set type argument \"se\" specify point_estimate argument x_bar order set center confidence interval sample mean original sample.results whether dplyr infer used, explained earlier, latter provides flexibility tests.like visualize interval (3.61, 3.75), can pipe bootstrap_means data frame visualize() function add shade_confidence_interval() layer plot. set endpoints argument standard_error_ci. resulting standard-error method based 95% confidence interval \\(\\mu\\) can seen Figure 8.22.\nFIGURE 8.22: Standard-error method 95% confidence interval.\nusing bootstrap samples construct intervals, call percentile standard error methods simulation-based methods. can compare 95% confidence intervals using simulation-based methods well one attained using theory-based method described 8.1.3:Percentile method: (3.61, 3.76)Standard error method: (3.61, 3.75)Theory-based method: (3.61, 3.76)\nLearning check\n(LC8.14) Construct 95% confidence interval median weight almonds percentile method. appropriate also use standard-error method?(LC8.15) advantages using infer building confidence intervals?(LC8.16) main purpose bootstrapping statistical inference?. visualize data distributions identify outliers.B. generate multiple samples original data estimating parameters.C. replace missing data points mean dataset.D. validate assumptions regression model.(LC8.17) “function denotes variables interest inference?”. rep_sample_n()B. calculate()C. specify()D. visualize()(LC8.18) key difference percentile method standard error method constructing confidence intervals using bootstrap samples?. percentile method requires population standard deviation.B. percentile method uses middle 95% bootstrap statistics, standard error method relies estimated standard error.C. standard error method always results narrower confidence interval.D. percentile method requires bootstrap samples.","code":"\nSE_boot <- bootstrap_means |>\n  summarize(SE = sd(stat)) |>\n  pull(SE)\nSE_boot[1] 0.0357\nalmonds_sample_100 |>\n  summarize(lower_bound = mean(weight) - 1.96 * SE_boot,\n            upper_bound = mean(weight) + 1.96 * SE_boot)# A tibble: 1 × 2\n  lower_bound upper_bound\n        <dbl>       <dbl>\n1     3.61210     3.75190\nx_bar <- almonds_sample_100 |> \n  specify(response = weight) |> \n  calculate(stat = \"mean\")\nx_barResponse: weight (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 3.682\nstandard_error_ci <- bootstrap_means |> \n  get_confidence_interval(type = \"se\", point_estimate = x_bar, level = 0.95)\nstandard_error_ci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1  3.61210  3.75190\nvisualize(bootstrap_means) + \n  shade_confidence_interval(endpoints = standard_error_ci)"},{"path":"confidence-intervals.html","id":"boot-remarks","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.3 Additional remarks about the bootstrap","text":"section expands explanations bootstrap methods, provides \nhistorical context, gives comparison theory-based approach simulation-based approach working confidence intervals, provides\nreasons using bootstrap. presentation \ntheoretical sections chapter, welcome skip \nSection 8.4 want go directly another\napplication bootstrap methods R. Additional theoretical explanations\navailable Appendices online version book.","code":""},{"path":"confidence-intervals.html","id":"the-bootstrap-and-other-resampling-methods","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.3.1 The bootstrap and other resampling methods","text":"bootstrap one many resampling methods. Chernick LaBudde noted \nbootstrap’s roots trace back development similar techniques like permutations\njackknife (Chernick LaBudde 2011).\nbootstrap initially conceived approximation another\nresampling method called jackknife quickly gained\nrecognition broader applicability efficiency.\nSince , shown multiple contexts bootstrap performs\nleast well traditional methods estimating standard errors,\nconstructing confidence intervals, performing hypothesis testing, many \nstatistical techniques.Furthermore, since 1980s (even last two decades), use \nsimulations compare advanced\nbootstrap methods techniques, cross-validation, \nestablished superiority specific contexts, particularly dealing \nsmall sample sizes.\nuse bootstrap bootstrap-related methods become cornerstone\nmodern statistical data science practices.section, introduce additional details bootstrap, \nexplain advantages limitations using bootstrap.","code":""},{"path":"confidence-intervals.html","id":"rate-convergence","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.3.2 Confidence intervals and rate of convergence","text":"want compare bootstrap performs building confidence intervals\nrespect theory-based approach discussed Section\n8.1.\nformal comparison requires mathematical concepts beyond scope \nbook, pursued . Instead, provide just enough elements \nhelp intuition \ncomparison made bootstrap-related methods can strong even\nstronger theory-based alternative methods.Let’s start illustration using theory-based confidence interval.\n95% confidence interval \\(\\mu\\), sample size \\(n = 100\\) used,\ngiven \\[\\overline{x} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\\]\\(\\sigma\\) unknown \\[\\overline{x} \\pm 1.98 \\frac{s}{\\sqrt{n}}\\]\\(\\sigma\\) unknown. use 95% \\(n = 100\\) illustration, \nexposition also true confidence level \nsample sizes.\nunderstand 95% possible samples lead interval \ncontains \\(\\mu\\).\nstatement exactly true distribution \\(\\overline X\\) precisely normal,\nsay 95% true coverage probability.\nreality, know distribution population,\nCentral Limit Theorem know large sample size\n\\(n\\) distribution \\(\\overline X\\) approximately normal.\ncase, 95% approximate coverage probability.\nmeans take every possible sample size \\(n\\) \nconstruct confidence interval using formulas , exactly 95% \nintervals include \\(\\mu\\).\n, happens \\(\\overline X\\) exactly, approximately,\nnormal.\nStill, Central Limit Theorem states \\(n\\) tends infinity,\ndistribution \\(\\overline X\\) tends normal, \nlarger sample size \\(n\\) closer distribution \\(\\overline X\\) \nnormal distribution, smaller difference true \napproximate coverage probability.Given can never make \\(n\\) infinity real-life applications, \nlike produce 95% confidence intervals make difference\napproximate coverage probability \ntrue coverage probability small possible increase sample\nsize sample.\nImagine sequence sample sizes \\(n_1, n_2, n_3,\\dots\\) gets bigger\nbigger bigger. rate corresponding consecutive differences\nconfidence intervals’ coverage probability decreases called \nrate convergence difference \napproximate true coverage probabilities.case 95% confidence interval \\(\\mu\\) using theory-based\napproach, rate convergence difference \\(1/\\sqrt{n}\\).\nmeans increase \\(n\\) 100 400, difference \napproximate true coverage goes factor \\(1/\\sqrt{100} = 0.1\\)\nfactor \\(1/\\sqrt{400} = 0.05\\). Thus, increasing \\(n\\) four times leads \ndecrease difference two times. statistical literature, method rate\nconvergence called first-order correct (Hall 1992)\nfirst-order accurate (Chernick LaBudde 2011).bootstrap percentile method discussed 8.2.4, case\n95% confidence interval \\(\\mu\\), also first-order accurate.\nThus, confidence intervals calculated using theory-based bootstrap percentile method comparable.\nconsistent results obtained earlier chapter.general, two different methods produce similar 95% confidence intervals \nneed choose one , choose one faster rate \nconvergence. discuss next subsection, bootstrap methods ,\ncertain contexts, faster rates convergence.","code":""},{"path":"confidence-intervals.html","id":"why-bootstrap-methods","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.3.3 Why bootstrap methods","text":"suitable learn use bootstrap\nmethods confidence intervals? important reason bootstrap\nmethods, particular advanced bootstrap methods, can deal many\nlimitations theory-based approach. Let’s discuss three limitations.First, 95% confidence interval \\(\\mu\\) appropriate population\ndistribution extreme sample size large enough \ndistribution \\(\\overline X\\) approximately normal.\nsituations conditions satisfied, example,\npopulation distribution heavily skewed right, case \nincome wealth; distribution constructed two values\n(1 0) chances getting zero \nmuch greater ones getting one (example, chance getting zero\n0.999 chance getting 1 0.001) case lottery outcomes\npresence disease population.\nsituations present, confidence intervals inaccurate\nsample mean \\(\\overline X\\) biased \\(n\\) large.\nmeans take large number random samples \nconstruct sample mean samples, average \nclearly different \\(\\mu\\), breaking theory developed \nSubsection 8.1. problem may fixed \nsample size large, , depending extreme population distribution\n, sample size may need extremely large; perhaps \norder thousands, tens thousands, even . Getting samples \nsizes may doable real-life situations.Second, chapter, study confidence intervals population\nmean \\(\\mu\\), fundamental quantity foundation \ncases. However, building confidence intervals parameters using\ntheory-based approach (example, median, first quartile,\nstandard deviation, etc.) becomes complicated even unfeasible.Third, working estimators complicated \n\\(\\overline X\\), often possible derive standard\nerror estimator formula clean \\(\\sigma/\\sqrt{n}\\). Sometimes, \nformula standard error alternative methods used \nestimate . can create additional source bias. bias present,\nconfidence intervals created using theory-based approach Subsection\n8.1.3 suspect, even completely useless.bootstrap percentile method affected directly second third\nlimitations. can implemented parameter beyond population\nmean, long information needed can extracted bootstrap\nsample. hand, first limitation listed can also affect \naccuracy method.Fortunately, since inception bootstrap, many\nimprovements made percentile method. Bootstrap methods \nproposed address presence bias either limitations discussed \nbias created obtaining estimators incorporating methods.\naddition, certain contexts, methods also improve rate convergence\ndifference approximate true coverage probability.\nmethods percentile-\\(t\\) Bias Correction \nAcceleration bootstrap method (BCa). terms rates convergence, methods \nsecond-order accurate; , rate\nconvergence \\(1/n\\). Another method called double bootstrap (generally,\niterated bootstrap) can even third-order accurate.included methods directly theory \njustifies goes beyond scope book , dealing \nconfidence intervals \\(\\mu\\) populations distributions \nextreme, real gains using theory-based approach\npercentile method. (encourage check one implementation \nbias-corrected confidence intervals infer package setting\ntype = \"bias-corrected\" get_confidence_interval()\nfunction.)summarize, working skewed\ndistributions, small sample sizes, estimators parameters \\(\\mu\\)\n(median), estimation standard error \nformulas obtain , many advanced bootstrap methods preferred \ntheory-based approach. Appendices online version book,\nplan explore advanced methods present simulations\nshow methods preferred percentile method \ntheory-based approach.","code":""},{"path":"confidence-intervals.html","id":"case-study-two-prop-ci","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.4 Case study: is yawning contagious?","text":"Let’s apply knowledge confidence intervals answer question: “yawning contagious?”. see someone else yawn, likely yawn? episode US show Mythbusters aired Discovery, hosts conducted experiment answer question. information episode available IMDb.","code":""},{"path":"confidence-intervals.html","id":"mythbusters-study-data","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.4.1 Mythbusters study data","text":"Fifty adult participants thought considered appearance show interviewed show recruiter. interview, recruiter either yawned . Participants sat large van asked wait. van, Mythbusters team watched participants using hidden camera see yawned. data frame containing results experiment available mythbusters_yawn included moderndive package: variables :subj: participant ID values 1 50.group: binary treatment variable indicating whether participant exposed yawning. \"seed\" indicates participant exposed yawning \"control\" indicates participant .yawn: binary response variable indicating whether participant ultimately yawned.Recall learned treatment response variables Subsection 5.3.1 discussion confounding variables. Let’s use data wrangling calculate counts four possible outcomes:Let’s first focus \"control\" group participants exposed yawning. 12 participants yawn, 4 participants . 16 people exposed yawning, 4/16 = 0.25 = 25% yawn.Let’s now focus \"seed\" group participants exposed yawning 24 participants yawn, 10 participants yawn. 34 people exposed yawning, 10/34 = 0.294 = 29.4% yawn. Comparing two percentages, participants exposed yawning yawned 29.4% - 25% = 4.4% often .","code":"\nmythbusters_yawn# A tibble: 50 × 3\n    subj group   yawn \n   <int> <chr>   <chr>\n 1     1 seed    yes  \n 2     2 control yes  \n 3     3 seed    no   \n 4     4 seed    yes  \n 5     5 seed    no   \n 6     6 control no   \n 7     7 seed    yes  \n 8     8 control no   \n 9     9 control no   \n10    10 seed    no   \n# ℹ 40 more rows\nmythbusters_yawn |> \n  group_by(group, yawn) |> \n  summarize(count = n(), .groups = \"keep\")# A tibble: 4 × 3\n# Groups:   group, yawn [4]\n  group   yawn  count\n  <chr>   <chr> <int>\n1 control no       12\n2 control yes       4\n3 seed    no       24\n4 seed    yes      10"},{"path":"confidence-intervals.html","id":"sampling-scenario","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.4.2 Sampling scenario","text":"Let’s review terminology notation related sampling studied Subsection 7.2.1. Chapter 7, study population bowl \\(N\\) = 2400 balls. population parameter interest population proportion balls red, denoted mathematically \\(p\\). order estimate \\(p\\), extracted sample 50 balls using shovel computed relevant point estimate: sample proportion red, denoted mathematically \\(\\widehat{p}\\).study population ? humans? people watch show Mythbusters? ’s hard say! question can answered know show’s hosts recruited participants! words, sampling methodology used Mythbusters recruit participants? alas provided information. purposes case study, however, ’ll assume 50 participants representative sample Americans given popularity show. Thus, ’ll assuming results experiment generalize \\(N\\) = 346 million Americans (2024 population estimate).Just like sampling bowl, population parameter involve proportions. However, case, difference population proportions \\(p_{seed} - p_{control}\\), \\(p_{seed}\\) proportion Americans exposed yawning yawn , \\(p_{control}\\) proportion Americans exposed yawning still yawn . Correspondingly, point estimate/sample statistic based Mythbusters’ sample participants difference sample proportions \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\). Let’s extend Table 8.1 scenarios sampling inference include latest scenario.\nTABLE 8.1: Scenarios sampling inference\nknown two-sample inference situation since two separate samples. Based two-samples size \\(n_{seed}\\) = 34 \\(n_{control}\\) = 16, point estimate \\[\n\\widehat{p}_{seed} - \\widehat{p}_{control} = \\frac{24}{34} - \\frac{12}{16} = 0.04411765 \\approx 4.4\\%\n\\]However, say Mythbusters repeated experiment. words, say recruited 50 new participants exposed 34 yawning 16 . find exact estimated difference 4.4%? Probably , , sampling variation.sampling variation affect estimate 4.4%? words, plausible range values difference accounts sampling variation? can answer question confidence intervals! Furthermore, since Mythbusters single two-sample 50 participants, construct 95% confidence interval \\(p_{seed} - p_{control}\\) using bootstrap resampling replacement.make couple important notes. First, comparison \"seed\" \"control\" groups make sense, however, groups need independent . Otherwise, influence ’s results. means participant selected \"seed\" \"control\" group influence another participant assigned one two groups. example, mother child participants study, wouldn’t necessarily group. assigned randomly one two groups explanatory variable.Second, order subtraction difference doesn’t matter long consistent tailor interpretations accordingly. words, using point estimate \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\) make material difference, just need stay consistent interpret results accordingly.","code":""},{"path":"confidence-intervals.html","id":"ci-build","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.4.3 Constructing the confidence interval","text":"Subsection 8.2.3, let’s first construct bootstrap distribution \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) use construct 95% confidence intervals \\(p_{seed} - p_{control}\\). ’ll using infer workflow . However, since difference proportions new scenario inference, ’ll need use new arguments infer functions along way.","code":""},{"path":"confidence-intervals.html","id":"specify-variables-1","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"1. specify variables","text":"Let’s take mythbusters_yawn data frame specify() variables interest using y ~ x formula interface :response variable yawn: whether participant yawned. levels \"yes\" \"\".explanatory variable group. levels \"seed\" (exposed yawning) \"control\" (exposed yawning).Alas, got error message infer telling us one levels categorical variable yawn needs defined success. Recall define success event interest trying count compute proportions . interested participants \"yes\" yawned \"\" didn’t yawn? isn’t clear R someone just picking code results first time, need set success argument \"yes\" follows improve transparency code:","code":"\nmythbusters_yawn |> \n  specify(formula = yawn ~ group)Error: A level of the response variable `yawn` needs to be specified for the \n`success` argument in `specify()`.\nmythbusters_yawn |> \n  specify(formula = yawn ~ group, success = \"yes\")Response: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 50 × 2\n   yawn  group  \n   <fct> <fct>  \n 1 yes   seed   \n 2 yes   control\n 3 no    seed   \n 4 yes   seed   \n 5 no    seed   \n 6 no    control\n 7 yes   seed   \n 8 no    control\n 9 no    control\n10 no    seed   \n# ℹ 40 more rows"},{"path":"confidence-intervals.html","id":"generate-replicates-1","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"2. generate replicates","text":"next step perform bootstrap resampling replacement like almonds activity Section 8.2.1. saw works single variable computing bootstrap means Section 8.2.2, haven’t yet worked bootstrapping involving multiple variables.infer package, bootstrapping multiple variables means row potentially resampled. Let’s investigate focusing first six rows mythbusters_yawn:bootstrap data, potentially pulling subject’s readings multiple times. Thus, see entries \"seed\" group \"\" yawn together new row bootstrap sample. seen exploring sample_n() function dplyr smaller 6-row data frame comprised head(mythbusters_yawn). sample_n() function can perform bootstrapping procedure similar rep_sample_n() function infer, except repeated, rather performs one sample without replacement.can see bootstrap sample generated first six rows mythbusters_yawn, rows repeated. true perform generate() step infer done follows. Using fact, generate 1000 replicates, , words, bootstrap resample 50 participants replacement 1000 times.Observe resulting data frame 50,000 rows. performed resampling 50 participants replacement 1000 times 50,000 = 1000 \\(\\cdot\\) 50. variable replicate indicates resample row belongs . value 1 50 times, value 2 50 times, way value 1000 50 times.","code":"\nfirst_six_rows <- head(mythbusters_yawn)\nfirst_six_rows# A tibble: 6 × 3\n   subj group   yawn \n  <int> <chr>   <chr>\n1     1 seed    yes  \n2     2 control yes  \n3     3 seed    no   \n4     4 seed    yes  \n5     5 seed    no   \n6     6 control no   \nfirst_six_rows |> \n  sample_n(size = 6, replace = TRUE)# A tibble: 6 × 3\n   subj group   yawn \n  <int> <chr>   <chr>\n1     6 control no   \n2     1 seed    yes  \n3     2 control yes  \n4     6 control no   \n5     4 seed    yes  \n6     4 seed    yes  \nmythbusters_yawn |> \n  specify(formula = yawn ~ group, success = \"yes\") |> \n  generate(reps = 1000, type = \"bootstrap\")Response: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate yawn  group  \n       <int> <fct> <fct>  \n 1         1 yes   seed   \n 2         1 yes   control\n 3         1 no    control\n 4         1 no    control\n 5         1 yes   seed   \n 6         1 yes   seed   \n 7         1 yes   seed   \n 8         1 yes   seed   \n 9         1 no    seed   \n10         1 yes   seed   \n# ℹ 49,990 more rows"},{"path":"confidence-intervals.html","id":"calculate-summary-statistics-1","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"3. calculate summary statistics","text":"generate() many replicates bootstrap resampling replacement, next want summarize bootstrap resamples size 50 single summary statistic, difference proportions. setting stat argument \"diff props\":see another warning . need specify order subtraction. \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\). specify \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) setting order = c(\"seed\", \"control\"). Note ’ve also set order = c(\"control\", \"seed\"). stated earlier, order subtraction matter, long stay consistent throughout analysis tailor interpretations accordingly.Let’s save output data frame bootstrap_distribution_yawning:Observe resulting data frame 1000 rows 2 columns corresponding 1000 replicate ID’s 1000 differences proportions bootstrap resample stat.","code":"\nmythbusters_yawn |> \n  specify(formula = yawn ~ group, success = \"yes\") |> \n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in props\")Warning message:\nThe statistic is based on a difference or ratio; by default, for \ndifference-based statistics, the explanatory variable is subtracted in the \norder \"control\" - \"seed\", or divided in the order \"control\" / \"seed\" for \nratio-based statistics. To specify this order yourself, supply \n`order = c(\"control\", \"seed\")` to the calculate() function. \nbootstrap_distribution_yawning <- mythbusters_yawn |> \n  specify(formula = yawn ~ group, success = \"yes\") |> \n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in props\", order = c(\"seed\", \"control\"))\nbootstrap_distribution_yawning# A tibble: 1,000 × 2\n   replicate        stat\n       <int>       <dbl>\n 1         1  0.0357143 \n 2         2  0.229167  \n 3         3  0.00952381\n 4         4  0.0106952 \n 5         5  0.00483092\n 6         6  0.00793651\n 7         7 -0.0845588 \n 8         8 -0.00466200\n 9         9  0.164686  \n10        10  0.124777  \n# ℹ 990 more rows"},{"path":"confidence-intervals.html","id":"visualize-the-results-1","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"4. visualize the results","text":"Figure 8.23 visualize() resulting bootstrap resampling distribution. Let’s also add vertical line 0 adding geom_vline() layer.\nFIGURE 8.23: Bootstrap distribution.\nFirst, let’s compute 95% confidence interval \\(p_{seed} - p_{control}\\) using percentile method, words, identifying 2.5th 97.5th percentiles include middle 95% values. Recall method require bootstrap distribution normally shaped.Second, since bootstrap distribution roughly bell-shaped, can construct confidence interval using standard error method well. Recall construct confidence interval using standard error method, need specify center interval using point_estimate argument. case, need set difference sample proportions 4.4% Mythbusters observed.can also use infer workflow compute value excluding generate() 1000 bootstrap replicates step. words, generate replicates, rather use original sample data. can achieve commenting generate() line, telling R ignore :thus plug value point_estimate argument.Let’s visualize confidence intervals Figure 8.24, percentile method interval marked black lines standard-error method marked grey lines. Observe similar .\nFIGURE 8.24: Two 95% confidence intervals: percentile method (black) standard error method (grey).\n","code":"\nbootstrap_distribution_yawning |> \n  get_confidence_interval(type = \"percentile\", level = 0.95)# A tibble: 1 × 2\n   lower_ci upper_ci\n      <dbl>    <dbl>\n1 -0.238276 0.302464\nobs_diff_in_props <- mythbusters_yawn |> \n  specify(formula = yawn ~ group, success = \"yes\") |> \n  # generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in props\", order = c(\"seed\", \"control\"))\nobs_diff_in_propsResponse: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n       stat\n      <dbl>\n1 0.0441176\nmyth_ci_se <- bootstrap_distribution_yawning |> \n  get_confidence_interval(type = \"se\", point_estimate = obs_diff_in_props,\n                          level = 0.95)\nmyth_ci_se# A tibble: 1 × 2\n   lower_ci upper_ci\n      <dbl>    <dbl>\n1 -0.227291 0.315526"},{"path":"confidence-intervals.html","id":"interpreting-the-confidence-interval","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.4.4 Interpreting the confidence interval","text":"Given confidence intervals quite similar, let’s focus interpretation percentile method confidence interval (-0.238, 0.302). precise statistical interpretation 95% confidence interval : construction procedure repeated 100 times, expect 95 confidence intervals capture true value \\(p_{seed} - p_{control}\\). words, gathered 100 samples \\(n\\) = 50 participants similar pool people constructed 100 confidence intervals based 100 samples, 95 contain true value \\(p_{seed} - p_{control}\\) five won’t. Given little long winded, use shorthand interpretation: ’re 95% “confident” true difference proportions \\(p_{seed} - p_{control}\\) (-0.238, 0.302).one value particular interest 95% confidence interval contains: zero. \\(p_{seed} - p_{control}\\) equal 0, difference proportion yawning two groups. suggest associated effect exposed yawning recruiter whether yawn .case, since 95% confidence interval includes 0, conclusively say either proportion larger. 1000 bootstrap resamples replacement, sometimes \\(\\widehat{p}_{seed}\\) higher thus exposed yawning yawned often. times, reverse happened.Say, hand, 95% confidence interval entirely zero. suggest \\(p_{seed} - p_{control} > 0\\), , words \\(p_{seed} > p_{control}\\), thus ’d evidence suggesting exposed yawning yawn often.","code":""},{"path":"confidence-intervals.html","id":"summary-CI","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.5 Summary and final remarks","text":"","code":""},{"path":"confidence-intervals.html","id":"additional-resources-6","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.5.1 Additional resources","text":"want examples infer workflow construct confidence intervals, suggest check infer package homepage, particular, series example analyses available https://infer.netlify.app/articles/.","code":""},{"path":"confidence-intervals.html","id":"whats-to-come-7","chapter":"8 Estimation, Confidence Intervals, and Bootstrapping","heading":"8.5.2 What’s to come?","text":"Now ’ve equipped confidence intervals, Chapter 9 ’ll cover common tool statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests used infer population using sample. However, ’ll see framework making inferences slightly different.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"9 Hypothesis Testing","heading":"9 Hypothesis Testing","text":"studied confidence intervals Chapter 8. now introduce hypothesis testing, another widely used method statistical inference. claim made value characteristic population random sample used infer plausibility claim hypothesis. example, Section 9.2, use data collected Spotify investigate whether metal music popular deep-house music.Many relevant concepts, ideas, already introduced many necessary concepts understand hypothesis testing Chapters 7 8. can now expand ideas provide general framework understanding hypothesis tests. understanding general framework, able adapt many different scenarios.can said confidence intervals. one general framework applies confidence intervals, infer package designed around framework. specifics may change slightly different types confidence intervals, general framework stays .believe approach better long-term learning focusing specific details specific confidence intervals. prefer approach also hypothesis tests well, tie ideas traditional theory-based methods well completeness.Section 9.1 review confidence intervals introduce hypothesis tests one-sample problems; particular, mean \\(\\mu\\). use theory-based simulation-based approaches, provide justification consider better idea carefully unpack simulation-based approach hypothesis testing context two-sample problems. also show direct link confidence intervals hypothesis tests. Section 9.2 introduce activity motivates simulation-based approach two-sample problems, data collected Spotify investigate whether metal music popular deep-house music. Sections 9.3, 9.4, 9.5 explain, conduct, interpret hypothesis tests, respectively, using simulation-based approach permutation. introduce case study Section 9.6, Section 9.7 conclude discussion theory-based approach two-sample problems additional remarks.’d like practice curious see framework applies different scenarios, can find fully-worked examples many common hypothesis tests corresponding confidence intervals Appendices online.recommend carefully review examples also cover general frameworks apply traditional theory-based methods like \\(t\\)-test normal-theory confidence intervals. see traditional methods just approximations computer-based methods focusing . However, also require conditions met results valid. Computer-based methods using randomization, simulation, bootstrapping much fewer restrictions. Furthermore, help develop computational thinking, one big reason emphasized throughout book.","code":""},{"path":"hypothesis-testing.html","id":"nhst-packages","chapter":"9 Hypothesis Testing","heading":"Needed packages","text":"needed, read Section 1.3 information install load R packages.Recall loading tidyverse package loads many packages encountered earlier. details refer Section 4.4. packages moderndive infer contain functions data frames used chapter.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)\nlibrary(nycflights23)\nlibrary(ggplot2movies)"},{"path":"hypothesis-testing.html","id":"tying-CI-hypo","chapter":"9 Hypothesis Testing","heading":"9.1 Tying confidence intervals to hypothesis testing","text":"Chapter 8, used random sample construct \ninterval estimate population mean.\nusing theory-based approach, relied Central Limit Theorem\nform intervals using simulation-based approach ,\nexample, using bootstrap percentile method.\nHypothesis testing takes advantages similar tools nature goal\nproblem different. Still, direct link confidence\nintervals hypothesis testing.section, first describe one-sample hypothesis test \npopulation mean. establish connection confidence intervals\nhypothesis test. connection direct using theory-based\napproach requires careful consideration using simulation-based\napproach.proceed describing hypothesis testing case two-sample problems.","code":""},{"path":"hypothesis-testing.html","id":"one-sample-hyp","chapter":"9 Hypothesis Testing","heading":"9.1.1 The one-sample hypothesis test for the population mean","text":"Let’s continue working population mean, \\(\\mu\\).\nChapter 8, used random sample construct \n95% confidence interval \\(\\mu\\).performing hypothesis testing, test claim \\(\\mu\\) collecting \nrandom sample using determine sample obtained consistent\nclaim made. illustrate idea return chocolate-covered\nalmonds activity. Assume almonds’ company stated \nwebsite average weight chocolate-covered almond exactly 3.6\ngrams. sure claim researchers believe \ndifferent 3.6 grams average. test competing claims,\nuse random sample almonds_sample_100 moderndive\npackage, first 10 lines shown:goal hypothesis testing answer question: “Assuming \nclaim true, likely observe sample extreme \nextreme one observed?” answer question :\n“claim true, \nunlikely observe sample one obtained” conclude\nclaim true reject . Otherwise, \nfail reject claim.claim statement called null\nhypothesis, \\(H_0\\). statement \\(\\mu\\), \ninitially assumed true.\ncompeting statement called alternative hypothesis, \\(H_A\\), also \nstatement \\(\\mu\\) contains possible values included \nnull hypothesis. almonds’ activity hypotheses :\\[\\begin{aligned}\n&H_0: &\\mu = 3.6\\\\\n&H_A: &\\mu \\ne 3.6\n\\end{aligned}\\]Evidence null may appear estimate \\(\\mu\\) random\nsample collected, sample mean, much greater much less value\n\\(\\mu\\) null hypothesis.determine claim null hypothesis one\nalternative hypothesis? Always remember null hypothesis privileged status since assume true find evidence . rule favor alternative hypothesis find evidence data reject null hypothesis. context, researcher wants show results conclusions new findings, needs prove null hypothesis\ntrue finding evidence . often say \nresearcher bears burden proof.hypothesis shown represents two-sided test \nevidence null hypothesis come either direction (greater\nless).\nSometimes convenient work \nleft-sided test. example, \nclaim null hypothesis becomes: “average weight least\n3.6 grams” researcher’s goal find evidence claim \nfavor competing claim “weight less 3.6 grams.” \ncompeting hypotheses can now written \\(H_0: \\mu \\ge 3.6\\) versus \\(H_A: \\mu < 3.6\\) \neven \\(H_0: \\mu = 3.6\\) versus \\(H_A: \\mu < 3.6\\).\nNotice can drop inequality part null hypothesis. \nfind simplification convenient focus equal part null\nhypothesis becomes clearer evidence null hypothesis\nmay come values left 3.6, hence left-sided test.Similarly, right-sided test can stated \\(H_0: \\mu = 3.6\\) versus\n\\(H_A: \\mu > 3.6\\). Claims null hypothesis type test stated “average weight 3.6 grams” even “average weight less 3.6 grams.” observe less contain equal part, can null \n\\(H_0: \\mu = 3.6\\)? reason related method used semantics statement.\nLet’s break : , null hypothesis, average weight less 3.6 grams, can find evidence null find sample means much greater 3.6 grams, hence alternative hypothesis \\(H_A: \\mu > 3.6\\).\nNow, find evidence looking , methods use require point reference, “less 3.6” fixed number since 2 less 3.6 3.59.\nhand, can find evidence average greater 3.6, also true average greater 3.5, 2, number less 3.6.\nThus, convention, include equal sign always statement null hypothesis.Let’s return test. work two-sided test follows, \ncomment changes needed process instead working left-\nright-sided alternatives.","code":"\nalmonds_sample_100# A tibble: 100 × 2\n      ID weight\n   <int>  <dbl>\n 1   166    4.2\n 2  1215    4.2\n 3  1899    3.9\n 4  1912    3.8\n 5  4637    3.3\n 6   511    3.5\n 7   127    4  \n 8  4419    3.5\n 9  4729    4.2\n10  2574    4.1\n# ℹ 90 more rows"},{"path":"hypothesis-testing.html","id":"the-theory-based-approach","chapter":"9 Hypothesis Testing","heading":"The theory-based approach","text":"use theory-based approach illustrate hypothesis test \nconducted. first calculate sample mean sample standard deviation \nsample:recall due Central Limit Theorem described Subsection\n7.3, sample mean weight almonds, \\(\\overline X\\),\napproximately normally distributed expected value equal \\(\\mu\\) \nstandard deviation equal \\(\\sigma/\\sqrt{n}\\). Since population standard\ndeviation unknown,\nuse sample standard deviation, \\(s\\), calculate standard error\n\\(s/\\sqrt{n}\\). presented Subsection 8.1.4, \\(t\\)-test\nstatistic\\[t = \\frac{\\overline{x} - \\mu}{s/\\sqrt{n}}\\]\nfollows \\(t\\) distribution \\(n-1\\) degrees freedom.\ncourse, know \\(\\mu\\) . since assume null\nhypothesis true, can use value obtain test statistic shown\ncode next. Table 9.1 presents values.\nTABLE 9.1: Sample mean, standard deviation, size, t-test statistic\nvalue \\(t = 2.26\\) sample mean\nstandardized claim \\(\\mu = 3.6\\) grams corresponds center \n\\(t\\) distribution (\\(t = 0\\)), sample mean observed,\n\\(\\overline x = 0.36\\), corresponds \\(t\\) test\nstatistic (t = 2.26).Assuming null hypothesis true (\\(\\mu = 3.6\\) grams) likely \nobserve sample extreme extreme almonds_sample_100?\ncorrespondingly, likely observe sample mean extreme\nextreme \\(\\overline x = 0.36\\)? even,\nlikely observe test statistic \n\\(t = 2.26\\) units away center \n\\(t\\) distribution?two-sided test, care extreme values \n2.26 away either direction distribution.\nshaded regions tails \\(t\\) distribution Figure\n9.1 represent probability extreme values.\nFIGURE 9.1: tails t curve hypothesis test.\nfunction pt() finds area \\(t\\) curve left given\nvalue. function requires argument q (quantile) example \nvalue \\(t\\) left part plot (-2.26)\nargument df, degrees freedom one-sample test \n\\(n-1\\).\nsample size almonds_sample_100 \\(n = 100\\).\nFinally, since need area tails \\(t\\) distribution \nsymmetric, simply multiply results 2:determined , assuming null hypothesis true, \nprobability getting sample extreme extreme \nalmonds_sample_100 0.026. probability called\n\\(p\\)-value.mean? Well, statistics textbooks state , given\nsignificance level, often set \\(\\alpha = 0.05\\), \\(p\\)-value less \\(\\alpha\\),\nreject null hypothesis. technically incorrect, type statement\nprovide enough insight students fully understand conclusion.Let’s unpack elements provide additional context :key element conclusion determine whether statement \nnull hypothesis can rejected. assuming null hypothesis \ntrue, unlikely (almost impossible) observe random sample \none observed, need reject null hypothesis, \nreject null hypothesis situation. sense, null\nhypothesis privileged status. reason want\nmake mistake reject null hypothesis claim \nactually true. statistics, making mistake called Type Error. , \nreject null hypothesis chances committing Type Error \ntruly small. significance level, denoted Greek letter \\(\\alpha\\)\n(pronounced “alpha”), precisely probability \ncommitting Type Error.chances willing take? , \\(\\alpha = 0.05\\) \ntextbooks use many research communities adopted decades. \nable work number, may need interact\npeople used , want treat one many possible\nnumbers.significance level, \\(\\alpha\\), predefined level accepted\nuncertainty. value defined well \\(p\\)-value \ncalculated, even data collected. Ideally, represent\ntolerance uncertainty. , can determine value \n?provide example. Assume student \ntake test statistics class worth 100 points. studied\n, expect get passing grade (score 80s) better\n.\nday exam instructor gives one additional option. can take\nexam receive grade based performance can play \nfollowing game: instructor rolls six-sided fair die. top face shows\n“one” score zero test, otherwise score 100. 1 6\nchance get zero. play game?play game, let’s change . Now instructor rolls \ndie twice, score 0 test rolls “one.” Otherwise, \nscore 100. 1 36 chance get zero. now play \ngame?instructor rolls die five times score zero test\nroll “one,” score 100 otherwise. 1 \n7776 chance get zero. play game?Converted probabilities, three games shown give \nprobabilities getting zero equal \\(1/6 = 0.167\\), \\(1/36 = 0.028\\), \n\\(1/7776 = 0.0001\\), respectively. Think \\(p\\)-values getting zero\ntest committing Type Error.context hypothesis test, random sample collected extreme, \\(p\\)-value really small, \nreject null hypothesis, always chance null hypothesis true, random sample collected atypical, results led us commit Type Error.\nalways uncertainty using random samples make inferences populations. can decide \nlevel tolerance uncertainty. \\(1/6 = 0.167\\), \\(1/36 = 0.028\\), \\(1/7776 = 0.0001\\), level? precisely significance level \\(\\alpha\\).Returning almond example, set \\(\\alpha = 0.04\\) observed \\(p\\)-value = 0.026, reject null hypothesis conclude \npopulation mean \\(\\mu\\) equal 3.6 grams. null hypothesis rejected say result test statistically significant.Let’s summarize steps hypothesis testing:Based claim planning test, state null alternative hypothesis terms \\(\\mu\\).\nRemember equal sign go null hypothesis needed method.\nstatement null hypothesis assumed true process.\nTypically, researchers want conclude favor alternative hypothesis; , try see data provides evidence null hypothesis.\nRemember equal sign go null hypothesis needed method.statement null hypothesis assumed true process.Typically, researchers want conclude favor alternative hypothesis; , try see data provides evidence null hypothesis.Set significance level \\(\\alpha\\), based tolerance committing Type Error, always working sample.Obtain sample mean, sample standard deviation, \\(t\\)-test statistic, \\(p\\)-value.\nworking two-sided test, almond example , \\(p\\)-value area tails.\nleft-sided test, find area \\(t\\) distribution left observed \\(t\\) test statistic.\nright-sided test, find area \\(t\\) distribution right observed \\(t\\) test statistic.\nworking two-sided test, almond example , \\(p\\)-value area tails.left-sided test, find area \\(t\\) distribution left observed \\(t\\) test statistic.right-sided test, find area \\(t\\) distribution right observed \\(t\\) test statistic.Determine whether result test statistically significant (null rejected) non-significant (null rejected).","code":"\nalmonds_sample_100 |>\n  summarize(sample_mean = mean(weight),\n            sample_sd = sd(weight))# A tibble: 1 × 2\n  sample_mean sample_sd\n        <dbl>     <dbl>\n1       3.682  0.362199\nalmonds_sample_100 |>\n  summarize(x_bar = mean(weight),\n            s = sd(weight),\n            n = length(weight),\n            t = (x_bar - 3.6)/(s/sqrt(n)))\n2 * pt(q = -2.26, df = 100 - 1)"},{"path":"hypothesis-testing.html","id":"the-simulation-based-approach","chapter":"9 Hypothesis Testing","heading":"The simulation-based approach","text":"using simulation-based approach bootstrap percentile method,\nrepeat first two steps theory-based approach:State null alternative hypothesis terms \\(\\mu\\). statement null hypothesis assumed true process.Set significance level, \\(\\alpha\\), based tolerance committing Type Error.step 1 need assume null hypothesis true. presents technical complication bootstrap percentile method sample collected corresponding bootstrap samples based real distribution null hypothesis true reflect . solution shift sample values constant make sample mean equal claimed population mean null hypothesis.infer workflow takes account automatically, introduced students first time, additional shifting tends create confusion intuition method. determined easier introduce elements simulation-based approach hypothesis testing via two-sample problem context using another resampling technique called permutation. Details method presented Sections 9.3, 9.4, \n9.5. comfortable using method, can explore bootstrap percentile method one-sample problems. Observe examples explanations simulation-based approach presented Appendices online including example one-sample mean hypothesis test using simulation-based methods.completeness, present code results one-sample hypothesis test almonds’ problem.\\(p\\)-value 0.032. fairly similar \\(p\\)-value obtained using theory-based approach. Using significance level \\(\\alpha = 0.04\\) reject null hypothesis.","code":"\nnull_dist <- almonds_sample_100 |>\n  specify(response = weight) |>\n  hypothesize(null = \"point\", mu = 3.6) |>\n  generate(reps = 1000, type = \"bootstrap\") |>\n  calculate(stat = \"mean\")\nx_bar_almonds <- almonds_sample_100 |>\n  summarize(sample_mean = mean(weight)) |>\n  select(sample_mean)\nnull_dist |>\n  get_p_value(obs_stat = x_bar_almonds, direction = \"two-sided\")# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.032"},{"path":"hypothesis-testing.html","id":"hypothesis-tests-and-confidence-intervals","chapter":"9 Hypothesis Testing","heading":"9.1.2 Hypothesis tests and confidence intervals","text":"Even though hypothesis tests confidence intervals two different approaches different goals, complement .\nexample, Subsection 8.1.4 calculated 95% confidence interval almonds’ mean weight, \\(\\mu\\), using sample almonds_sample_100. theory-based approach given \\[\n\\begin{aligned}\n\\left(\\overline{x} - 1.98 \\frac{s}{\\sqrt{n}},\\quad \\overline{x} + 1.98 \\frac{s}{\\sqrt{n}}\\right)\n\\end{aligned}\n\\]95% confidence interval :Using simulation-based approach via bootstrap percentile method, 95% confidence interval isBoth 95% confidence intervals similar , importantly, \nintervals contain \\(\\mu = 3.6\\) grams.\nRecall performing hypothesis testing rejected null hypothesis,\n\\(H_0: \\mu = 3.6\\).\nresults obtained using confidence intervals consistent \nconclusions hypothesis testing.general, values \\(\\mu\\) null hypothesis part \nconfidence interval, null hypothesis rejected.\nNote, however, confidence level used constructing interval,\n95% example, needs consistent significance level,\n\\(\\alpha\\), used hypothesis test.\nparticular, hypothesis test two-sided significance level\n\\(\\alpha\\) used, calculate confidence interval confidence level equal\n\\((1 - \\alpha)\\times 100\\%\\). example, \\(\\alpha = 0.05\\) \ncorresponding confidence level \\((1 - 0.05) = 0.95\\) \\(95\\%\\).\ncorrespondence direct confidence intervals calculate\nalways two-sided.\nhand, hypothesis test used one-sided (left right),\ncalculate confidence interval confidence level equal \n\\((1 - 2\\alpha)\\times 100\\%\\). example, \\(\\alpha = 0.05\\) , \ncorresponding confidence level needed \\((1 - 2\\cdot 0.05) = 0.9\\) \\(90\\%\\).section concludes discussion one-sample hypothesis tests.\nObserve , done confidence intervals, can also construct\nhypothesis tests proportions, using bootstrap percentile method,\ncan also quantities, population median, quartiles, etc.focus now building hypothesis tests two-sample problems.","code":"\nalmonds_sample_100 |>\n  summarize(lower_bound = mean(weight) - 1.98*sd(weight)/sqrt(length(weight)),\n            upper_bound = mean(weight) + 1.98*sd(weight)/sqrt(length(weight)))# A tibble: 1 × 2\n  lower_bound upper_bound\n        <dbl>       <dbl>\n1     3.61028     3.75372\nbootstrap_means <- almonds_sample_100 |> \n  specify(response = weight) |> \n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"mean\")\nbootstrap_means |> \n  get_confidence_interval(level = 0.95, type = \"percentile\")# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1  3.61198    3.756"},{"path":"hypothesis-testing.html","id":"ht-activity","chapter":"9 Hypothesis Testing","heading":"9.2 Music popularity activity","text":"Let’s start activity studying effect music genre Spotify song popularity.","code":""},{"path":"hypothesis-testing.html","id":"is-metal-music-more-popular-than-deep-house-music","chapter":"9 Hypothesis Testing","heading":"9.2.1 Is metal music more popular than deep house music?","text":"Imagine music analyst Spotify, curious whether fans metal deep house passionate favorite genres. want determine ’s significant difference popularity two genres. Popularity, case, measured Spotify, say, average number streams recent user interactions tracks classified genre. (Note Spotify actually disclose metric calculated, take best guess.) question sets stage exploration hypothesis testing.Metal music, characterized aggressive sounds, powerful vocals, complex instrumentals, cultivated loyal fanbase often prides deep appreciation genre’s intensity technical skill. hand, deep house music, smooth, soulful rhythms steady beats, attracts listeners enjoy genre’s calming immersive vibe, often associated late-night clubs chill-sessions.comparing popularity metrics two genres, can determine one truly resonates listeners Spotify. exploration deepens understanding musical preferences also serves practical introduction principles hypothesis testing.begin analysis, 2000 tracks selected random Spotify’s song archive. use “song” “track” interchangeably going forward. 1000 metal tracks 1000 deep house tracks selected.moderndive package contains data songs genre spotify_by_genre data frame. six genres selected data (country, deep-house, dubstep, hip-hop, metal, rock). opportunity explore relationships genres popularity Learning checks. Let’s explore data focusing just metal deep-house looking 12 randomly selected rows columns interest Table 9.2. Note also group selection four possible groupings track_genre popular_or_not selected.\nTABLE 9.2: Sample twelve songs Spotify data frame.\ntrack_genre variable indicates genre song classified , artists track_name columns provide additional information track providing artist name song, popularity metric mentioned earlier given Spotify, popular_or_not categorical representation popularity column value 50 (75th percentile popularity) referring popular anything 50 not_popular. decision made authors call song “popular” 75th percentile (3rd quartile) popularity arbitrary changed value.)Let’s perform exploratory data analysis relationship two categorical variables track_genre popular_or_not. Recall saw Subsection 2.8.3 one way can visualize relationship using stacked barplot.\nFIGURE 9.2: Barplot relating genre popularity.\nObserve Figure 9.2 , sample, metal songs slightly popular deep house songs looking height popular bars. Let’s quantify popularity rates computing proportion songs classified popular two genres using dplyr package data wrangling. Note use tally() function shortcut summarize(n = n()) get counts.1000 metal songs, 563 popular, proportion 563/1000 = 0.563 = 56.3%. hand, 1000 deep house songs, 529 popular, proportion 529/1000 = 0.529 = 52.9%. Comparing two rates popularity, appears metal songs popular rate 0.563 \\(-\\) 0.529 = 0.034 = 3.4% higher deep house songs. suggestive advantage metal songs terms popularity.question , however, provide conclusive evidence greater popularity metal songs compared deep house songs? difference popularity rates 3.4% still occur chance, even hypothetical world difference popularity existed two genres? words, role sampling variation hypothesized world? answer question, rely computer run simulations.","code":"\nspotify_metal_deephouse <- spotify_by_genre |> \n  filter(track_genre %in% c(\"metal\", \"deep-house\")) |> \n  select(track_genre, artists, track_name, popularity, popular_or_not) \nspotify_metal_deephouse |>\n  group_by(track_genre, popular_or_not) |> \n  sample_n(size = 3)\nggplot(spotify_metal_deephouse, aes(x = track_genre, fill = popular_or_not)) +\n  geom_bar() +\n  labs(x = \"Genre of track\")\nspotify_metal_deephouse |> \n  group_by(track_genre, popular_or_not) |>\n  tally() # Same as summarize(n = n())# A tibble: 4 × 3\n# Groups:   track_genre [2]\n  track_genre popular_or_not     n\n  <chr>       <chr>          <int>\n1 deep-house  not popular      471\n2 deep-house  popular          529\n3 metal       not popular      437\n4 metal       popular          563"},{"path":"hypothesis-testing.html","id":"shuffling-once","chapter":"9 Hypothesis Testing","heading":"9.2.2 Shuffling once","text":"First, try imagine hypothetical universe difference popularity metal deep house. hypothetical universe, genre song bearing chances popularity. Bringing things back spotify_metal_deephouse data frame, popular_or_not variable thus irrelevant label. popular_or_not labels irrelevant, randomly reassign “shuffling” consequence!illustrate idea, let’s narrow focus 52 chosen songs 2000 saw earlier. track_genre column shows original genre song . Note keep smaller dataset 52 rows representative sample 2000 rows, sampled popularity rate metal deep-house close original rates 0.563 0.529, respectively, prior shuffling. data available spotify_52_original data frame moderndive package. also remove track_id column simplicity. identification variable relevant analysis. sample shown Table 9.3.\nTABLE 9.3: Representative sample metal deep-house songs\nhypothesized universe difference genre popularity, popularity irrelevant thus consequence randomly “shuffle” values popular_or_not. popular_or_not column spotify_52_shuffled data frame moderndive package shows one possible random shuffling.\nTABLE 9.4: Shuffled version popular_or_not sample\nObserve Table 9.4 popular_or_not column popular popular results now listed different order. original popular now popular, popular popular, others original., random shuffling popular_or_not label makes sense hypothesized universe difference popularity genres. tactile way us understand going shuffling? One way using standard deck 52 playing cards, display Figure 9.3.\nFIGURE 9.3: Standard deck 52 playing cards.\nSince started equal sample sizes 1000 songs genre, can think splitting deck half 26 cards two piles (one metal another deep-house). shuffling 52 cards seen Figure 9.4, split deck equally two piles 26 cards . , can flip cards one--one, assigning “popular” red card “popular” black card keeping tally many genre popular.\nFIGURE 9.4: Shuffling deck cards.\nLet’s repeat exploratory data analysis original spotify_metal_deephouse data spotify_52_original spotify_52_shuffled data frames. Let’s create barplot visualizing relationship track_genre new shuffled popular_or_not variable, compare original un-shuffled version Figure 9.5.\nFIGURE 9.5: Barplots relationship genre popular ' (left) shuffledpopular ’ (right).\ndifference metal versus deep house popularity rates now different. Compared original data left barplot, new “shuffled” data right barplot popularity rates actually opposite direction originally. shuffling process removed relationship genre popularity.Let’s also compute proportion tracks now “popular” popular_or_not column genre:one sample hypothetical universe difference genre popularity, \\(13/26 = 0.5 = 50\\%\\) metal songs popular. hand, \\(16/26 = 0.615 = 61.5\\%\\) deep house songs popular. Let’s next compare two values. appears metal tracks popular rate \\(0.5 - 0.615 = -0.115 = -11.5\\) percentage points different deep house songs.Observe difference rates difference rates 0.034 = 3.4% originally observed. due sampling variation. can better understand effect sampling variation? repeating shuffling several times!","code":"\nspotify_52_original |> \n  select(-track_id) |> \n  head(10)\nspotify_52_shuffled |> \n  select(-track_id) |> \n  head(10)\nggplot(spotify_52_shuffled, aes(x = track_genre, fill = popular_or_not)) +\n  geom_bar() + \n  labs(x = \"Genre of track\")\nspotify_52_shuffled |> \n  group_by(track_genre, popular_or_not) |> \n  tally()# A tibble: 4 × 3\n# Groups:   track_genre [2]\n  track_genre popular_or_not     n\n  <chr>       <chr>          <int>\n1 deep-house  not popular       10\n2 deep-house  popular           16\n3 metal       not popular       13\n4 metal       popular           13"},{"path":"hypothesis-testing.html","id":"ht-what-did-we-just-do","chapter":"9 Hypothesis Testing","heading":"9.2.3 What did we just do?","text":"just demonstrated activity statistical procedure known hypothesis testing using permutation test. term “permutation” mathematical term “shuffling”: taking series values reordering randomly, playing cards. fact, permutations another form resampling, like bootstrap method performed Chapter 8. bootstrap method involves resampling replacement, permutation methods involve resampling without replacement.need restrict analysis dataset 52 rows . useful manually shuffle deck cards assign values popular popular different songs, ideas can applied 2000 tracks spotify_metal_deephouse data. can think data inference unknown difference population proportions 2000 tracks sample. denote \\(p_{m} - p_{d}\\), \\(p_{m}\\) population proportion songs metal names popular \\(p_{d}\\) equivalent deep house songs. Recall one scenarios inference seen far Table 9.5.\nTABLE 9.5: Scenarios sampling inference\n, based sample \\(n_m = 1000\\) metal tracks \\(n_f = 1000\\) deep house tracks, point estimate \\(p_{m} - p_{d}\\) difference sample proportions\\[\\widehat{p}_{m} -\\widehat{p}_{f} = 0.563 - 0.529 = 0.034.\\]difference favor metal songs 0.034 (3.4 percentage points) greater 0, suggesting metal songs popular deep house songs.However, question ask “difference meaningfully greater 0?”. words, difference indicative true popularity, can just attribute sampling variation? Hypothesis testing allows us make distinctions.","code":""},{"path":"hypothesis-testing.html","id":"understanding-ht","chapter":"9 Hypothesis Testing","heading":"9.3 Understanding hypothesis tests","text":"Much like terminology, notation, definitions relating sampling saw Section 7.2, lot terminology, notation, definitions related hypothesis testing well. introduced Section 9.1. Learning may seem like daunting task first. However, practice, practice, practice, anyone can master .First, hypothesis statement value unknown population parameter. genre popularity activity, population parameter interest difference population proportions \\(p_{m} - p_{d}\\). Hypothesis tests can involve population parameters Table 8.1 five inference scenarios cover book also advanced types cover .Second, hypothesis test consists test two competing hypotheses: (1) null hypothesis \\(H_0\\) (pronounced “H-naught”) versus (2) alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\)).working comparison two populations parameters, typically, null hypothesis claim “effect” “difference interest.” many cases, null hypothesis represents status quo. Furthermore, alternative hypothesis claim experimenter researcher wants establish find evidence support. viewed “challenger” hypothesis null hypothesis \\(H_0\\). genre popularity activity, appropriate hypothesis test :\\[\n\\begin{aligned}\nH_0 &: \\text{metal deep house popularity rate}\\\\\n\\text{vs } H_A &: \\text{metal popular higher rate deep house}\n\\end{aligned}\n\\]Note choices made. First, set null hypothesis \\(H_0\\) difference popularity rate “challenger” alternative hypothesis \\(H_A\\) difference favor metal. discussed earlier, null hypothesis set reflect situation “change.” discussed earlier, case, \\(H_0\\) corresponds difference popularity. Furthermore, set \\(H_A\\) metal popular higher rate, subjective choice reflecting prior suspicion case. discussed earlier one-sided test. can left- right-sided, becomes clear express terms proportions. someone else however share suspicions wants investigate difference, whether higher lower, construct two-sided test.can re-express formulation hypothesis test using mathematical notation population parameter interest, difference population proportions \\(p_{m} - p_{d}\\):\\[\n\\begin{aligned}\nH_0 &: p_{m} - p_{d} = 0\\\\\n\\text{vs } H_A&: p_{m} - p_{d} > 0\n\\end{aligned}\n\\]Observe alternative hypothesis \\(H_A\\) written \\(p_{m} - p_{d} > 0\\). Since chosen particular formulation, one-sided test becomes right-sided looking difference greater zero evidence reject null hypothesis. opted two-sided alternative, set \\(p_{m} - p_{d} \\neq 0\\). work right-sided test present example two-sided test Section 9.6.Third, test statistic point estimate/sample statistic formula used hypothesis testing. Note sample statistic merely summary statistic based sample observations. Recall saw Section 3.3 summary statistic takes many values returns one. , samples \\(n_m = 1000\\) metal songs \\(n_f = 1000\\) deep house songs. Hence, point estimate interest difference sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{d}\\).Fourth, observed test statistic value test statistic observed real life. case, computed value using data saved spotify_metal_deephouse data frame. observed difference \\(\\widehat{p}_{m} -\\widehat{p}_{d} = 0.563 - 0.529 = 0.034 = 3.4\\%\\) favor metal songs.Fifth, null distribution sampling distribution test statistic assuming null hypothesis \\(H_0\\) true. Let’s unpack ideas slowly. key understanding null distribution null hypothesis \\(H_0\\) assumed true. saying \\(H_0\\) true point, assuming true hypothesis-testing purposes. case, corresponds hypothesized universe difference popularity rates. Assuming null hypothesis \\(H_0\\), also stated “\\(H_0\\),” test statistic vary due sampling variation? case, difference sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) vary due sampling \\(H_0\\)? Recall Subsection 7.3.4 distributions displaying point estimates vary due sampling variation called sampling distributions. additional thing keep mind null distributions sampling distributions assuming null hypothesis \\(H_0\\) true.Sixth, \\(p\\)-value probability obtaining test statistic just extreme extreme observed test statistic assuming null hypothesis \\(H_0\\) true. can think \\(p\\)-value quantification “surprise”: assuming \\(H_0\\) true, surprised observed? case, hypothesized universe difference genre popularity, surprised observed higher popularity rates 0.034 collected samples difference genre popularity exists? surprised? Somewhat surprised?\\(p\\)-value quantifies probability, proportion “extreme” result? , extreme defined terms alternative hypothesis \\(H_A\\) metal popularity higher rate deep house. words, often popularity metal even pronounced \\(0.563 - 0.529 = 0.034 = 3.4\\%\\)?Seventh lastly, many hypothesis testing procedures, commonly recommended set significance level test beforehand. denoted \\(\\alpha\\). Please review discussion \\(\\alpha\\) Section 9.1.1 discussed theory-based approach. now, sufficient recall \\(p\\)-value less equal \\(\\alpha\\), reject null hypothesis \\(H_0\\).Alternatively, \\(p\\)-value greater \\(\\alpha\\), “fail reject \\(H_0\\).” Note latter statement quite saying “accept \\(H_0\\).” distinction rather subtle immediately obvious. revisit later Section 9.5.different fields tend use different values \\(\\alpha\\), commonly used values \\(\\alpha\\) 0.1, 0.01, 0.05; 0.05 choice people often make without putting much thought . talk \\(\\alpha\\) significance levels Section 9.5, first let’s fully conduct hypothesis test corresponding genre popularity activity using infer package.","code":""},{"path":"hypothesis-testing.html","id":"ht-infer","chapter":"9 Hypothesis Testing","heading":"9.4 Conducting hypothesis tests","text":"Section 8.2.2, showed construct confidence intervals. first illustrated using dplyr data wrangling verbs rep_sample_n() function Subsection 7.1.3 used virtual shovel. particular, constructed confidence intervals resampling replacement setting replace = TRUE argument rep_sample_n() function.showed perform task using infer package workflow. workflows resulted bootstrap distribution can construct confidence intervals, infer package workflow emphasizes steps overall process Figure 9.6. using function names intuitively named verbs:specify() variables interest data frame.generate() replicates bootstrap resamples replacement.calculate() summary statistic interest.visualize() resulting bootstrap distribution confidence interval.\nFIGURE 9.6: Confidence intervals infer package.\nsection, now show seamlessly modify previously seen infer code constructing confidence intervals conduct hypothesis tests. notice basic outline workflow almost identical, except additional hypothesize() step specify() generate() steps, can seen Figure 9.7.\nFIGURE 9.7: Hypothesis testing infer package.\nFurthermore, use pre-specified significance level \\(\\alpha\\) = 0.1 hypothesis test. Please read discussion \\(\\alpha\\) Subsection 9.1.1 later Section 9.5.","code":""},{"path":"hypothesis-testing.html","id":"infer-workflow-ht","chapter":"9 Hypothesis Testing","heading":"9.4.1 infer package workflow","text":"","code":""},{"path":"hypothesis-testing.html","id":"specify-variables-2","chapter":"9 Hypothesis Testing","heading":"1. specify variables","text":"Recall use specify() verb specify response variable , needed, explanatory variables study. case, since interested potential effects genre popularity rates, set popular_or_not response variable track_genre explanatory variable. using formula = response ~ explanatory response name response variable data frame explanatory name explanatory variable. case popular_or_not ~ track_genre.Furthermore, since interested proportion songs \"popular\", proportion songs popular, set argument success \"popular\"., notice spotify_metal_deephouse data change, Response: popular_or_not (factor) Explanatory: track_genre (factor) meta-data . similar group_by() verb dplyr change data, adds “grouping” meta-data, saw Section 3.4. also now focus two columns interest data problem hand popular_or_not track_genre.","code":"\nspotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\")Response: popular_or_not (factor)\nExplanatory: track_genre (factor)\n# A tibble: 2,000 × 2\n   popular_or_not track_genre\n   <fct>          <fct>      \n 1 popular        deep-house \n 2 popular        deep-house \n 3 popular        deep-house \n 4 popular        deep-house \n 5 popular        deep-house \n 6 popular        deep-house \n 7 popular        deep-house \n 8 popular        deep-house \n 9 popular        deep-house \n10 popular        deep-house \n# ℹ 1,990 more rows"},{"path":"hypothesis-testing.html","id":"hypothesize-the-null","chapter":"9 Hypothesis Testing","heading":"2. hypothesize the null","text":"order conduct hypothesis tests using infer workflow, need new step present confidence intervals: hypothesize(). Recall Section 9.3 hypothesis test \\[\n\\begin{aligned}\nH_0 &: p_{m} - p_{d} = 0\\\\\n\\text{vs. } H_A&: p_{m} - p_{d} > 0\n\\end{aligned}\n\\]words, null hypothesis \\(H_0\\) corresponding “hypothesized universe” stated difference genre popularity rates. set null hypothesis \\(H_0\\) infer workflow using null argument hypothesize() function either:\"point\" hypotheses involving single sample \"independence\" hypotheses involving two samples.case, since two samples (metal songs deep house songs), set null = \"independence\"., data changed yet. occur upcoming generate() step; merely setting meta-data now.terms \"point\" \"independence\" come ? two technical statistical terms. term “point” relates fact single group observations, test value single point. Going back pennies example Chapter 8, say wanted test mean weight chocolate-covered almonds equal 3.5 grams . testing value “point” \\(\\mu\\), mean weight grams chocolate-covered almonds, follows\\[\n\\begin{aligned}\nH_0 &: \\mu = 3.5\\\\\n\\text{vs } H_A&: \\mu \\neq 3.5\n\\end{aligned}\n\\]term “independence” relates fact two groups observations, testing whether response variable independent explanatory variable assigns groups. case, testing whether popular_or_not response variable “independent” explanatory variable track_genre.","code":"\nspotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  hypothesize(null = \"independence\")Response: popular_or_not (factor)\nExplanatory: track_genre (factor)\nNull Hypothesis: independence\n# A tibble: 2,000 × 2\n   popular_or_not track_genre\n   <fct>          <fct>      \n 1 popular        deep-house \n 2 popular        deep-house \n 3 popular        deep-house \n 4 popular        deep-house \n 5 popular        deep-house \n 6 popular        deep-house \n 7 popular        deep-house \n 8 popular        deep-house \n 9 popular        deep-house \n10 popular        deep-house \n# ℹ 1,990 more rows"},{"path":"hypothesis-testing.html","id":"generate-replicates-2","chapter":"9 Hypothesis Testing","heading":"3. generate replicates","text":"hypothesize() null hypothesis, generate() replicates “shuffled” datasets assuming null hypothesis true. repeating shuffling exercise performed Section 9.2 several times full dataset 2000 rows. Let’s use computer repeat 1000 times setting reps = 1000 generate() function. However, unlike confidence intervals generated replicates using type = \"bootstrap\" resampling replacement, now perform shuffles/permutations setting type = \"permute\". Recall shuffles/permutations kind resampling, unlike bootstrap method, involve resampling without replacement.resulting data frame 2,000,000 rows. performed shuffles/permutations 2000 rows 1000 times \\(2,000,000 = 1000 \\cdot 2000\\). explore spotify_generate data frame View(), notice variable replicate indicates resample row belongs . value 1 2000 times, value 2 2000 times, way value 1000 2000 times.","code":"\nspotify_generate <- spotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\")\nnrow(spotify_generate)[1] 2000000"},{"path":"hypothesis-testing.html","id":"calculate-summary-statistics-2","chapter":"9 Hypothesis Testing","heading":"4. calculate summary statistics","text":"Now generated 1000 replicates “shuffles” assuming null hypothesis true, let’s calculate() appropriate summary statistic 1000 shuffles. Section 9.3, point estimates related hypothesis testing specific name: test statistics. Since unknown population parameter interest difference population proportions \\(p_{m} - p_{d}\\), test statistic difference sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\).1000 shuffles, can calculate test statistic setting stat = \"diff props\". Furthermore, since interested \\(\\widehat{p}_{m} - \\widehat{p}_{d}\\) set order = c(\"metal\", \"deep-house\"). stated earlier, order subtraction matter, long stay consistent throughout analysis tailor interpretations accordingly. Let’s save result data frame called null_distribution:Observe 1000 values stat, representing one instance \\(\\widehat{p}_{m} - \\widehat{p}_{d}\\) hypothesized world difference genre popularity. Observe well chose name data frame carefully: null_distribution. Recall Section 9.3 sampling distributions null hypothesis \\(H_0\\) assumed true special name: null distribution.observed difference popularity rates? words, observed test statistic \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\)? Recall Section 9.2 computed observed difference hand 0.563 - 0.529 = 0.034 = 3.4%. can also compute value using previous infer code hypothesize() generate() steps removed. Let’s save obs_diff_prop:Note also wrapper function infer called observe() can used calculate observed test statistic. However, chose use specify(), calculate(), hypothesize() functions help continue use common verbs build practice .","code":"\nnull_distribution <- spotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in props\", order = c(\"metal\", \"deep-house\"))\nnull_distributionResponse: popular_or_not (factor)\nExplanatory: track_genre (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate       stat\n       <int>      <dbl>\n 1         1  0.0140000\n 2         2 -0.0420000\n 3         3  0.0220000\n 4         4 -0.0140000\n 5         5 -0.0180000\n 6         6 -0.0160000\n 7         7  0.0160000\n 8         8 -0.0400000\n 9         9  0.0140000\n10        10  0.0120000\n# ℹ 990 more rows\nobs_diff_prop <- spotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  calculate(stat = \"diff in props\", order = c(\"metal\", \"deep-house\"))\nobs_diff_propResponse: popular_or_not (factor)\nExplanatory: track_genre (factor)\n# A tibble: 1 × 1\n       stat\n      <dbl>\n1 0.0340000\nspotify_metal_deephouse |> \n  observe(formula = popular_or_not ~ track_genre, \n          success = \"popular\", \n          stat = \"diff in props\", \n          order = c(\"metal\", \"deep-house\"))Response: popular_or_not (factor)\nExplanatory: track_genre (factor)\n# A tibble: 1 × 1\n       stat\n      <dbl>\n1 0.0340000"},{"path":"hypothesis-testing.html","id":"visualize-the-p-value","chapter":"9 Hypothesis Testing","heading":"5. visualize the p-value","text":"final step measure surprised difference 3.4% hypothesized universe difference genre popularity. observed difference 0.034 highly unlikely, inclined reject validity hypothesized universe.start visualizing null distribution 1000 values \\(\\widehat{p}_{m} - \\widehat{p}_{d}\\) using visualize() Figure 9.8. Recall values difference popularity rates assuming \\(H_0\\) true. corresponds hypothesized universe difference genre popularity.\nFIGURE 9.8: Null distribution.\nLet’s now add happened real life Figure 9.8, observed difference popularity rates 0.563 - 0.529 = 0.034 = 3.4%. However, instead merely adding vertical line using geom_vline(), let’s use shade_p_value() function obs_stat set observed test statistic value saved obs_diff_prop.Furthermore, set direction = \"right\" reflecting alternative hypothesis \\(H_A: p_{m} - p_{d} > 0\\). Recall alternative hypothesis \\(H_A\\) \\(p_{m} - p_{d} > 0\\), stating difference popularity rates favor metal songs. “extreme” corresponds differences “bigger” “positive” “right.” Hence set direction argument shade_p_value() \"right\".hand, alternative hypothesis \\(H_A\\) possible one-sided alternative \\(p_{m} - p_{d} < 0\\), suggesting popularity favor deep house songs, set direction = \"left\". alternative hypothesis \\(H_A\\) two-sided \\(p_{m} - p_{d} \\neq 0\\), suggesting discrimination either direction, set direction = \"\".\nFIGURE 9.9: Shaded histogram show \\(p\\)-value.\nresulting Figure 9.9, solid dark line marks 0.034 = 3.4%. However, shaded-region correspond ? \\(p\\)-value. Recall definition \\(p\\)-value Section 9.3:\\(p\\)-value probability obtaining test statistic just extreme observed test statistic assuming null hypothesis \\(H_0\\) true.judging shaded region Figure 9.9, seems somewhat rarely observe differences popularity rates 0.034 = 3.4% hypothesized universe difference genre popularity. words, \\(p\\)-value somewhat small. Hence, inclined reject hypothesized universe, using statistical language “reject \\(H_0\\).”fraction null distribution shaded? words, exact value \\(p\\)-value? can compute using get_p_value() function arguments previous shade_p_value() code:Keeping definition \\(p\\)-value mind, probability observing difference popularity rates large 0.034 = 3.4% due sampling variation alone null distribution 0.065 = 6.5%. Since \\(p\\)-value smaller pre-specified significance level \\(\\alpha\\) = 0.1, reject null hypothesis \\(H_0: p_{m} - p_{d} = 0\\). words, \\(p\\)-value sufficiently small reject hypothesized universe difference genre popularity. instead enough evidence change mind favor difference genre popularity likely culprit . Observe whether reject null hypothesis \\(H_0\\) depends large part choice significance level \\(\\alpha\\). discuss Subsection 9.5.3.","code":"\nvisualize(null_distribution, bins = 25)\nvisualize(null_distribution, bins = 25) + \n  shade_p_value(obs_stat = obs_diff_prop, direction = \"right\")\nnull_distribution |> \n  get_p_value(obs_stat = obs_diff_prop, direction = \"right\")# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.065"},{"path":"hypothesis-testing.html","id":"comparing-infer-workflows","chapter":"9 Hypothesis Testing","heading":"9.4.2 Comparison with confidence intervals","text":"One great things infer package can jump seamlessly conducting hypothesis tests constructing confidence intervals minimal changes! Recall code previous section creates null distribution, turn needed compute \\(p\\)-value:create corresponding bootstrap distribution needed construct 90% confidence interval \\(p_{m} - p_{d}\\), need make two changes. First, remove hypothesize() step since longer assuming null hypothesis \\(H_0\\) true. can deleting commenting hypothesize() line code. Second, switch type resampling generate() step \"bootstrap\" instead \"permute\".Using bootstrap_distribution, let’s first compute percentile-based confidence intervals, Section 8.2.2:Using shorthand interpretation 90% confidence intervals, 90% “confident” true difference population proportions \\(p_{m} - p_{d}\\) (0, 0.07). Let’s visualize bootstrap_distribution percentile-based 90% confidence interval \\(p_{m} - p_{d}\\) Figure 9.10.\nFIGURE 9.10: Percentile-based 90% confidence interval.\nNotice key value included 90% confidence interval \\(p_{m} - p_{d}\\): value 0 (just barely!). words, difference 0 included net, suggesting \\(p_{m}\\) \\(p_{d}\\) truly different! Furthermore, observe entirety 90% confidence interval \\(p_{m} - p_{d}\\) lies 0, suggesting difference favor metal.\nLearning check\n(LC9.1) following code produce error? words, response predictor variables make possible computation infer package?(LC9.2) relatively confident distributions sample proportions good approximations population distributions popularity proportions two genres?(LC9.3) Using definition p-value, write words \\(p\\)-value represents hypothesis test comparing popularity rates metal deep house.","code":"\nnull_distribution <- spotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in props\", order = c(\"metal\", \"deep-house\"))\nbootstrap_distribution <- spotify_metal_deephouse |> \n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  # Change 1 - Remove hypothesize():\n  # hypothesize(null = \"independence\") |> \n  # Change 2 - Switch type from \"permute\" to \"bootstrap\":\n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in props\", order = c(\"metal\", \"deep-house\"))\npercentile_ci <- bootstrap_distribution |> \n  get_confidence_interval(level = 0.90, type = \"percentile\")\npercentile_ci# A tibble: 1 × 2\n     lower_ci  upper_ci\n        <dbl>     <dbl>\n1 0.000355780 0.0701690\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci)\nlibrary(moderndive)\nlibrary(infer)\nnull_distribution_mean <- spotify_metal_deephouse |>\n  specify(formula = popular_or_not ~ track_genre, success = \"popular\") |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in means\", order = c(\"metal\", \"deep-house\"))"},{"path":"hypothesis-testing.html","id":"only-one-test","chapter":"9 Hypothesis Testing","heading":"9.4.3 There is only one test","text":"Let’s recap steps necessary conduct hypothesis test using terminology, notation, definitions related sampling saw Section 9.3 infer workflow Subsection 9.4.1:specify() variables interest data frame.hypothesize() null hypothesis \\(H_0\\). words, set “model universe” assuming \\(H_0\\) true.generate() shuffles assuming \\(H_0\\) true. words, simulate data assuming \\(H_0\\) true.calculate() test statistic interest, observed data simulated data.visualize() resulting null distribution compute \\(p\\)-value comparing null distribution observed test statistic.lot digest, especially first time encounter hypothesis testing, nice thing understand general framework, can understand hypothesis test. famous blog post, computer scientist Allen Downey called “one test” framework, created flowchart displayed Figure 9.11.\nFIGURE 9.11: Allen Downey’s hypothesis testing framework.\nNotice similarity “hypothesis testing infer” diagram saw Figure 9.7. infer package explicitly designed match “one test” framework. can understand framework, can easily generalize ideas hypothesis-testing scenarios. Whether population proportions \\(p\\), population means \\(\\mu\\), differences population proportions \\(p_1 - p_2\\), differences population means \\(\\mu_1 - \\mu_2\\), see Chapter 10 inference regression, population regression slopes \\(\\beta_1\\) well. fact, applies generally even just examples complicated hypothesis tests test statistics well.\nLearning check\n(LC9.4) Describe paragraph used Allen Downey’s diagram conclude statistical difference existed popularity rate metal deep house Spotify example.","code":""},{"path":"hypothesis-testing.html","id":"ht-interpretation","chapter":"9 Hypothesis Testing","heading":"9.5 Interpreting hypothesis tests","text":"Interpreting results hypothesis tests one challenging aspects method statistical inference. section, focus ways help deciphering process address common misconceptions.","code":""},{"path":"hypothesis-testing.html","id":"trial","chapter":"9 Hypothesis Testing","heading":"9.5.1 Two possible outcomes","text":"Section 9.3, mentioned given pre-specified significance level \\(\\alpha\\) two possible outcomes hypothesis test:\\(p\\)-value less \\(\\alpha\\), reject null hypothesis \\(H_0\\) favor \\(H_A\\).\\(p\\)-value greater equal \\(\\alpha\\), fail reject null hypothesis \\(H_0\\).Unfortunately, latter result often misinterpreted “accepting null hypothesis \\(H_0\\).” first glance may seem statements “failing reject \\(H_0\\)” “accepting \\(H_0\\)” equivalent, actually subtle difference. Saying “accept null hypothesis \\(H_0\\)” equivalent stating “think null hypothesis \\(H_0\\) true.” However, saying “fail reject null hypothesis \\(H_0\\)” saying something else: “\\(H_0\\) might still false, enough evidence say .” words, absence enough proof. However, absence proof proof absence.shed light distinction, let’s use United States criminal justice system analogy. criminal trial United States similar situation hypothesis tests whereby choice two contradictory claims must made defendant trial:defendant truly either “innocent” “guilty.”defendant presumed “innocent proven guilty.”defendant found guilty strong evidence defendant guilty. phrase “beyond reasonable doubt” often used guideline determining cutoff enough evidence exists find defendant guilty.defendant found either “guilty” “guilty” ultimate verdict.words, guilty verdicts suggesting defendant innocent, instead “defendant may still actually guilty, enough evidence prove fact.” Now let’s make connection hypothesis tests:Either null hypothesis \\(H_0\\) alternative hypothesis \\(H_A\\) true.Hypothesis tests conducted assuming null hypothesis \\(H_0\\) true.reject null hypothesis \\(H_0\\) favor \\(H_A\\) evidence found sample suggests \\(H_A\\) true. significance level \\(\\alpha\\) used guideline set threshold just strong evidence require.ultimately decide either “fail reject \\(H_0\\)” “reject \\(H_0\\).”gut instinct may suggest “failing reject \\(H_0\\)” “accepting \\(H_0\\)” equivalent statements, . “Accepting \\(H_0\\)” equivalent finding defendant innocent. However, courts find defendants “innocent,” rather find “guilty.” Putting things differently, defense attorneys need prove clients innocent, rather need prove clients “guilty beyond reasonable doubt.”going back songs activity Section 9.4, recall hypothesis test \\(H_0: p_{m} - p_{d} = 0\\) versus \\(H_A: p_{m} - p_{d} > 0\\) used pre-specified significance level \\(\\alpha\\) = 0.1. found \\(p\\)-value 0.065. Since \\(p\\)-value smaller \\(\\alpha\\) = 0.1, rejected \\(H_0\\). words, found needed levels evidence particular sample say \\(H_0\\) false \\(\\alpha\\) = 0.1 significance level. also state conclusion using non-statistical language: found enough evidence data suggest difference popularity two genres music.","code":""},{"path":"hypothesis-testing.html","id":"types-of-errors","chapter":"9 Hypothesis Testing","heading":"9.5.2 Types of errors","text":"Unfortunately, chance jury judge can make incorrect decision criminal trial reaching wrong verdict. example, finding truly innocent defendant “guilty.” hand, finding truly guilty defendant “guilty.” can often stem fact prosecutors access relevant evidence, instead limited whatever evidence police can find.holds hypothesis tests. can make incorrect decisions population parameter sample data population thus sampling variation can lead us incorrect conclusions.two possible erroneous conclusions criminal trial: either (1) truly innocent person found guilty (2) truly guilty person found guilty. Similarly, two possible errors hypothesis test: either (1) rejecting \\(H_0\\) fact \\(H_0\\) true, called Type error (2) failing reject \\(H_0\\) fact \\(H_0\\) false, called Type II error. Another term used “Type error” “false positive,” another term “Type II error” “false negative.”risk error price researchers pay basing inference sample instead performing census entire population. seen numerous examples activities far, censuses often expensive times impossible, thus researchers choice use sample. Thus hypothesis test based sample, choice tolerate chance Type error made chance Type II error occur.help understand concepts Type error Type II errors, apply terms criminal justice analogy Figure 9.12.\nFIGURE 9.12: Type Type II errors criminal trials.\nThus, Type error corresponds incorrectly putting truly innocent person jail, whereas Type II error corresponds letting truly guilty person go free. Let’s show corresponding table Figure 9.13 hypothesis tests.\nFIGURE 9.13: Type Type II errors hypothesis tests.\n","code":""},{"path":"hypothesis-testing.html","id":"choosing-alpha","chapter":"9 Hypothesis Testing","heading":"9.5.3 How do we choose alpha?","text":"using sample make inferences population, operating uncertainty run risk making statistical errors. errors calculations procedure used, errors sense sample used may lead us construct confidence interval contain true value population parameter, example.\ncase hypothesis testing, two well-defined errors: Type Type II error:Type Error rejecting null hypothesis true. probability Type Error occurring \\(\\alpha\\), significance level, defined Subsection 9.1.1 Section 9.3A Type II Error failing reject null hypothesis false. probability Type II Error denoted \\(\\beta\\). value \\(1-\\beta\\) known power test.Ideally, like minimize errors, like \\(\\alpha = 0\\) \\(\\beta = 0\\). However, possible always possibility committing one error making decision based sample data. Furthermore, two error probabilities inversely related. probability Type error goes , probability Type II error goes .constructing hypothesis test, control probability committing Type Error can decide significance level \\(\\alpha\\) want use. \\(\\alpha\\) pre-specified, try minimize \\(\\beta\\), fraction incorrect non-rejections null hypothesis.example used \\(\\alpha\\) = 0.01, using hypothesis testing procedure long run incorrectly reject null hypothesis \\(H_0\\) one percent time. analogous setting confidence level confidence interval.value use \\(\\alpha\\)? different fields study adopted different conventions, although \\(\\alpha = 0.05\\) perhaps popular threshold, nothing special number. Please review Subsection 9.1.1 discussion \\(\\alpha\\) tolerance uncertainty. addition, observe choosing relatively small value \\(\\alpha\\) reduces chances rejecting null hypothesis, also committing Type Error; increases probability committing Type II Error.hand, choosing relatively large value \\(\\alpha\\) increases chances failing reject null hypothesis, also committing Type Error; reduces probability committing Type II Error. Depending problem hand, may willing larger significance level certain scenarios smaller significance level others.\nLearning check\n(LC9.5) wrong saying, “defendant innocent.” based US system criminal trials?(LC9.6) purpose hypothesis testing?(LC9.7) flaws hypothesis testing? alleviate ?(LC9.8) Consider two \\(\\alpha\\) significance levels 0.1 0.01. two, lead higher chance committing Type Error?","code":""},{"path":"hypothesis-testing.html","id":"ht-case-study","chapter":"9 Hypothesis Testing","heading":"9.6 Case study: are action or romance movies rated higher?","text":"Let’s apply knowledge hypothesis testing answer question: “action romance movies rated higher IMDb?”. IMDb database internet providing information movie television show casts, plot summaries, trivia, ratings. investigate , average, action romance movies get higher ratings IMDb.","code":""},{"path":"hypothesis-testing.html","id":"imdb-data","chapter":"9 Hypothesis Testing","heading":"9.6.1 IMDb ratings data","text":"movies dataset ggplot2movies package contains information 58,788 movies rated users IMDb.com.focus random sample 68 movies classified either “action” “romance” movies . disregard movies classified can assign 68 movies either category. Furthermore, since original movies dataset little messy, provide pre-wrangled version data movies_sample data frame included moderndive package. curious, can look necessary data-wrangling code GitHub.variables include title year movie filmed. Furthermore, numerical variable rating, IMDb rating 10 stars, binary categorical variable genre indicating movie Action Romance movie. interested whether Action Romance movies got higher rating average.Let’s perform exploratory data analysis data. Recall Subsection 2.7.1 boxplot visualization can use show relationship numerical categorical variable. Another option saw Section 2.6 use faceted histogram. However, interest brevity, let’s present boxplot Figure 9.14.\nFIGURE 9.14: Boxplot IMDb rating vs. genre.\nEyeballing Figure 9.14, romance movies higher median rating. reason believe, however, significant difference mean rating action movies compared romance movies? hard say just based plot. boxplot show median sample rating higher romance movies.However, large amount overlap boxes. Recall median necessarily mean either, depending whether distribution skewed.Let’s calculate summary statistics split binary categorical variable genre: number movies, mean rating, standard deviation split genre. using dplyr data wrangling verbs. Notice particular count number type movie using n() summary function.Observe 36 movies average rating 6.322 stars 32 movies average rating 5.275 stars. difference average ratings thus 6.322 - 5.275 = 1.047. appears edge 1.047 stars favor romance movies. question , however, results indicative true difference romance action movies? attribute difference chance sampling variation?","code":"\nmovies# A tibble: 58,788 × 24\n   title        year length budget rating votes    r1    r2    r3    r4    r5    r6    r7    r8    r9   r10 mpaa  Action\n   <chr>       <int>  <int>  <int>  <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>  <int>\n 1 $            1971    121     NA    6.4   348   4.5   4.5   4.5   4.5  14.5  24.5  24.5  14.5   4.5   4.5 \"\"         0\n 2 $1000 a To…  1939     71     NA    6      20   0    14.5   4.5  24.5  14.5  14.5  14.5   4.5   4.5  14.5 \"\"         0\n 3 $21 a Day …  1941      7     NA    8.2     5   0     0     0     0     0    24.5   0    44.5  24.5  24.5 \"\"         0\n 4 $40,000      1996     70     NA    8.2     6  14.5   0     0     0     0     0     0     0    34.5  45.5 \"\"         0\n 5 $50,000 Cl…  1975     71     NA    3.4    17  24.5   4.5   0    14.5  14.5   4.5   0     0     0    24.5 \"\"         0\n 6 $pent        2000     91     NA    4.3    45   4.5   4.5   4.5  14.5  14.5  14.5   4.5   4.5  14.5  14.5 \"\"         0\n 7 $windle      2002     93     NA    5.3   200   4.5   0     4.5   4.5  24.5  24.5  14.5   4.5   4.5  14.5 \"R\"        1\n 8 '15'         2002     25     NA    6.7    24   4.5   4.5   4.5   4.5   4.5  14.5  14.5  14.5   4.5  14.5 \"\"         0\n 9 '38          1987     97     NA    6.6    18   4.5   4.5   4.5   0     0     0    34.5  14.5   4.5  24.5 \"\"         0\n10 '49-'17      1917     61     NA    6      51   4.5   0     4.5   4.5   4.5  44.5  14.5   4.5   4.5   4.5 \"\"         0\n# ℹ 58,778 more rows\n# ℹ 6 more variables: Animation <int>, Comedy <int>, Drama <int>, Documentary <int>, Romance <int>, Short <int>\nmovies_sample# A tibble: 68 × 4\n   title                     year rating genre  \n   <chr>                    <int>  <dbl> <chr>  \n 1 Underworld                1985    3.1 Action \n 2 Love Affair               1932    6.3 Romance\n 3 Junglee                   1961    6.8 Romance\n 4 Eversmile, New Jersey     1989    5   Romance\n 5 Search and Destroy        1979    4   Action \n 6 Secreto de Romelia, El    1988    4.9 Romance\n 7 Amants du Pont-Neuf, Les  1991    7.4 Romance\n 8 Illicit Dreams            1995    3.5 Action \n 9 Kabhi Kabhie              1976    7.7 Romance\n10 Electric Horseman, The    1979    5.8 Romance\n# ℹ 58 more rows\nggplot(data = movies_sample, aes(x = genre, y = rating)) +\n  geom_boxplot() +\n  labs(y = \"IMDb rating\")\nmovies_sample |> \n  group_by(genre) |> \n  summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating))# A tibble: 2 × 4\n  genre       n mean_rating std_dev\n  <chr>   <int>       <dbl>   <dbl>\n1 Action     32     5.275   1.36121\n2 Romance    36     6.32222 1.60963"},{"path":"hypothesis-testing.html","id":"sampling-scenario-1","chapter":"9 Hypothesis Testing","heading":"9.6.2 Sampling scenario","text":"Let’s now revisit study terms terminology notation related sampling studied Subsection 7.2.1. study population movies IMDb database either action romance (). sample population 68 movies included movies_sample dataset.Since sample randomly taken population movies, representative romance action movies IMDb. Thus, analysis results based movies_sample can generalize entire population. relevant population parameter point estimates? introduce fourth sampling scenario Table 9.6.\nTABLE 9.6: Scenarios sampling inference\n, whereas sampling bowl exercise Section 7.1 concerned proportions, almonds exercise Section 8.2.1 concerned means, case study whether yawning contagious Section 8.4 music genre activity Section 9.2 concerned differences proportions, now concerned differences means.words, population parameter interest difference population mean ratings \\(\\mu_a - \\mu_r\\), \\(\\mu_a\\) mean rating action movies IMDb similarly \\(\\mu_r\\) mean rating romance movies. Additionally point estimate/sample statistic interest difference sample means \\(\\overline{x}_a - \\overline{x}_r\\), \\(\\overline{x}_a\\) mean rating \\(n_a\\) = 32 movies sample \\(\\overline{x}_r\\) mean rating \\(n_r\\) = 36 sample. Based earlier exploratory data analysis, estimate \\(\\overline{x}_a - \\overline{x}_r\\) \\(5.275 - 6.322 = -1.047\\).appears slight difference -1.047 favor romance movies. question , however, difference -1.047 merely due chance sampling variation? results indicative true difference mean ratings romance action movies IMDb? answer question, use hypothesis testing.","code":""},{"path":"hypothesis-testing.html","id":"conducting-the-hypothesis-test","chapter":"9 Hypothesis Testing","heading":"9.6.3 Conducting the hypothesis test","text":"testing:\\[\n\\begin{aligned}\nH_0 &: \\mu_a - \\mu_r = 0\\\\\n\\text{vs } H_A&: \\mu_a - \\mu_r \\neq 0\n\\end{aligned}\n\\]words, null hypothesis \\(H_0\\) suggests romance action movies mean rating. “hypothesized universe” assume true. hand, alternative hypothesis \\(H_A\\) suggests difference. Unlike one-sided alternative used popularity exercise \\(H_A: p_m - p_f > 0\\), now considering two-sided alternative \\(H_A: \\mu_a - \\mu_r \\neq 0\\).Furthermore, pre-specify low significance level \\(\\alpha\\) = 0.001. setting value low, things equal, lower chance \\(p\\)-value less \\(\\alpha\\). Thus, lower chance reject null hypothesis \\(H_0\\) favor alternative hypothesis \\(H_A\\). words, reject hypothesis difference mean ratings action romance movies, quite strong evidence. known “conservative” hypothesis testing procedure.","code":""},{"path":"hypothesis-testing.html","id":"specify-variables-3","chapter":"9 Hypothesis Testing","heading":"1. specify variables","text":"Let’s now perform steps infer workflow. first specify() variables interest movies_sample data frame using formula rating ~ genre. tells infer numerical variable rating outcome variable, binary variable genre explanatory variable. Note unlike previously interested proportions, since now interested mean numerical variable, need set success argument.Observe point data movies_sample changed. change far newly defined Response: rating (numeric) Explanatory: genre (factor) meta-data.","code":"\nmovies_sample |> \n  specify(formula = rating ~ genre)Response: rating (numeric)\nExplanatory: genre (factor)\n# A tibble: 68 × 2\n   rating genre  \n    <dbl> <fct>  \n 1    3.1 Action \n 2    6.3 Romance\n 3    6.8 Romance\n 4    5   Romance\n 5    4   Action \n 6    4.9 Romance\n 7    7.4 Romance\n 8    3.5 Action \n 9    7.7 Romance\n10    5.8 Romance\n# ℹ 58 more rows"},{"path":"hypothesis-testing.html","id":"hypothesize-the-null-1","chapter":"9 Hypothesis Testing","heading":"2. hypothesize the null","text":"set null hypothesis \\(H_0: \\mu_a - \\mu_r = 0\\) using hypothesize() function. Since two samples, action romance movies, set null \"independence\" described Section 9.4.","code":"\nmovies_sample |> \n  specify(formula = rating ~ genre) |> \n  hypothesize(null = \"independence\")Response: rating (numeric)\nExplanatory: genre (factor)\nNull Hypothesis: independence\n# A tibble: 68 × 2\n   rating genre  \n    <dbl> <fct>  \n 1    3.1 Action \n 2    6.3 Romance\n 3    6.8 Romance\n 4    5   Romance\n 5    4   Action \n 6    4.9 Romance\n 7    7.4 Romance\n 8    3.5 Action \n 9    7.7 Romance\n10    5.8 Romance\n# ℹ 58 more rows"},{"path":"hypothesis-testing.html","id":"generate-replicates-3","chapter":"9 Hypothesis Testing","heading":"3. generate replicates","text":"set null hypothesis, generate “shuffled” replicates assuming null hypothesis true repeating shuffling/permutation exercise performed Section 9.2.repeat resampling without replacement type = \"permute\" total reps = 1000 times. Feel free run code check generate() step produces.","code":"\nmovies_sample |> \n  specify(formula = rating ~ genre) |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  View()"},{"path":"hypothesis-testing.html","id":"calculate-summary-statistics-3","chapter":"9 Hypothesis Testing","heading":"4. calculate summary statistics","text":"Now 1000 replicated “shuffles” assuming null hypothesis \\(H_0\\) Action Romance movies average ratings IMDb, let’s calculate() appropriate summary statistic 1000 replicated shuffles. Section 9.3, summary statistics relating hypothesis testing specific name: test statistics. Since unknown population parameter interest difference population means \\(\\mu_{} - \\mu_{r}\\), test statistic interest difference sample means \\(\\overline{x}_{} - \\overline{x}_{r}\\).1000 shuffles, can calculate test statistic setting stat = \"diff means\". Furthermore, since interested \\(\\overline{x}_{} - \\overline{x}_{r}\\), set order = c(\"Action\", \"Romance\"). Let’s save results data frame called null_distribution_movies:Observe 1000 values stat, representing one instance \\(\\overline{x}_{} - \\overline{x}_{r}\\). 1000 values form null distribution, technical term sampling distribution difference sample means \\(\\overline{x}_{} - \\overline{x}_{r}\\) assuming \\(H_0\\) true. happened real life? observed difference popularity rates? observed test statistic \\(\\overline{x}_{} - \\overline{x}_{r}\\)? Recall earlier data wrangling, observed difference means \\(5.275 - 6.322 = -1.047\\). can also achieve using code constructed null distribution null_distribution_movies hypothesize() generate() steps removed. save obs_diff_means:","code":"\nnull_distribution_movies <- movies_sample |> \n  specify(formula = rating ~ genre) |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in means\", order = c(\"Action\", \"Romance\"))\nnull_distribution_movies# A tibble: 1,000 × 2\n   replicate      stat\n       <int>     <dbl>\n 1         1  0.511111\n 2         2  0.345833\n 3         3 -0.327083\n 4         4 -0.209028\n 5         5 -0.433333\n 6         6 -0.102778\n 7         7  0.387153\n 8         8  0.168750\n 9         9  0.257292\n10        10  0.334028\n# ℹ 990 more rows\nobs_diff_means <- movies_sample |> \n  specify(formula = rating ~ genre) |> \n  calculate(stat = \"diff in means\", order = c(\"Action\", \"Romance\"))\nobs_diff_meansResponse: rating (numeric)\nExplanatory: genre (factor)\n# A tibble: 1 × 1\n      stat\n     <dbl>\n1 -1.04722"},{"path":"hypothesis-testing.html","id":"visualize-the-p-value-1","chapter":"9 Hypothesis Testing","heading":"5. visualize the p-value","text":"Lastly, order compute \\(p\\)-value, assess “extreme” observed difference means -1.047 . comparing -1.047 null distribution, constructed hypothesized universe true difference movie ratings. visualize null distribution \\(p\\)-value Figure 9.15. Unlike example Subsection 9.4.1 involving music popularity, since two-sided \\(H_A: \\mu_a - \\mu_r \\neq 0\\), allow possibilities extreme, set direction = \"\".\nFIGURE 9.15: Null distribution, observed test statistic, \\(p\\)-value.\nLet’s go elements plot. First, histogram null distribution. Second, solid line observed test statistic, difference sample means observed real life \\(5.275 - 6.322 = -1.047\\). Third, two shaded areas histogram form \\(p\\)-value, probability obtaining test statistic just extreme observed test statistic assuming null hypothesis \\(H_0\\) true.proportion null distribution shaded? words, numerical value \\(p\\)-value? use get_p_value() function compute value:\\(p\\)-value 0.004 small. words, small chance observe difference 5.275 - 6.322 = -1.047 hypothesized universe truly difference ratings.\\(p\\)-value larger (even smaller) pre-specified \\(\\alpha\\) significance level 0.001. Thus, inclined fail reject null hypothesis \\(H_0: \\mu_a - \\mu_r = 0\\). non-statistical language, conclusion : evidence needed sample data suggest reject hypothesis difference mean IMDb ratings romance action movies. , thus, say difference exists romance action movie ratings, average, IMDb movies.\nLearning check\n(LC9.9) Conduct analysis comparing action movies versus romantic movies using median rating instead mean rating. different ?(LC9.10) conclusions can make viewing faceted histogram looking rating versus genre see looking boxplot?(LC9.11) Describe paragraph used Allen Downey’s diagram conclude statistical difference existed mean movie ratings action romance movies.(LC9.12) relatively confident distributions sample ratings good approximations population distributions ratings two genres?(LC9.13) Using definition \\(p\\)-value, write words \\(p\\)-value represents hypothesis test comparing mean rating romance action movies.(LC9.14) value \\(p\\)-value two-sided hypothesis test comparing mean rating romance action movies?(LC9.15) Test data-wrangling knowledge EDA skills:Use dplyr tidyr create necessary data frame focused action romance movies () movies data frame ggplot2movies package.Make boxplot faceted histogram population data comparing ratings action romance movies IMDb.Discuss plots compare similar plots produced movies_sample data.","code":"\nvisualize(null_distribution_movies, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_means, direction = \"both\")\nnull_distribution_movies |> \n  get_p_value(obs_stat = obs_diff_means, direction = \"both\")# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.004"},{"path":"hypothesis-testing.html","id":"nhst-conclusion","chapter":"9 Hypothesis Testing","heading":"9.7 Summary and Final Remarks","text":"","code":""},{"path":"hypothesis-testing.html","id":"theory-hypo","chapter":"9 Hypothesis Testing","heading":"9.7.1 Theory-based approach for two-sample hypothesis tests","text":"previously discussed theory-based approach confidence intervals hypothesis tests one-sample problem, discuss now theory needed perform two-sample hypothesis tests. present example traditional theory-based method conduct hypothesis tests. method relies Central Limit Theorem properties random variables, expected value, variance, standard deviation. also direct extension one-sample problem discussed Section 9.1.Theory-based methods used decades researchers access computers run thousands calculations quickly efficiently. Now computing power accessible simulation-based methods becoming popular, researchers many fields continue use theory-based methods.theory-based method focus known Welch’s two-sample \\(t\\)-test testing differences sample means. test statistic use two-sample \\(t\\)-statistic, standardized version difference sample means \\(\\overline{x}_1 - \\overline{x}_2\\). data use movies_sample data action romance movies Section 9.6.","code":""},{"path":"hypothesis-testing.html","id":"welchs-two-sample-t-statistic","chapter":"9 Hypothesis Testing","heading":"Welch’s two-sample t-statistic","text":"Section 7.5 introduced sampling distribution difference two sample means. let \\(\\overline X_a\\) random variable sample mean action films’ rating \\(\\overline X_r\\) one romance film’s rating, distribution difference random variables \\[\\overline X_a - \\overline X_r \\sim Normal \\left(\\mu_a - \\mu_r, \\sqrt{\\frac{\\sigma_a^2}{n_a} + \\frac{\\sigma^2_r}{n_r}} \\right)\\]\\(\\mu_a\\) \\(\\mu_r\\) population mean ratings, \\(\\sigma_a\\) \\(\\sigma_r\\) population standard deviations, \\(n_a\\) \\(n_r\\) sample sizes action romance genres, respectively.using samples, one-sample problems, standardize difference sample means, \\(\\overline{x}_a - \\overline{x}_r\\), subtracting mean (differences population means) dividing standard error. construct test statistic known Welch’s two-sample \\(t\\)-test statistic:\\[t = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - (\\mu_a - \\mu_r)}{ \\text{SE}(\\bar{x}_a - \\bar{x}_r) } = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - (\\mu_a - \\mu_r)}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}}  }\\]Observe formula \\(\\text{SE}(\\bar{x}_a - \\bar{x}_r)\\) sample sizes \\(n_a\\) \\(n_r\\) . sample sizes increase, standard error goes . seen characteristic one-sample problems Subsections 7.3.4 7.4.5 describing sample distribution sample mean sample proportion Section 8.1 discussing sample size effect confidence intervals.Let’s state null alternative hypotheses test:\\[\\begin{aligned}&H_0: &\\mu_a - \\mu_r = 0\\\\&H_A: &\\mu_a - \\mu_r \\ne 0\\end{aligned}\\]claim null hypothesis difference population means zero. equivalent claiming means , \\(\\mu_a = \\mu_r\\), typical test, try determine whether population means different. Yet, structure test allows testing differences well, needed.Welch’s two-sample \\(t\\)-test becomes:\\[t = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - 0}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}}  } = \\dfrac{ \\bar{x}_a - \\bar{x}_r}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}}  }\\]Using results based Central Limit Theorem, linear combinations independent random variables, properties expected value, variance, standard deviation, can shown Welch’s test statistic follows \\(t\\) distribution.Section 8.1.4 discussed properties \\(t\\) distribution Figure 8.7 shown different \\(t\\) distributions different degrees freedom. Recall \\(t\\) distribution similar standard normal; density curve also bell-shaped, symmetric around zero, tails \\(t\\) distribution little thicker (heavier) standard normal.\nimportant hypothesis testing, since \\(p\\)-value calculated areas tails.\nAlso recall degrees freedom increase, \\(t\\)-distribution approximates standard normal curve.terms Welch’s two-sample \\(t\\)-test, shown test statistics follows \\(t\\) distribution degrees freedom can approximated \\[\n\\widehat{df} = \\left( \\dfrac{s_a^2}{n_a} + \\dfrac{s_r^2}{n_r} \\right)^2 \\Bigg/ \\left( \\dfrac{\\left( \\dfrac{s_a^2}{n_a} \\right)^2}{n_a - 1} + \\dfrac{\\left( \\dfrac{s_r^2}{n_r} \\right)^2}{n_r - 1} \\right)\n\\]formula just long enter manually every time needed. (suitable approximation degrees freedom using \\(n_a + n_r - 2\\) often used instead reasonably large sample sizes.) , fortunately, R statistical software already done formula inputting us introducing relevant functions. important get good approximations degrees freedom order get appropriate \\(p\\)-values, learning formula goes beyond reach new statistical inference, little build intuition \\(t\\)-test. Therefore, trust results R statistical packages provide us.Let’s compute \\(t\\)-test statistic. Recall summary statistics computed exploratory data analysis Section 9.6.1.Using values, observed two-sample \\(t\\)-test statistic \\[\n\\dfrac{ \\bar{x}_a - \\bar{x}_r}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}}  } =\n\\dfrac{5.28 - 6.32}{ \\sqrt{\\dfrac{{1.36}^2}{32} + \\dfrac{{1.61}^2}{36}}  } =\n-2.906\n\\]can compute \\(p\\)-value using theory-based test statistic? calculating degrees freedom using function pt() earlier. Instead, use function t_test() package infer appropriate formula order, simulation-based approach.results shown next:Based \\(p\\)-value = 0.005 reject null hypothesis, average rating Romance movies average rating Action movies. result similar one calculated using simulation-based approach.able use Welch’s \\(t\\)-test, conditions necessary underlying mathematical theory holds:populations close normal samples large. Many textbooks suggest sample sizes greater 30, clear mathematical foundation rule thumb. general, long distribution samples appear close symmetric, Welch’s \\(t\\)-test may provide useful results.samples random samples.sample one population independent sample population.Let’s see conditions hold movies_sample data:met since \\(n_a\\) = 32 \\(n_r\\) = 36 seem highly skewed therefore somewhat symmetric.met since sampled action romance movies random unbiased fashion database IMDb movies.Unfortunately, know IMDb computes ratings. example, person can rate multiple movies, observations may related. appear major problem context though.Assuming three conditions clearly broken, can reasonably certain theory-based \\(t\\)-test results valid.","code":"\nmovies_sample |> \n  group_by(genre) |> \n  summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating))# A tibble: 2 × 4\n  genre       n mean_rating std_dev\n  <chr>   <int>       <dbl>   <dbl>\n1 Action     32     5.275   1.36121\n2 Romance    36     6.32222 1.60963\nmovies_sample |>\n  t_test(formula = rating ~ genre, \n         order = c(\"Action\", \"Romance\"), \n         alternative = \"two-sided\")# A tibble: 1 × 7\n  statistic    t_df    p_value alternative estimate lower_ci  upper_ci\n      <dbl>   <dbl>      <dbl> <chr>          <dbl>    <dbl>     <dbl>\n1  -2.90589 65.8496 0.00498319 two.sided   -1.04722 -1.76677 -0.327671"},{"path":"hypothesis-testing.html","id":"when-inference-is-not-needed","chapter":"9 Hypothesis Testing","heading":"9.7.2 When inference is not needed","text":"now walked several different examples use infer package perform statistical inference: constructing confidence intervals conducting hypothesis tests. examples, made point always perform exploratory data analysis (EDA) first; specifically, looking raw data values, using data visualization ggplot2, data wrangling dplyr beforehand. highly encourage always . beginner statistics, EDA helps develop intuition statistical methods like confidence intervals hypothesis tests can tell us. Even seasoned practitioner statistics, EDA helps guide statistical investigations. particular, statistical inference even needed?Let’s consider example. Say interested following question: flights leaving New York City airport, Hawaiian Airlines flights air longer Alaska Airlines flights? Furthermore, let’s assume 2023 flights representative sample flights. can use flights data frame nycflights23 package introduced Section 1.4 answer question. Let’s filter data frame include Hawaiian Alaska Airlines using carrier codes HA :two possible statistical inference methods use answer questions. First, construct 95% confidence interval difference population means \\(\\mu_{HA} - \\mu_{}\\), \\(\\mu_{HA}\\) mean air time Hawaiian Airlines flights \\(\\mu_{}\\) mean air time Alaska Airlines flights. check entirety interval greater 0, suggesting \\(\\mu_{HA} - \\mu_{} > 0\\), , words suggesting \\(\\mu_{HA} > \\mu_{}\\). Second, perform hypothesis test null hypothesis \\(H_0: \\mu_{HA} - \\mu_{} = 0\\) versus alternative hypothesis \\(H_A: \\mu_{HA} - \\mu_{} > 0\\).However, let’s first construct exploratory visualization suggested earlier. Since air_time numerical carrier categorical, boxplot can display relationship two variables, display Figure 9.16.\nFIGURE 9.16: Air time Hawaiian Alaska Airlines flights departing NYC 2023.\nlike call “PhD Statistics needed” moments. expert statistics know Alaska Airlines Hawaiian Airlines notably different air times. two boxplots even overlap! Constructing confidence interval conducting hypothesis test frankly provide much insight Figure 9.16.Let’s investigate observe clear-cut difference two airlines using data wrangling. Let’s first group rows flights_sample carrier also destination dest. Subsequently, compute two summary statistics: number observations using n() mean airtime:turns New York City 2023 Alaska flew seven different airports West Coast region US Hawaiian flew HNL (Honolulu) NYC. Given clear difference distance New York City West Coast versus New York City Honolulu, surprising observe different (statistically significantly different, fact) air times flights.clear example needing anything simple exploratory data analysis using data visualization descriptive statistics get appropriate conclusion. highly recommend perform EDA sample data running statistical inference methods like confidence intervals hypothesis tests.","code":"\nflights_sample <- flights |> \n  filter(carrier %in% c(\"HA\", \"AS\"))\nggplot(data = flights_sample, mapping = aes(x = carrier, y = air_time)) +\n  geom_boxplot() +\n  labs(x = \"Carrier\", y = \"Air Time\")\nflights_sample |> \n  group_by(carrier, dest) |> \n  summarize(n = n(), mean_time = mean(air_time, na.rm = TRUE), .groups = \"keep\")# A tibble: 8 × 4\n# Groups:   carrier, dest [8]\n  carrier dest      n mean_time\n  <chr>   <chr> <int>     <dbl>\n1 AS      LAS       1   299    \n2 AS      LAX     980   323.929\n3 AS      PDX     710   326.383\n4 AS      PSP      18   309.611\n5 AS      SAN    1034   325.457\n6 AS      SEA    2417   324.787\n7 AS      SFO    2683   343.542\n8 HA      HNL     366   623.287"},{"path":"hypothesis-testing.html","id":"problems-with-p-values","chapter":"9 Hypothesis Testing","heading":"9.7.3 Problems with p-values","text":"top many common misunderstandings hypothesis testing \\(p\\)-values listed Section 9.5, another unfortunate consequence expanded use \\(p\\)-values hypothesis testing phenomenon known “p-hacking.” p-hacking act “cherry-picking” results “statistically significant” dismissing , even expense scientific ideas. lots articles written recently misunderstandings problems \\(p\\)-values. encourage check :Misuse \\(p\\)-valuesWhat nerdy debate \\(p\\)-values shows science – fix itStatisticians issue warning misuse \\(P\\) valuesYou Trust Read NutritionA Litany Problems p-valuesSuch issues getting problematic American Statistical Association (ASA) put statement 2016 titled, “ASA Statement Statistical Significance \\(P\\)-Values,” six principles underlying proper use interpretation \\(p\\)-values. ASA released guidance \\(p\\)-values improve conduct interpretation quantitative science inform growing emphasis reproducibility science research.authors much prefer use confidence intervals statistical inference, since opinion much less prone large misinterpretation. However, many fields still exclusively use \\(p\\)-values statistical inference one reason including text. encourage learn “p-hacking” well implication science.","code":""},{"path":"hypothesis-testing.html","id":"additional-resources-7","chapter":"9 Hypothesis Testing","heading":"9.7.4 Additional resources","text":"R script file R code used chapter available .want examples infer workflow conducting hypothesis tests, suggest check infer package homepage, particular, series example analyses available https://infer.netlify.app/articles/.","code":""},{"path":"hypothesis-testing.html","id":"whats-to-come-8","chapter":"9 Hypothesis Testing","heading":"9.7.5 What’s to come","text":"conclude infer pipeline hypothesis testing Figure 9.17.\nFIGURE 9.17: infer package workflow hypothesis testing.\nNow armed understanding confidence intervals Chapter 8 hypothesis tests chapter, now study inference regression upcoming Chapter 10.","code":""},{"path":"inference-for-regression.html","id":"inference-for-regression","chapter":"10 Inference for Regression","heading":"10 Inference for Regression","text":"chapter, revisit regression model studied Chapters 5 6.\ntaking account inferential statistics methods introduced Chapters 8 9.\nshow applying linear regression methods introduced earlier sample data, can gain insight relationships response explanatory variables entire population.","code":""},{"path":"inference-for-regression.html","id":"inf-packages","chapter":"10 Inference for Regression","heading":"Needed packages","text":"needed, read Section 1.3 information install load R packages.Recall loading tidyverse package loads many packages encountered earlier. details refer Section 4.4. packages moderndive infer contain functions data frames used chapter.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)\nlibrary(gridExtra)\nlibrary(GGally)"},{"path":"inference-for-regression.html","id":"the-simple-linear-regression-model","chapter":"10 Inference for Regression","heading":"10.1 The simple linear regression model","text":"","code":""},{"path":"inference-for-regression.html","id":"un-member-states-revisited","chapter":"10 Inference for Regression","heading":"10.1.1 UN member states revisited","text":"briefly review example UN member states covered Section 5.1.\nData current UN member states, 2024, can found un_member_states_2024 data frame included moderndive package.\nSection 5.1, save data new data frame called UN_data_ch10, select() required variables, include rows without missing data using na.omit():show summary two numerical variables.\nObserve 183 observations without missing values.\nUsing simple linear regression response variable fertility rate (fert_rate) \\(y\\), regressor life expectancy (life_exp) \\(x\\), regression line :\\[\n\\widehat{y}_i = b_0 + b_1 \\cdot x_i.\n\\]presented equation Section 5.1, now add subscript \\(\\) represent \\(\\)th observation country UN dataset, let \\(= 1\\), \\(\\dots\\), \\(n\\) \\(n = 183\\) UN data.\nvalue \\(x_i\\) represents life expectancy value \\(\\)th member state, \\(\\widehat{y}_i\\) fitted fertility rate \\(\\)th member state.\nfitted fertility rate result regression line typically different observed response \\(y_i\\).\nresidual given difference \\(y_i - \\widehat{y}_i\\).discussed Subsection 5.3.2, intercept (\\(b_0\\)) slope (\\(b_1\\)) regression coefficients, regression line “best-fitting” line based least-squares criterion.\nwords, fitted values \\(\\widehat{y}\\) calculated using least-squares coefficients (\\(b_0\\) \\(b_1\\)) minimize sum squared residuals:\\[\n\\sum_{=1}^{n}(y_i - \\widehat{y}_i)^2\n\\]Section 5.1, fit linear regression model.\n“fit” mean calculate regression coefficients, \\(b_0\\) \\(b_1\\), minimize sum squared residuals.\nR, use lm() function formula fert_rate ~ life_exp save solution simple_model:regression line \\(\\widehat{y}_i = b_0 + b_1 \\cdot x_i = 12.613 - 0.137 \\cdot x_i\\), \\(x_i\\) life expectancy \\(\\)th country \\(\\widehat{y}_i\\) corresponding fitted fertility rate.\n\\(b_0\\) coefficient intercept meaning range values regressor, \\(x_i\\), includes zero.\nSince life expectancy always positive value, provide interpretation intercept example.\n\\(b_1\\) coefficient slope regression line; country, life expectancy increase one year, expect associated reduction fertility rate 0.137 units.visualize relationship data observed Figure 10.1 plotting scatterplot fertility rate life expectancy UN member states complete data.\nalso include regression line using least-squares criterion:\nFIGURE 10.1: Relationship regression line.\nFinally, review determine fitted values residuals observations dataset.\nFrance one UN member states, suppose want determine fitted fertility rate France based linear regression.\nstart determining location France UN_data_ch10 data frame, using rowid_to_column() filter() variable country equal “France.”\npull() function converts row number data frame single value:France 57th member state UN_data_ch10. observed fertility rate life expectancy :France’s life expectancy \\(x_{57} = 82.59\\) years fertility rate \\(y_{57} =1.8\\).\nUsing regression line earlier, can determine France’s fitted fertility rate:\\[\n\\begin{aligned}\n\\widehat{y}_{57} &= 12.613 - 0.137 \\cdot x_{57}\\\\\n&= 12.613 - 0.137 \\cdot 82.59\\\\\n&= 1.258.\n\\end{aligned}\n\\]Based regression line expect France’s fertility rate 1.258.\nobserved fertility rate France 1.8, residual France \\(y_{57} - \\widehat{y}_{57} = 1.8 - 1.258 = 0.542\\).Using R required manually calculate fitted values residual UN member state.\ndirectly using regression model simple_model get_regression_points() function.\nFrance, filter() 57th observation data frame.can retrieve information observation.\nshow first rows:concludes review material covered Section 5.1. now explain use information statistical inference.","code":"\nUN_data_ch10 <- un_member_states_2024 |>\n  select(country,\n         life_exp = life_expectancy_2022, \n         fert_rate = fertility_rate_2022)|>\n  na.omit()\nUN_data_ch10\nsimple_model <- lm(fert_rate ~ life_exp, data = UN_data_ch10)\ncoef(simple_model)\nggplot(UN_data_ch10, aes(x = life_exp, y = fert_rate)) +\n  geom_point() +\n  labs(x = \"Life Expectancy (x)\", \n       y = \"Fertility Rate (y)\",\n       title = \"Relationship between fertility rate and life expectancy\") +  \n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5)\nUN_data_ch10 |>\n  rowid_to_column() |>\n  filter(country == \"France\")|>\n  pull(rowid)[1] 57\nUN_data_ch10 |>\n  filter(country == \"France\")# A tibble: 1 × 3\n  country life_exp fert_rate\n  <chr>      <dbl>     <dbl>\n1 France     82.59       1.8\nsimple_model |>\n  get_regression_points() |>\n  filter(ID == 57)\nsimple_model |>\n  get_regression_points()# A tibble: 183 × 5\n      ID fert_rate life_exp fert_rate_hat residual\n   <int>     <dbl>    <dbl>         <dbl>    <dbl>\n 1     1       4.3    53.65         5.237   -0.937\n 2     2       1.4    79.47         1.687   -0.287\n 3     3       2.7    78.03         1.885    0.815\n 4     4       5      62.11         4.074    0.926\n 5     5       1.6    77.8          1.916   -0.316\n 6     6       1.9    78.31         1.846    0.054\n 7     7       1.6    76.13         2.146   -0.546\n 8     8       1.6    83.09         1.189    0.411\n 9     9       1.5    82.27         1.302    0.198\n10    10       1.6    74.15         2.418   -0.818\n# ℹ 173 more rows"},{"path":"inference-for-regression.html","id":"simple-linear-model","chapter":"10 Inference for Regression","heading":"10.1.2 The model","text":"Chapters 8 confidence intervals 9 hypothesis testing, present problem context population associated parameters interest.\ncollect random sample population use estimate parameters.assume population response variable (\\(Y\\)), explanatory variable (\\(X\\)), statistical linear relationship variables, given linear model\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon,\\]\\(\\beta_0\\) population intercept \\(\\beta_1\\) population slope.\nnow parameters model alongside explanatory variable (\\(X\\)) produce equation line.\nstatistical part relationship given \\(\\epsilon\\), random variable called error term.\nerror term accounts portion \\(Y\\) explained line.make additional assumptions distribution error term, \\(\\epsilon\\).\nassumed expected value error term zero, assumed standard deviation equal positive constant called \\(\\sigma\\), mathematical terms: \\(E(\\epsilon) = 0\\) \\(SD(\\epsilon) = \\sigma.\\)review meaning quantities.\ntake large number observations population, expect error terms sometimes greater zero sometimes less zero, average, equal zero.\nSimilarly, error terms close zero others far zero, average, expect roughly \\(\\sigma\\) units away zero.Recall square standard deviation called variance, \\(Var(\\epsilon) = \\sigma^2\\).\nvariance error term equal \\(\\sigma^2\\) regardless value \\(X\\).\nproperty called homoskedasticity constancy variance.\nuseful later analysis.","code":""},{"path":"inference-for-regression.html","id":"sample-regression-inference","chapter":"10 Inference for Regression","heading":"10.1.3 Using a sample for inference","text":"Chapters 8 9, use sample estimate parameters population.\nuse data collected Old Faithful Geyser Yellowstone National Park Wyoming, USA.\ndataset contains duration geyser eruption seconds waiting time next eruption minutes.\nduration current eruption can help determine fairly well waiting time next eruption.\nexample, use sample data collected volunteers saved website https://geysertimes.org/ June 1st, 2024 August 19th, 2024.data stored old_faithful_2024 data frame moderndive package.\ndata collected volunteers random sample, volunteers introduce sort bias, eruptions selected volunteers specific patterns.\n, beyond individual skill volunteer measuring times appropriately, response bias preference seems present.\nTherefore, seems safe consider data random sample. first ten rows shown :looking first row can tell, example, eruption August 19, 2024, 5:38 lasted 235 seconds, waiting time next eruption 180 minutes.\nnext display summary two variables:sample 114 eruptions, lasting 99 seconds 300 seconds, waiting time next eruption 102 minutes 201 minutes.\nObserve observation pair values, value explanatory variable (\\(X\\)) value response (\\(Y\\)). sample takes form:\\[\\begin{array}{c}\n(x_1,y_1)\\\\\n(x_2, y_2)\\\\\n\\vdots\\\\\n(x_n, y_n)\\\\\n\\end{array}\\], example, \\((x_2, y_2)\\) pair explanatory response values, respectively, second observation sample.\ngenerally, denote \\(\\)th pair \\((x_i, y_i)\\), \\(x_i\\) observed value explanatory variable \\(X\\) \\(y_i\\) observed value response variable \\(Y\\).\nSince sample \\(n\\) observations let \\(=1\\), \\(\\dots\\), \\(n\\).example \\(n = 114\\), \\((x_2, y_2) = (259, 184)\\).\nFigure 10.2 shows scatterplot entire sample transparency set check overplotting:\nFIGURE 10.2: Scatterplot relationship eruption duration waiting time.\nrelationship seems positive , extent, linear.","code":"\nold_faithful_2024# A tibble: 114 × 6\n   eruption_id date        time waiting webcam duration\n         <dbl> <date>     <dbl>   <dbl> <chr>     <dbl>\n 1     1473854 2024-08-19   538     180 Yes         235\n 2     1473352 2024-08-15  1541     184 Yes         259\n 3     1473337 2024-08-15  1425     116 Yes         137\n 4     1473334 2024-08-15  1237     188 Yes         222\n 5     1473331 2024-08-15  1131     106 Yes         105\n 6     1473328 2024-08-15   944     187 Yes         180\n 7     1473207 2024-08-14  1231     182 Yes         244\n 8     1473201 2024-08-14  1041     190 Yes         278\n 9     1473137 2024-08-13  1810     138 Yes         249\n10     1473108 2024-08-13  1624     186 Yes         262\n# ℹ 104 more rows\nold_faithful_2024 |>\n  select(duration, waiting) |> \n  tidy_summary()"},{"path":"inference-for-regression.html","id":"least-squares","chapter":"10 Inference for Regression","heading":"10.1.4 The method of least squares","text":"association variables linear approximately linear, can apply linear model described Subsection 10.1.2 observation sample:\\[\\begin{aligned}\ny_1 &= \\beta_0 + \\beta_1 \\cdot x_1 + \\epsilon_1\\\\\ny_2 &= \\beta_0 + \\beta_1 \\cdot x_2 + \\epsilon_2\\\\\n\\vdots & \\phantom{= \\beta_0 + \\beta_1 \\cdot + \\epsilon_2 +}\\vdots \\\\\ny_n &= \\beta_0 + \\beta_1 \\cdot x_n + \\epsilon_n\n\\end{aligned}\\]want able use model describe relationship explanatory variable response, parameters \\(\\beta_0\\) \\(\\beta_1\\) unknown us.\nestimate parameters using random sample applying least-squares method introduced Section 5.1.\ncompute estimators intercept (\\(\\beta_0\\)) slope (\\(\\beta_1\\)) minimize sum squared residuals:\\[\\sum_{=1}^n \\left[y_i - (\\beta_0 + \\beta_1 \\cdot x_i)\\right]^2.\\]optimization problem solve analytically require calculus topic goes beyond scope book.\nprovide sketch solution familiar method: using expression find partial derivative respect \\(\\beta_0\\) equate expression zero, partial derivative respect \\(\\beta_1\\) equate expression zero, use two equations solve \\(\\beta_0\\) \\(\\beta_1\\).\nsolutions regression coefficients introduced first Section 5.1: \\(b_0\\) estimator \\(\\beta_0\\) \\(b_1\\) estimator \\(\\beta_1\\).\ncalled least squares estimators mathematical expressions :\\[b_1 =  \\frac{\\sum_{=1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{=1}^n(x_i - \\bar x)^2} \\text{ } b_0 = \\bar y - b_1 \\cdot \\bar x.\\]Furthermore, estimator standard deviation \\(\\epsilon_i\\) given \\[s = \\sqrt{\\frac{\\sum_{=1}^n \\left[y_i - (b_0 + b_1 \\cdot x_i)\\right]^2}{n-2}} = \\sqrt{\\frac{\\sum_{=1}^n \\left(y_i - \\widehat{y}_i\\right)^2}{n-2}}.\\]equivalent calculations done R using lm() function.\nold_faithful_2024 get results shown Table\n10.1:\nTABLE 10.1: Old Faithful geyser linear regression coefficients\nBased data assuming linear model appropriate, can say every additional second eruption lasts, waiting time next eruption increases, average, 0.37 minutes.\neruption lasts longer zero seconds, intercept meaningful interpretation example.\nFinally, roughly expect waiting time next eruption 20.37 minutes away regression line value, average.","code":"\n# Fit regression model:\nmodel_1 <- lm(waiting ~ duration, data = old_faithful_2024)\n\n# Get the coefficients and standard deviation for the model\ncoef(model_1)\nsigma(model_1)"},{"path":"inference-for-regression.html","id":"properties-least-squares","chapter":"10 Inference for Regression","heading":"10.1.5 Properties of the least squares estimators","text":"least squares method produces best-fitting line selecting least squares estimators, \\(b_0\\) \\(b_1\\), make sum residual squares smallest possible.\nchoice \\(b_0\\) \\(b_1\\) depends sample observed.\nevery random sample taken data, different values \\(b_0\\) \\(b_1\\) determined.\nsense, least squares estimators, \\(b_0\\) \\(b_1\\), random variables , useful properties:\\(b_0\\) \\(b_1\\) unbiased estimators \\(\\beta_0\\) \\(\\beta_1\\), using mathematical notation: \\(E(b_0) = \\beta_0\\) \\(E(b_1) = \\beta_1\\).\nmeans , random samples, \\(b_1\\) greater \\(\\beta_1\\) others less \\(\\beta_1\\).\naverage, \\(b_1\\) equal \\(\\beta_1\\).\\(b_0\\) \\(b_1\\) linear combinations observed responses \\(y_1\\), \\(y_2\\), \\(\\dots\\), \\(y_n\\).\nmeans , example \\(b_1\\), known constants \\(c_1\\), \\(c_2\\), \\(\\dots\\), \\(c_n\\) \\(b_1 = \\sum_{=1}^n c_iy_i\\).\\(s^2\\) unbiased estimator variance \\(\\sigma^2\\).properties useful next subsection, perform theory-based inference regression.","code":""},{"path":"inference-for-regression.html","id":"relating-basic-regression-to-other-methods","chapter":"10 Inference for Regression","heading":"10.1.6 Relating basic regression to other methods","text":"wrap-section, ’ll investigating regression relates two different statistical techniques. One covered already book, difference sample means, new text related, ANOVA. ’ll see can represented regression framework. ","code":""},{"path":"inference-for-regression.html","id":"two-sample-difference-in-means","chapter":"10 Inference for Regression","heading":"Two-sample difference in means","text":"two-sample difference means common statistical technique used compare means two groups seen Section 9.6. often used determine significant difference mean response two groups, treatment group control group. two-sample difference means can represented regression framework using dummy variable represent two groups.Let’s consider movies_sample data frame moderndive package. ’ll compare average rating genres “Action” versus “Romance.” can use lm() function fit linear model dummy variable genre use get_regression_table():\nTABLE 10.2: Regression table two-sample difference means example\nNote Table 10.2 p_value genre: Romance row \\(p\\)-value hypothesis test \\[\nH_0: \\text{action romance mean rating}\n\\]\n\\[\nH_A: \\text{action romance different mean ratings}\n\\]\\(p\\)-value result matches closely found Section 9.6, using theory-based approach linear model. estimate genre: Romance row observed difference means “Action” “Romance” genres also saw Section 9.6, except sign switched since “Action” genre reference level.","code":"\nmod_diff_means <- lm(rating ~ genre, data = movies_sample)\nget_regression_table(mod_diff_means)"},{"path":"inference-for-regression.html","id":"anova","chapter":"10 Inference for Regression","heading":"ANOVA","text":"ANOVA, analysis variance, statistical technique used compare means three groups seeing statistically significant difference means multiple groups. ANOVA can represented regression framework using dummy variables represent groups. Let’s say wanted compare popularity (numeric) values spotify_by_genre data frame moderndive package across genres country, hip-hop, rock. use slice_sample() function narrowing selected columns filtered rows interest see rows data frame look like Table 10.3.\nTABLE 10.3: Five randomly selected rows spotify_for_anova\nfit linear model, let’s take look boxplot track_genre versus popularity Figure 10.3 see differences distributions three genres.\nFIGURE 10.3: Boxplot popularity genre.\ncan also compute mean popularity grouping track_genre:can use lm() function fit linear model dummy variables genres. ’ll use get_regression_table() function get regression table Table 10.4.\nTABLE 10.4: Regression table ANOVA example\nestimate track_genre: hip-hop track_genre: rock rows differences means “hip-hop” “country” genres “rock” “country” genres, respectively. “country” genre reference level. values match (rounding differences) shown mean_popularities_by_genre.p_value column corresponds hip-hop statistically higher mean popularity compared country value close 0 (reported 0). also gives us rock statistically significant \\(p\\)-value 0.153, make us inclined say rock significantly higher popularity compared country.traditional ANOVA doesn’t give level granularity. can performed using aov() function anova() function via pipe (|>):small \\(p\\)-value 2.2e-16 close 0, lead us reject null hypothesis mean popularities equal across three genres. consistent results found using linear model. traditional ANOVA results tell us means different though, linear model . ANOVA tells us difference exists means groups.\nLearning check\n(LC10.1) error term \\(\\epsilon\\) linear model \\(Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\) represent?. exact value response variable.B. predicted value response variable based model.C. part response variable explained line.D. slope linear relationship \\(X\\) \\(Y\\).(LC10.2) following property least squares estimators \\(b_0\\) \\(b_1\\)?. biased estimators population parameters \\(\\beta_0\\) \\(\\beta_1\\).B. linear combinations observed responses \\(y_1, y_2, \\ldots, y_n\\).C. always equal population parameters \\(\\beta_0\\) \\(\\beta_1\\).D. depend specific values explanatory variable \\(X\\) .(LC10.3) can difference means two groups represented linear regression model?. adding interaction term groups response variable.B. fitting separate regression lines group comparing slopes.C. including dummy variable represent groups.D. subtracting mean one group mean using difference predictor.","code":"\nspotify_for_anova <- spotify_by_genre |> \n  select(artists, track_name, popularity, track_genre) |> \n  filter(track_genre %in% c(\"country\", \"hip-hop\", \"rock\")) \nspotify_for_anova |> \n  slice_sample(n = 5)\nggplot(spotify_for_anova, aes(x = track_genre, y = popularity)) +\n  geom_boxplot() +\n  labs(x = \"Genre\", y = \"Popularity\")\nmean_popularities_by_genre <- spotify_for_anova |> \n  group_by(track_genre) |>\n  summarize(mean_popularity = mean(popularity))\nmean_popularities_by_genre# A tibble: 3 × 2\n  track_genre mean_popularity\n  <chr>                 <dbl>\n1 country              17.028\n2 hip-hop              37.759\n3 rock                 19.001\nmod_anova <- lm(popularity ~ track_genre, data = spotify_for_anova)\nget_regression_table(mod_anova)\naov(popularity ~ track_genre, data = spotify_for_anova) |> \n  anova()Analysis of Variance Table\n\nResponse: popularity\n              Df  Sum Sq Mean Sq F value              Pr(>F)    \ntrack_genre    2  261843  130922     137 <0.0000000000000002 ***\nResiduals   2997 2855039     953                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"inference-for-regression.html","id":"theory-simple-regression","chapter":"10 Inference for Regression","heading":"10.2 Theory-based inference for simple linear regression","text":"section introduces conceptual framework needed theory-based inference regression (see Subsections 10.2.1 10.2.2) discusses two prominent methods inference: confidence intervals (Subsection 10.2.3) hypothesis tests (Subsection 10.2.4).\nmaterial slightly technical sections chapter, material illustrated working real example interpretations explanations complement theory. Subsection 10.2.5 presents R code needed calculate relevant quantities inference. Feel free read section first.","code":""},{"path":"inference-for-regression.html","id":"framework-simple-lm","chapter":"10 Inference for Regression","heading":"10.2.1 Conceptual framework","text":"start reviewing assumptions linear model. continue using old_faithful_2024 illustrate framework. Recall random sample \\(n = 114\\) observations.\nSince assume linear relationship duration eruption waiting time next eruption, can express linear relationship \\(\\)th observation \\(y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\) \\(=1,\\dots,n\\). Observe \\(x_i\\) duration \\(\\)th eruption sample, \\(y_i\\) waiting time next eruption, \\(\\beta_0\\) \\(\\beta_1\\) population parameters considered constant.\nerror term \\(\\epsilon_i\\) random variable represents different observed response \\(y_i\\) expected response \\(\\beta_0 + \\beta_1 \\cdot x_i\\).can illustrate role error term using two observations old_faithful_2024 dataset.\nassume now linear model appropriate truly represents relationship duration waiting times.\nselect 49th 51st observations sample using function slice() corresponding rows:Observe duration time observations, response waiting time different.\nAssuming linear model appropriate, responses can expressed :\\[\\begin{aligned}\ny_{49} &= \\beta_0 + \\beta_1 \\cdot 236 + \\epsilon_{49}\\\\\ny_{51} &= \\beta_0 + \\beta_1 \\cdot 236 + \\epsilon_{51}\n\\end{aligned}\\]\\(y_{49} = 139\\) \\(y_{51} = 176\\).\ndifference responses due error term accounts variation response accounted linear model.linear model error term \\(\\epsilon_i\\) expected value \\(E(\\epsilon_i) = 0\\) standard deviation \\(SD(\\epsilon_i) = \\sigma\\).\nSince random sample taken, assume two error terms \\(\\epsilon_i\\) \\(\\epsilon_j\\) two different eruptions \\(\\) \\(j\\) independent.order perform theory-based inference require one additional assumption.\nlet error term normally distributed expected value (mean) equal zero standard deviation equal \\(\\sigma\\):\\[\\epsilon_i \\sim Normal(0, \\sigma).\\]population parameters \\(\\beta_0\\) \\(\\beta_1\\) constants.\nSimilarly, duration \\(\\)th eruption, \\(x_i\\), known also constant.\nTherefore, expression \\(\\beta_0 + \\beta_1 \\cdot x_i\\) constant. contrast, \\(\\epsilon_i\\) normally distributed random variable.response \\(y_i\\) (waiting time \\(\\)th eruption next) sum constant \\(\\beta_0 + \\beta_1 \\cdot x_i\\) normally distributed random variable \\(\\epsilon_i\\).\nBased properties random variables normal distribution, can state \\(y_i\\) also normally distribution random variable mean equal \\(\\beta_0 + \\beta_1 \\cdot x_i\\) standard deviation equal \\(\\sigma\\):\\[y_i \\sim Normal(\\beta_0 + \\beta_1 x_i\\,,\\, \\sigma)\\]\\(=1,\\dots,n\\).\nSince \\(\\epsilon_i\\) \\(\\epsilon_j\\) independent, \\(y_i\\) \\(y_j\\) also independent \\(\\ne j\\).addition, stated Subsection 10.1.5, least-squares estimator \\(b_1\\) linear combination random variables \\(y_1, \\dots, y_n\\).\n\\(b_1\\) also random variable!\nmean?\ncoefficient slope results particular sample \\(n\\) pairs duration waiting times.\ncollected different sample \\(n\\) pairs, coefficient slope likely different due sampling variation.Say hypothetically collect many random samples pairs duration waiting times, using least-squares method compute slope \\(b_1\\) samples.\nslopes form sampling distribution \\(b_1\\), discussed Subsection 7.3.4 context sample proportions.\nlearn , \\(y_1, \\dots, y_n\\) normally distributed \\(b_1\\) linear combination random variables, \\(b_1\\) also normally distributed.\ncalculations go beyond scope book take account properties expected value standard deviation responses \\(y_1, \\dots, y_n\\), can shown :\\[b_1 \\sim Normal \\left(\\beta_1\\,,\\, \\frac{\\sigma}{\\sqrt{\\sum_{=1}^n(x_i - \\bar x)^2}}\\right)\\], \\(b_1\\) normally distributed expected value \\(\\beta_1\\) standard deviation equal expression (inside parentheses comma).\nSimilarly, \\(b_0\\) linear combination \\(y_1, \\dots, y_n\\) using properties expected value standard deviation responses, get:\\[b_0 \\sim Normal \\left(\\beta_0\\,,\\, \\sigma\\sqrt{\\frac1n + \\frac{\\bar x^2}{\\sum_{=1}^n(x_i - \\bar x)^2}}\\right)\\]can also standardize least-square estimators \\[z_0 = \\frac{b_0 - \\beta_0}{\\left(\\sigma\\sqrt{\\frac1n + \\frac{\\bar x^2}{\\sum_{=1}^n(x_i - \\bar x)^2}}\\right)}\\qquad\\text{ }\\qquad z_1 = \\frac{b_1 - \\beta_1}{\\left(\\frac{\\sigma}{\\sqrt{\\sum_{=1}^n(x_i - \\bar x)^2}}\\right)}\\]corresponding standard normal distributions.","code":"\nold_faithful_2024 |>\n  slice(c(49, 51))# A tibble: 2 × 6\n  eruption_id date        time waiting webcam duration\n        <dbl> <date>     <dbl>   <dbl> <chr>     <dbl>\n1     1469584 2024-07-18  1117     139 Yes         236\n2     1469437 2024-07-17  1157     176 Yes         236"},{"path":"inference-for-regression.html","id":"se-simple-lm","chapter":"10 Inference for Regression","heading":"10.2.2 Standard errors for least-squares estimators","text":"Recall Chapter 7 Subsection 7.4.6 discussed , due Central Limit Theorem, distribution sample mean \\(\\overline X\\) approximately normal mean equal parameter \\(\\mu\\) standard deviation equal \\(\\sigma/\\sqrt n\\).\nused estimated standard error \\(\\overline X\\) construct confidence intervals hypothesis tests.analogous treatment now used construct confidence intervals hypothesis tests \\(b_0\\) \\(b_1\\).\nObserve equations standard deviations \\(b_0\\) \\(b_1\\) constructed using sample size \\(n\\), values explanatory variables, means, standard deviation \\(y_i\\) (\\(\\sigma\\)).\nvalues known us, \\(\\sigma\\) typically .Instead, estimate \\(\\sigma\\) using estimator standard deviation, \\(s\\), introduced Subsection 10.1.4.\nestimated standard deviation \\(b_1\\) called standard error \\(b_1\\), given :\\[SE(b_1) = \\frac{s}{\\sqrt{\\sum_{=1}^n(x_i - \\bar x)^2}}.\\]Recall standard error standard deviation point estimate computed sample.\nstandard error \\(b_1\\) quantifies much variation estimator slope \\(b_1\\) may different random samples.\nlarger standard error, variation expect estimated slope \\(b_1\\).\nSimilarly, standard error \\(b_0\\) :\\[SE(b_0) = s\\sqrt{\\frac1n + \\frac{\\bar x^2}{\\sum_{=1}^n(x_i - \\bar x)^2}}\\]discussed Subsection 8.1.4, using estimator \\(s\\) instead parameter \\(\\sigma\\), introducing additional uncertainty calculations.\nexample, can standardize \\(b_1\\) using\\[t = \\frac{b_1 - \\beta_1}{SE(b_1)}.\\]using \\(s\\) calculate \\(SE(b_1)\\), value standard error changes sample sample, additional uncertainty makes distribution test statistic \\(t\\) longer normal.\nInstead, follows \\(t\\)-distribution \\(n-2\\) degrees freedom.\nloss two degrees freedom relates fact trying estimate two parameters linear model: \\(\\beta_0\\) \\(\\beta_1\\). ready use information perform inference least-square estimators, \\(b_0\\) \\(b_1\\).","code":""},{"path":"inference-for-regression.html","id":"conf-intervals-b0-b1","chapter":"10 Inference for Regression","heading":"10.2.3 Confidence intervals for the least-squares estimators","text":"95% confidence interval \\(\\beta_1\\) can thought range plausible values population slope \\(\\beta_1\\) linear relationship duration waiting times.\ngeneral, sampling distribution estimator normal approximately normal, confidence interval relevant parameter \\[\n\\text{point estimate} \\pm \\text{margin error} = \\text{point estimate} \\pm (\\text{critical value} \\cdot \\text{standard error}).\n\\]formula 95% confidence interval \\(\\beta_1\\) given \\(b_1 \\pm q \\cdot SE(b_1)\\) critical value \\(q\\) determined level confidence required, sample size used, corresponding degrees freedom needed \\(t\\)-distribution.\nnow illustrate find 95% confidence interval slope Old Faithful example manually, show later directly R using function get_regression_table().\nFirst, observe \\(n = 114\\), degrees freedom \\(n-2 = 112\\). critical value 95% confidence interval \\(t\\)-distribution 112 degrees freedom \\(q = 1.981\\). Second, estimates \\(b_0\\), \\(b_1\\), \\(s\\) found earlier shown Table 10.5:\nTABLE 10.5: Old Faithful linear regression coefficients\nThird, standard error \\(b_1\\) using formula presented earlier :\\[SE(b_1) = \\frac{s}{\\sqrt{\\sum_{=1}^n(x_i - \\bar x)^2}} = \\frac{20.37}{627.583} = 0.032.\\]Finally, 95% confidence interval \\(\\beta_1\\) given :\\[\\begin{aligned}\nb_1 &\\pm q \\cdot SE(b_1)\\\\\n&= 0.371 \\pm 1.981\\cdot 0.032\\\\\n&= (0.308 , 0.434)\n\\end{aligned}\\]95% confident population slope \\(\\beta_1\\) number 0.308 0.434.construction 95% confidence interval \\(\\beta_0\\) follows exactly steps using \\(b_0\\), \\(SE(b_0)\\), critical value \\(q\\) degrees freedom \\(t\\)-distribution exactly , \\(n-2\\):\\[\\begin{aligned}\nb_0 &\\pm q \\cdot SE(b_0)\\\\\n&= 79.459 \\pm 1.981\\cdot 7.311\\\\\n&= (-14.112, 14.854)\n\\end{aligned}\\]results confidence intervals valid linear model assumptions satisfied.\ndiscuss assumptions Section 10.2.6.","code":""},{"path":"inference-for-regression.html","id":"hypo-test-simple-lm","chapter":"10 Inference for Regression","heading":"10.2.4 Hypothesis test for population slope","text":"perform hypothesis test \\(\\beta_1\\), general formulation two-sided test \\[\\begin{aligned}\nH_0: \\beta_1 = B\\\\\nH_A: \\beta_1 \\ne B\n\\end{aligned}\\]\\(B\\) hypothesized value \\(\\beta_1\\). Recall terminology, notation, definitions related hypothesis tests introduced Section 9.3.\nhypothesis test consists test two competing hypotheses: (1) null hypothesis \\(H_0\\) versus (2) alternative hypothesis \\(H_A\\).","code":""},{"path":"inference-for-regression.html","id":"t-test-stat-simple-lm","chapter":"10 Inference for Regression","heading":"Test statistic","text":"test statistic point estimator used hypothesis testing. , t-test statistic given \\[t = \\frac{b_1 - B}{SE(b_1)}.\\]test statistic follows, null hypothesis, \\(t\\)-distribution \\(n-2\\) degrees freedom.\nparticularly useful test whether linear association explanatory variable response, equivalent testing:\\[\\begin{aligned}\nH_0: \\beta_1 = 0\\\\\nH_1: \\beta_1 \\ne 0\n\\end{aligned}\\]example, may use test determine whether linear relationship duration Old Faithful geyser eruptions (duration) waiting time next eruption (waiting).\nnull hypothesis \\(H_0\\) assumes population slope \\(\\beta_1\\) 0.\ntrue, linear relationship duration waiting times.\nperforming hypothesis test, assume null hypothesis \\(H_0: \\beta_1 = 0\\) true try find evidence based data observed.alternative hypothesis \\(H_A\\), hand, states population slope \\(\\beta_1\\) 0, meaning longer eruption duration may result greater smaller waiting times next eruption.\nsuggests either positive negative linear relationship explanatory variable response.\nSince evidence null hypothesis may happen either direction context, call two-sided test.\nt-test statistic problem given :\\[t = \\frac{b_1 - 0}{SE(b_1)} = \\frac{0.371 - 0}{0.032} = 11.594\\]","code":""},{"path":"inference-for-regression.html","id":"the-p-value","chapter":"10 Inference for Regression","heading":"The p-value","text":"Recall terminology, notation, definitions related hypothesis tests introduced Section 9.3.\ndefinition \\(p\\)-value probability obtaining test statistic just extreme extreme one observed, assuming null hypothesis \\(H_0\\) true.\ncan intuitively think \\(p\\)-value quantifying “extreme” estimated slope (\\(b_1\\) = 0.371), assuming relationship duration waiting times.two-sided test, test statistic \\(t = 2\\) example, \\(p\\)-value calculated area \\(t\\)-curve left \\(-2\\) right \\(2\\) shown Figure 10.4.\nFIGURE 10.4: Illustration two-sided p-value t-test.\nOld Faithful geyser eruptions example, test statistic test \\(H_0: \\beta_1 = 0\\) \\(t = 11.594\\).\n\\(p\\)-value small R simply shows equal zero.","code":""},{"path":"inference-for-regression.html","id":"interpretation","chapter":"10 Inference for Regression","heading":"Interpretation","text":"Following hypothesis testing procedure outlined Section 9.5, since \\(p\\)-value practically 0, choice significance level \\(\\alpha\\), reject \\(H_0\\) favor \\(H_A\\).\nwords, assuming linear association duration waiting times, probability observing slope extreme one attained using random sample, practically zero.\nconclusion, reject null hypothesis linear relationship duration waiting times.\nenough statistical evidence conclude linear relationship variables.\nLearning check\n(LC10.4) context linear regression model, null hypothesis \\(H_0: \\beta_1 = 0\\) represent?. linear association response explanatory variable.B. difference observed predicted values zero.C. linear association response explanatory variable crosses origin.D. probability committing Type II Error zero.(LC10.5) following assumption linear regression model?. error terms \\(\\epsilon_i\\) normally distributed constant variance.B. error terms \\(\\epsilon_i\\) non-zero mean.C. error terms \\(\\epsilon_i\\) dependent .D. explanatory variable must normally distributed.(LC10.6) mean say slope estimator \\(b_1\\) random variable?. \\(b_1\\) every sample taken population.B. \\(b_1\\) fixed value change different samples.C. \\(b_1\\) can vary sample sample due sampling variation.D. \\(b_1\\) always equal population slope \\(\\beta_1\\).","code":""},{"path":"inference-for-regression.html","id":"regression-table","chapter":"10 Inference for Regression","heading":"10.2.5 The regression table in R","text":"least-square estimates, standard errors, test statistics, \\(p\\)-values, confidence interval bounds discussed Section 10.2 Subsections 10.2.1, 10.2.2, 10.2.3, 10.2.4 can calculated, , using R wrapper function get_regression_table() moderndive package.\nmodel_1, output presented Table 10.6.\nTABLE 10.6: regression table model\nNote first row Table 10.6 addresses inferences intercept \\(\\beta_0\\), second row deals inference slope \\(\\beta_1\\).\nheaders table present information found inference:estimate column contains least-squares estimates, \\(b_0\\) (first row) \\(b_1\\) (second row).std_error contains \\(SE(b_0)\\) \\(SE(b_1)\\) (standard errors \\(b_0\\) \\(b_1\\)), respectively.\ndefined standard errors Subsection 10.2.2.statistic column contains \\(t\\)-test statistic \\(b_0\\) (first row) \\(b_1\\) (second row).\nfocus \\(b_1\\), \\(t\\)-test statistic constructed using equation\\[\nt = \\frac{b_1 - 0}{SE(b_1)} = 11.594\n\\]corresponds hypotheses \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\ne 0\\).p_value probability obtaining test statistic just extreme extreme one observed, assuming null hypothesis true.\nhypothesis test, \\(t\\)-test statistic equal 11.594 , therefore, \\(p\\)-value near zero, suggesting rejection null hypothesis favor alternative.values lower_ci upper_ci lower upper bounds 95% confidence interval \\(\\beta_1\\).Please refer previous subsections conceptual framework detailed description quantities.","code":"\nget_regression_table(model_1)"},{"path":"inference-for-regression.html","id":"model-fit","chapter":"10 Inference for Regression","heading":"10.2.6 Model fit and model assumptions","text":"introduced linear model alongside assumptions many elements assumed along appropriate representation relationship response explanatory variable.\nreal-life applications, uncertain whether relationship appropriately described linear model whether assumptions introduced satisfied.course, expect linear model described chapter, model, perfect representation phenomenon presented nature.\nModels simplifications reality intend represent exactly relationship question rather provide useful approximations help improve understanding relationship.\nEven , want models simple possible still capture relevant features natural phenomenon studying.\napproach known principle parsimony Occam’s razor.even simple model like linear one, still want know accurately represents relationship data.\ncalled model fit.\naddition, want determine whether model assumptions met.four elements linear model want check.\nacrostic composition certain letters piece form word words.\nhelp remember four elements, can use following acrostic:Linearity relationship variables\nrelationship \\(y_i\\) \\(x_i\\) truly linear \\(= 1, \\dots, n\\)?\nwords, linear model \\(y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\) good fit?\nrelationship \\(y_i\\) \\(x_i\\) truly linear \\(= 1, \\dots, n\\)?\nwords, linear model \\(y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\) good fit?Independence response values \\(y_i\\)\n\\(y_i\\) \\(y_j\\) independent \\(\\ne j\\)?\n\\(y_i\\) \\(y_j\\) independent \\(\\ne j\\)?Normality error terms\ndistribution error terms least approximately normal?\ndistribution error terms least approximately normal?Equality constancy variance \\(y_i\\) (error term \\(\\epsilon_i\\))\nvariance, equivalently standard deviation, response \\(y_i\\) always , regardless fitted value (\\(\\widehat{y}_i\\)) regressor value (\\(x_i\\))?\nvariance, equivalently standard deviation, response \\(y_i\\) always , regardless fitted value (\\(\\widehat{y}_i\\)) regressor value (\\(x_i\\))?case, acrostic follows word LINE.\ncan serve nice reminder check using linear regression.\ncheck Linearity, Normality, Equal constant variance, use residuals linear regression via residual diagnostics explain next subsection.\ncheck Independence can use residuals data collected using time sequence type sequences.\nOtherwise, independence may achieved taking random sample, eliminates sequential type dependency.start reviewing residuals calculated, introduce residual diagnostics via visualizations, use example Old Faithful geyser eruptions determine whether four LINE elements met, discuss implications.","code":""},{"path":"inference-for-regression.html","id":"residuals","chapter":"10 Inference for Regression","heading":"Residuals","text":"Recall given random sample \\(n\\) pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) linear regression given :\\[\\widehat{y}_i = b_0 + b_1 \\cdot x_i\\]observations \\(= 1, dots,n\\).\nRecall residual defined Subsection 5.1.3, observed response minus fitted value.\ndenote residuals letter \\(e\\) get:\\[e_i = y_i - \\widehat{y}_i\\]\\(= 1, \\dots, n\\).\nCombining two formulas get\\[y_i = \\underline{\\widehat{y}_i} + e_i = \\underline{b_0 + b_1 \\cdot x_i} + e_i\\]resulting formula looks similar linear model:\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\]context, residuals can thought rough estimates error terms.\nSince many assumptions linear model related error terms, can check assumptions studying residuals.Figure 10.5, illustrate one particular residual Old Faithful geyser eruption duration time explanatory variable waiting time response.\nuse arrow connect observed waiting time (circle) fitted waiting time (square).\nvertical distance two points (equivalently, magnitude arrow) value residual observation.\nFIGURE 10.5: Example observed value, fitted value, residual.\ncan calculate \\(n = 114\\) residuals applying get_regression_points() function regression model model_1.\nObserve resulting values residual roughly equal waiting - waiting_hat (may slight difference due rounding error).","code":"\n# Fit regression model:\nmodel_1 <- lm(waiting ~ duration, data = old_faithful_2024)\n# Get regression points:\nfitted_and_residuals <- get_regression_points(model_1)\nfitted_and_residuals# A tibble: 114 × 5\n      ID waiting duration waiting_hat residual\n   <int>   <dbl>    <dbl>       <dbl>    <dbl>\n 1     1     180      235     166.666   13.334\n 2     2     184      259     175.572    8.428\n 3     3     116      137     130.299  -14.299\n 4     4     188      222     161.842   26.158\n 5     5     106      105     118.424  -12.424\n 6     6     187      180     146.256   40.744\n 7     7     182      244     170.006   11.994\n 8     8     190      278     182.623    7.377\n 9     9     138      249     171.861  -33.861\n10    10     186      262     176.686    9.314\n# ℹ 104 more rows"},{"path":"inference-for-regression.html","id":"residual-diagnostics","chapter":"10 Inference for Regression","heading":"Residual diagnostics","text":"Residual diagnostics used verify conditions L, N, E.\nsophisticated statistical approaches can used, focus data visualization. One useful plots residual plot, scatterplot residuals fitted values.\nuse fitted_and_residuals object draw scatterplot using geom_point() fitted values (waiting_hat) x-axis residuals (residual) y-axis.\naddition, add titles axes labs() draw horizontal line 0 reference using geom_hline() yintercept = 0, shown following code:Figure 10.6 show residual plot (right plot) alongside scatterplot duration vs waiting (left plot).\nNote residuals left plot determined vertical distance observed response linear regression.\nright plot (residuals), removed effect linear regression effect residuals seen vertical distance point zero line (y-axis).\nUsing residuals plot, easier spot patterns trends may conflict assumptions model, describe next.\nFIGURE 10.6: scatterplot residual plot Old Faithful data.\nfollows show residual plot can used determine whether linear model assumptions met.","code":"\nggplot(fitted_and_residuals, aes(x = waiting_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"duration\", y = \"residual\") +\n  geom_hline(yintercept = 0, col = \"blue\")"},{"path":"inference-for-regression.html","id":"linearity-of-relationship","chapter":"10 Inference for Regression","heading":"Linearity of relationship","text":"want check whether association response \\(y_i\\) explanatory variable \\(x_i\\) Linear.\nexpect, due error term model, scatterplot residuals fitted values shows random variation, variation systematic direction trend show non-linear patterns.scatterplot residuals fitted values showing patterns simply cloud points seems randomly assigned every direction residuals’ variation (y-axis) fitted values (x-axis) points located much zero line called null plot.\nPlots residuals fitted values regressors null plots show evidence assumptions model.\nwords, want linear model adequate, hope see null plots plotting residuals fitted values.largely case Old Faithful geyser example residuals fitted values (waiting_hat) shown right-plot Figure 10.6.\nresidual plot null plot appears clusters points opposed complete random assignment, clear systematic trends direction appearance non-linear relationship.\n, based plot, believe data violate assumption linearity.contrast, assume now scatterplot waiting duration associated residual plot shown Figure 10.7.\nusing real data , simulated data.\nquick look scatterplot regression line (left plot) lead us believe regression line appropriate summary relationship.\nlook carefully, may notice residuals low values duration mostly regression line, residuals values middle range duration mostly regression line, residuals large values duration regression line.reason prefer use plots residuals fitted values (right plot) removed effect regression can focus entirely residuals.\npoints clearly form line, rather U-shaped polynomial curve.\nreal data observed, using linear regression data produce results valid adequate.\nFIGURE 10.7: Example non-linear relationship.\n","code":""},{"path":"inference-for-regression.html","id":"independence-of-the-error-terms-and-the-response","chapter":"10 Inference for Regression","heading":"Independence of the error terms and the response","text":"Another assumption want check Independence response values.\nindependent, patterns dependency may appear observed data.residuals used purpose rough approximation error terms.\ndata collected time sequence type sequence, residuals may also help us determine lack independence plotting residuals time.\nhappens, Old Faithful geyser eruption example time component can use: old_faithful_2024 dataset contains date variable.\nshow plot residuals date (time) Figure 10.8:\nFIGURE 10.8: Scatterplot date (time) vs residuals Old Faithful example.\nplot residuals time (date) seems null plot.\nBased plot say residuals exhibit evidence dependency.Now, observations dataset subset Old Faithful geyser eruptions happen time frame eruptions happened sequentially, one next.\nobservation dataset represents unique eruption Old Faithful, waiting times duration recorded separately event.\nSince eruptions occur independently one another, residuals derived regression waiting versus duration also expected independent.\ndiscussed Subsection 10.1.3, can consider random sample.case, assumption independence seems acceptable.\nNote old_faithful_2024 data involve repeated measurements grouped observations lead dependency issues.\nTherefore, can proceed trusting regression analysis believe error terms systematically related one another.\ndetermining lack independence may easy certain settings, particular time sequence sequence measurements involved, taking random sample golden standard.","code":""},{"path":"inference-for-regression.html","id":"normality-of-the-error-terms","chapter":"10 Inference for Regression","heading":"Normality of the error terms","text":"third assumption want check whether error terms follow Normal distributions expected value equal zero.\nUsing residuals rough estimate error term values, seen right plot Figure 10.6 sometimes residuals positive times negative.\nwant see , average, errors equal zero shape distribution approximate bell shaped curve.can use histogram visualize distribution residuals:can also use quantile--quantile plot QQ-plot.\nQQ-plot creates scatterplot quantiles (percentiles) residuals quantiles normal distribution.\nresiduals follow approximately normal distribution, scatterplot straight line 45 degrees.\ndraw QQ-plot Old Faithful geyser eruptions example, use fitted_and_residuals data frame contains residuals regression, ggplot() aes(sample = residual) geom_qq() function drawing QQ-plot.\nalso include function geom_qq_line() add 45-degree line comparison:Figure 10.9 include histogram residuals including normal curve comparison (left plot) QQ-plot (right plot):\nFIGURE 10.9: Histogram residuals.\nhistogram residuals shown Figure 10.9 (left plot) appear exactly normal deviations, highest bin value appearing just right center.\nhistogram seem far normality either.\nQQ-plot (right plot) supports conclusion.\nscatterplot exactly 45-degree line deviate much either.compare results residuals found simulation appear follow normality shown Figure 10.10.\ncase model yielding clearly non-normal residuals right, results inference regression valid.\nFIGURE 10.10: Non-normal residuals.\n","code":"\nggplot(fitted_and_residuals, aes(residual)) +\n  geom_histogram(binwidth = 10, color = \"white\")\nfitted_and_residuals |>\n  ggplot(aes(sample = residual)) +\n  geom_qq() +\n  geom_qq_line()"},{"path":"inference-for-regression.html","id":"equality-or-constancy-of-variance-for-errors","chapter":"10 Inference for Regression","heading":"Equality or constancy of variance for errors","text":"final assumption check Equality constancy variance error term across fitted values regressor values.\nConstancy variance also known homoskedasticity. Using residuals rough estimates error terms, want check dispersion residuals fitted value \\(\\widehat{y}_i\\) regressor \\(x_i\\).\nFigure 10.6, showed scatterplot residuals fitted values (right plot).\ncan also produce scatterplot residuals regressor duration Figure 10.11:\nFIGURE 10.11: Plot residuals regressor.\nexception change scale x-axis, equivalent (visualization purposes) producing plot residuals (\\(e_i\\)) either fitted values (\\(\\widehat{y}_i\\)) regressor values (\\(x_i\\)).\nhappens fitted values linear transformation regressor values, \\(\\widehat{y}_i = b_0 + b_1\\cdot x_i\\).Observe vertical dispersion spread residuals different values duration:duration values 100 150 seconds, residual values somewhere -25 40, spread 65 units.duration values 150 200 seconds, handful observations clear spread .duration values 200 250 seconds, residual values somewhere -37 32, spread 69 units.duration values 250 300 seconds, residual values somewhere -42 27, spread 69 units.spread exactly constant across values duration.\nseems slightly higher greater values duration, seems larger number observations higher values duration well.\nObserve also two three cluster points dispersion residuals completely uniform.\nresidual plot exactly null plot, clear evidence assumption homoskedasticity.surprised see plots one dealing real data.\npossible residual plot exactly null plot, may information missing improve model.\nexample, include another regressor model.\nforget using linear model approximate relationship duration waiting times, expect model perfectly describe relationship.\nlook plots, trying find clear evidence data meeting assumptions used.\nexample appear violate constant variance assumption.Figure 10.12 present example using simulated data non-constant variance.\nFIGURE 10.12: Example clearly non-equal variance.\nObserve spread residuals increases regressor value increases.\nLack constant variance also known heteroskedasticity.\nheteroskedasticity present, results standard error least-square estimators, confidence intervals, conclusion related hypothesis test valid.","code":"\nggplot(fitted_and_residuals, aes(x = duration, y = residual)) +\n  geom_point(alpha = 0.6) +\n  labs(x = \"duration\", y = \"residual\") +\n  geom_hline(yintercept = 0)"},{"path":"inference-for-regression.html","id":"what-is-the-conclusion","chapter":"10 Inference for Regression","heading":"What is the conclusion?","text":"find conclusive evidence assumptions model:Linearity relationship variablesIndependence error termsNormality error termsEquality constancy varianceThis mean model perfectly adequate.\nexample, residual plot null plot clusters points explained model.\noverall, trends considered clear violations assumptions conclusions get model may valid.assumptions met?clear violations assumptions model, results found may suspect.\naddition, may remedial measures can taken improve model.\nNone measures addressed depth material extends beyond scope book, briefly discuss potential solutions future reference.Linearity relationship variables met, simple transformation regressor, response, variables may solve problem.\nexample transformation given Appendix online.\n, alternative methods spline regression, generalized linear models, non-linear models may used address situations.\nadditional regressors available, including regressors multiple linear regression may produce better results.Independence assumption met, dependency established variable within data hand, linear mixed-effects models can also used. models may also referred hierarchical multilevel models.Small departures Normality error terms assumption concerning results, including related confidence intervals hypothesis tests, may still valid.\nhand, number violations normality assumption large, many results may longer valid.\nUsing advanced methods suggested earlier may correct problems .Equality constancy variance met, adjusting variance adding weights individual observations may possible relevant information available makes weights known.\nmethod called weighted linear regression weighted least squares, direct extension model studied.\ninformation weights available, methods can used provide estimator internal structure variance model.\nOne popular methods called sandwich estimator.Checking assumptions model satisfied key component regression analysis.\nConstructing interpreting confidence intervals well conducting hypothesis tests providing conclusions results hypothesis tests directly affected whether assumptions satisfied.\ntime, often case regression analysis level subjectivity visualizing interpreting plots present, sometimes faced difficult statistical decisions.can done? suggest transparency clarity communicating results.\nimportant highlight important elements may suggest departures relevant assumptions, provide pertinent conclusions.\nway, stakeholders analysis aware model’s shortcomings can decide whether agree conclusions presented .\nLearning check\n(LC10.7) Use un_member_states_2024 data frame included moderndive package response variable fertility rate (fertility_rate_2022) regressor human development index (hdi_2022). Make sure omit missing values un_member_states_2024.Use get_regression_points() function get observed values, fitted values, residuals UN member countries.Perform residual analysis look systematic patterns residuals.\nIdeally, little pattern comment find .(LC10.8) context linear regression, p_value near zero slope coefficient suggests following?. intercept statistically significant 95% confidence level.B. strong evidence null hypothesis slope coefficient zero, suggesting exists linear relationship explanatory response variables.C. variance response variable significantly greater variance explanatory variable.D. residuals normally distributed mean zero constant variance.(LC10.9) Explain whether residual plot helps assess one following assumptions.Linearity relationship variablesIndependence error termsNormality error termsEquality constancy variance(LC10.10) residual plot fitted values shows “U-shaped” pattern, suggest?. variance residuals constant.B. linearity assumption violated.C. independence assumption violated.D. normality assumption satisfied.","code":""},{"path":"inference-for-regression.html","id":"infer-regression","chapter":"10 Inference for Regression","heading":"10.3 Simulation-based inference for simple linear regression","text":"section, ’ll use simulation-based methods previously learned Chapters 8 9 recreate values regression table.\nparticular, ’ll use infer package workflow toConstruct 95% confidence interval population slope \\(\\beta_1\\) using bootstrap resampling replacement. previously Sections 8.2.2 almonds data 8.4 mythbusters_yawn data.Conduct hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\) using permutation test. previously Sections 9.4 spotify_sample data 9.6 movies_sample IMDb data.","code":""},{"path":"inference-for-regression.html","id":"infer-ci-slr","chapter":"10 Inference for Regression","heading":"10.3.1 Confidence intervals for the population slope using infer","text":"’ll construct 95% confidence interval \\(\\beta_1\\) using infer workflow outlined Subsection 8.2.3. Specifically, ’ll first construct bootstrap distribution fitted slope \\(b_1\\) using single sample 114 eruptions:specify() variables interest old_faithful_2024 formula: waiting ~ duration.generate() replicates using bootstrap resampling replacement original sample 114 courses. generate reps = 1000 replicates using type = \"bootstrap\".calculate() summary statistic interest: fitted slope \\(b_1\\).Using bootstrap distribution, ’ll construct 95% confidence interval using percentile method (appropriate) standard error method well. important note case bootstrapping replacement done row--row. Thus, original pairs waiting duration values always kept together, different pairs waiting duration values may resampled multiple times. resulting confidence interval denote range plausible values unknown population slope \\(\\beta_1\\) quantifying relationship waiting times duration Old Faithful eruptions.Let’s first construct bootstrap distribution fitted slope \\(b_1\\):Observe 1000 values bootstrapped slope \\(b_1\\) stat column. Let’s visualize 1000 bootstrapped values Figure 10.13.\nFIGURE 10.13: Bootstrap distribution slope.\nObserve bootstrap distribution roughly bell-shaped. Recall Subsection 8.2.1 shape bootstrap distribution \\(b_1\\) closely approximates shape sampling distribution \\(b_1\\).","code":"\nbootstrap_distn_slope <- old_faithful_2024 |> \n  specify(formula = waiting ~ duration) |>\n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"slope\")\nbootstrap_distn_slopeResponse: waiting (numeric)\nExplanatory: duration (numeric)\n# A tibble: 1,000 × 2\n   replicate     stat\n       <int>    <dbl>\n 1         1 0.334197\n 2         2 0.331819\n 3         3 0.385334\n 4         4 0.380571\n 5         5 0.369226\n 6         6 0.370921\n 7         7 0.337145\n 8         8 0.417517\n 9         9 0.343136\n10        10 0.359239\n# ℹ 990 more rows\nvisualize(bootstrap_distn_slope)"},{"path":"inference-for-regression.html","id":"percentile-method","chapter":"10 Inference for Regression","heading":"Percentile-method","text":"First, let’s compute 95% confidence interval \\(\\beta_1\\) using percentile method. ’ll identifying 2.5th 97.5th percentiles include middle 95% values. Recall method require bootstrap distribution normally shaped.resulting percentile-based 95% confidence interval \\(\\beta_1\\) (0.309, 0.425).","code":"\npercentile_ci <- bootstrap_distn_slope |> \n  get_confidence_interval(type = \"percentile\", level = 0.95)\npercentile_ci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1 0.309088 0.425198"},{"path":"inference-for-regression.html","id":"standard-error-method","chapter":"10 Inference for Regression","heading":"Standard error method","text":"Since bootstrap distribution Figure 10.13 appears roughly bell-shaped, can also construct 95% confidence interval \\(\\beta_1\\) using standard error method.order , need first compute fitted slope \\(b_1\\), act center standard error-based confidence interval. saw regression table Table 10.6 \\(b_1\\) = 0.371, can also use infer pipeline generate() step removed calculate :use get_ci() function level = 0.95 compute 95% confidence interval \\(\\beta_1\\). Note setting point_estimate argument observed_slope 0.371 sets center confidence interval.resulting standard error-based 95% confidence interval \\(\\beta_1\\) \\((0.311, 0.431)\\) slightly different percentile-based confidence interval. Note neither confidence intervals contains 0 entirely located 0. suggesting fact meaningful positive relationship waiting times duration Old Faithful eruptions.","code":"\nobserved_slope <- old_faithful_2024 |> \n  specify(waiting ~ duration) |> \n  calculate(stat = \"slope\")\nobserved_slopeResponse: waiting (numeric)\nExplanatory: duration (numeric)\n# A tibble: 1 × 1\n      stat\n     <dbl>\n1 0.371095\nse_ci <- bootstrap_distn_slope |> \n  get_ci(level = 0.95, type = \"se\", point_estimate = observed_slope)\nse_ci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1 0.311278 0.430912"},{"path":"inference-for-regression.html","id":"infer-hypo-slr","chapter":"10 Inference for Regression","heading":"10.3.2 Hypothesis test for population slope using infer","text":"Let’s now conduct hypothesis test \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\). use infer package, follows hypothesis testing paradigm “one test” diagram Figure 9.11.Let’s first think means \\(\\beta_1\\) zero assumed null hypothesis \\(H_0\\). Recall said \\(\\beta_1 = 0\\), saying relationship waiting time duration. Thus, assuming particular null hypothesis \\(H_0\\) means “hypothesized universe” relationship waiting duration. can therefore shuffle/permute waiting variable consequence.construct null distribution fitted slope \\(b_1\\) performing steps follow. Recall Section 9.3 terminology, notation, definitions related hypothesis testing defined null distribution: sampling distribution test statistic \\(b_1\\) assuming null hypothesis \\(H_0\\) true.specify() variables interest old_faithful_2024 formula: waiting ~ duration.hypothesize() null hypothesis independence. Recall Section 9.4 additional step needs added hypothesis testing.generate() replicates permuting/shuffling values original sample 114 eruptions. generate reps = 1000 replicates using type = \"permute\" .calculate() test statistic interest: fitted slope \\(b_1\\).case, permute values waiting across values duration 1000 times. can shuffling/permuting since assumed “hypothesized universe” relationship two variables. calculate \"slope\" coefficient 1000 generated samples.Observe resulting null distribution fitted slope \\(b_1\\) Figure 10.14.\nFIGURE 10.14: Null distribution slopes.\nNotice centered \\(b_1\\) = 0. hypothesized universe, relationship waiting duration \\(\\beta_1 = 0\\). Thus, typical fitted slope \\(b_1\\) observe across simulations 0. Observe, furthermore, variation around central value 0.Let’s visualize \\(p\\)-value null distribution comparing observed test statistic \\(b_1\\) = 0.371 Figure 10.15. ’ll adding shade_p_value() layer previous visualize() code.\nFIGURE 10.15: Null distribution \\(p\\)-value.\nSince observed fitted slope 0.371 falls far right null distribution thus shaded region doesn’t overlap , ’ll \\(p\\)-value 0. completeness, however, let’s compute numerical value \\(p\\)-value anyways using get_p_value() function. Recall takes inputs shade_p_value() function:matches \\(p\\)-value 0 regression table. therefore reject null hypothesis \\(H_0: \\beta_1 = 0\\) favor alternative hypothesis \\(H_A: \\beta_1 \\neq 0\\). thus evidence suggests significant relationship waiting time duration values eruptions Old Faithful.conditions inference regression met null distribution bell shape, likely see similar results simulation-based results just demonstrated theory-based results shown regression table.\nLearning check\n(LC10.11) Repeat inference time correlation coefficient instead slope. Note implementation stat = \"correlation\" calculate() function infer package.(LC10.12) appropriate use bootstrap percentile method construct 95% confidence interval population slope \\(\\beta_1\\) Old Faithful data?. assumes slope follows perfect normal distribution.B. relies resampling residuals instead original data points.C. requires original data uniformly distributed.D. require bootstrap distribution normally shaped.(LC10.13) role permutation test hypothesis testing population slope \\(\\beta_1\\)?. generates new samples confirm confidence interval boundaries.B. assesses whether observed slope occurred chance null hypothesis relationship.C. adjusts sample size reduce sampling variability.D. ensures residuals regression model normally distributed.(LC10.14) generating null distribution slope using infer, find \\(p\\)-value near 0. indicate relationship waiting duration Old Faithful data?. evidence relationship waiting duration.B. observed slope likely due random variation null hypothesis.C. observed slope significantly different zero, suggesting meaningful relationship waiting duration.D. null hypothesis rejected \\(p\\)-value small.","code":"\nnull_distn_slope <- old_faithful_2024 |> \n  specify(waiting ~ duration) |>\n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"slope\")\n# Observed slope\nb1 <- old_faithful_2024 |> \n  specify(waiting ~ duration) |>\n  calculate(stat = \"slope\")\nb1Response: waiting (numeric)\nExplanatory: duration (numeric)\n# A tibble: 1 × 1\n      stat\n     <dbl>\n1 0.371095\nnull_distn_slope |> \n  get_p_value(obs_stat = b1, direction = \"both\")# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0"},{"path":"inference-for-regression.html","id":"the-multiple-linear-regression-model","chapter":"10 Inference for Regression","heading":"10.4 The multiple linear regression model","text":"","code":""},{"path":"inference-for-regression.html","id":"multiple-linear-model","chapter":"10 Inference for Regression","heading":"10.4.1 The model","text":"extension simple multiple regression model discussed next.\nassume population response variable (\\(Y\\)) two explanatory variables (\\(X_1, X_2, \\dots, X_p\\)) \\(p \\ge 2\\).\nstatistical linear relationship variables given \\[Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\dots + \\beta_p X_p + \\epsilon\\] \\(\\beta_0\\) population intercept \\(\\beta_j\\) population partial slope related regressor \\(X_j\\).\nerror term \\(\\epsilon\\) accounts portion \\(Y\\) explained line.\nsimple case, assume expected value \\(E(\\epsilon) = 0\\), standard deviation \\(SD(\\epsilon) = \\sigma\\), variance \\(Var(\\epsilon) = \\sigma^2\\).\nvariance standard deviation constant regardless value \\(X_1, X_2, \\dots, X_p\\).\ntake large number observations population, expect error terms sometimes greater zero times less zero, average equal zero, give take \\(\\sigma\\) units away zero.","code":""},{"path":"inference-for-regression.html","id":"example-coffee-quality-rating-scores","chapter":"10 Inference for Regression","heading":"10.4.2 Example: coffee quality rating scores","text":"case simple linear regression use random sample estimate parameters population.\nillustrate methods, use coffee_quality data frame moderndive package.\ndataset Coffee Quality Institute contains information coffee rating scores based ten different attributes: aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness, overall.\naddition, data frame contains information moisture_percentage coffee’s country continent_of_origin.\ncan assume random sample.plan regress total_cup_points (response variable) numerical explanatory variables aroma, flavor, moisture_percentage categorical explanatory variable four categories; Africa, Asia, North America, South America.\nproceeding, construct new data frame called coffee_data keeping variables interest.\naddition, variable continent_of_origin read R type character, want make type factor.\nusing dplyr verbs including command .factor() inside mutate() make continent_of_origin factor ordering \"Africa\", \"Asia\", \"North America\", \"South America\".first ten rows coffee_data shown :looking fourth row can tell, example, total_cup_points 87.17 aroma score equal 8.08 points, flavor score equal 8.17 points, moisture_percentage equal 11.8%, North America country_of_origin.\nalso display summary variables Table 10.7:\nTABLE 10.7: Summary coffee data\nObserve sample 207 observations, total_cup_points ranges 6.5 8.58, average aroma score 7.745, median flavor score 10.8. Note observation composed \\(p+1\\) values: values explanatory variables \\((X_1, \\dots, X_p)\\) value response (\\(Y\\)).\nsample takes form:\\[\\begin{array}{c}\n(x_{11}, x_{12},\\dots, x_{1p}, y_1)\\\\\n(x_{21}, x_{22},\\dots, x_{2p}, y_2)\\\\\n\\vdots\\\\\n(x_{n1}, x_{n2},\\dots, x_{np}, y_n)\\\\\n\\end{array}\\]\\((x_{i1}, x_{i2},\\dots, x_{ip}, y_i)\\) values \\(\\)th observation sample \\(=1\\), \\(\\dots\\), \\(n\\).\nUsing notation, example, \\(x_{i2}\\) value \\(\\)th observation data second explanatory variable \\(X_2\\). coffee example \\(n = 207\\) value second explanatory variable (flavor) 4th observation \\(x_{42} = 11.8\\).can now create data visualizations.\nperforming multiple regression useful construct scatterplot matrix, matrix contains scatterplots variable pair combinations.\nR, can use function ggpairs() package GGally generate scatterplot matrix useful additional information. Figure 10.16, present scatterplot matrix variables interest.\nFIGURE 10.16: Scatterplot matrix coffee variables interest.\nfirst comment plots response (total_cup_points) vertical axis.\nlocated last row plots figure.\nplotting total_cup_points aroma (bottom row, leftmost plot) total_cup_points flavor (bottom row, second plot left), observe strong positive linear relationship.\nplot total_cup_points moisture_percentage (bottom row, third plot left) provide much information appears variables associated way, observe outlying observation moisture_percentage around zero.\nplot total_cup_points versus continent_of_origin (bottom row, fourth plot left) shows four histograms, one factor level; fourth group seems larger dispersion, even though number observations seems smaller.\nassociated boxplots connecting two variables (fourth row, rightmost plot) suggest first level factor (\"Africa\") higher mean cup points three.\noften useful also find linear associations numerical regressors case scatterplot aroma flavor suggests strong positive linear association.\ncontrast, almost relationship can found observing moisture_percentage either aroma flavor.function ggpairs() produces plots, includes correlation coefficient pair numerical variables.\nexample, correlation coefficients support findings using scatterplot matrix.\ncorrelations total_cup_points aroma (0.87) total_cup_points flavor (0.94) positive close one, suggesting strong positive linear association.\nRecall correlation coefficient relevant association approximately linear.\ncorrelation moisture_percentage variable close zero, suggesting moisture_percentage likely linearly associated variable (either response regressor).\nNote also correlation aroma flavor (0.82) supports conclusion strong positive association.","code":"\ncoffee_data <- coffee_quality |>\n  select(aroma, \n         flavor, \n         moisture_percentage, \n         continent_of_origin, \n         total_cup_points) |>\n  mutate(continent_of_origin = as.factor(continent_of_origin))\ncoffee_data# A tibble: 207 × 5\n   aroma flavor moisture_percentage continent_of_origin total_cup_points\n   <dbl>  <dbl>               <dbl> <fct>                          <dbl>\n 1  8.58   8.5                 11.8 South America                  89.33\n 2  8.5    8.5                 10.5 Asia                           87.58\n 3  8.33   8.42                10.4 Asia                           87.42\n 4  8.08   8.17                11.8 North America                  87.17\n 5  8.33   8.33                11.6 South America                  87.08\n 6  8.33   8.33                10.7 North America                  87   \n 7  8.33   8.17                 9.1 Asia                           86.92\n 8  8.25   8.25                10   Asia                           86.75\n 9  8.08   8.08                10.8 Asia                           86.67\n10  8.08   8.17                11   Africa                         86.5 \n# ℹ 197 more rows\ncoffee_data |>\n  tidy_summary()\ncoffee_data |>\n  ggpairs()"},{"path":"inference-for-regression.html","id":"least-squares-multiple","chapter":"10 Inference for Regression","heading":"10.4.3 Least squares for multiple regression","text":"Observe three numerical regressors one factor (continent_of_origin) four levels: \"Africa\", \"Asia\", \"North America\", \"South America\".\nintroduce factor levels linear model, represent factor levels using dummy variables described Subsection 6.1.2.\ndummy variables need:\\[\\begin{aligned}\nD_1 &= \\left\\{\n\\begin{array}{ll}\n1 & \\text{continent origin Africa} \\phantom{afdasfd} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\\\\\nD_2 &= \\left\\{\n\\begin{array}{ll}\n1 & \\text{continent origin Asia}\\phantom{asdfasdfa} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\\\\\nD_3 &= \\left\\{\n\\begin{array}{ll}\n1 & \\text{continent origin North America}\\phantom{} \\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\\\\\nD_4 &= \\left\\{\n\\begin{array}{ll}\n1 & \\text{continent origin South America} \\phantom{}\\\\\n0 & \\text{otherwise}\\end{array}\n\\right.\\\\\n\\end{aligned}\\]Recall also drop first level level accounted intercept model.\nsimple linear case, assume linearity response regressors holds apply linear model described Subsection 10.4.1 observation sample.\nexpress model terms \\(\\)th observation, get\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\beta_{04}D_{i4} + \\epsilon_i\n\\], \\(\\)th observation sample, \\(x_{i1}\\) represents aroma score, \\(x_{i2}\\) flavor score, \\(x_{i3}\\) moisture_percentage, \\(D_{i2}\\) dummy variable Asia, \\(D_{i3}\\) dummy variable North America, \\(D_{i4}\\) dummy variable South America.\nRecall \\(\\) subscript represents one observation sample.\nAlternatively, present model observations:\\[\\begin{aligned}\ny_1\n&=\n\\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\beta_3 x_{13}+ \\beta_{02}D_{12} + \\beta_{03}D_{13} + \\beta_{04}D_{14} + \\epsilon_1\\\\\ny_2\n&=\n\\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\beta_3 x_{23}+ \\beta_{02}D_{22} + \\beta_{03}D_{23} + \\beta_{04}D_{24} + \\epsilon_2\\\\\n& \\phantom{  }\\vdots \\\\\ny_n\n&=\n\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\beta_3 x_{n3}+ \\beta_{02}D_{n2} + \\beta_{03}D_{n3} + \\beta_{04}D_{n4} + \\epsilon_n\n\\end{aligned}\\]extension least-squares method applied multiple regression follows.\nwant retrieve coefficient estimators minimize sum squared residuals:\\[\\sum_{=1}^n \\left[y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3}+ \\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\beta_{04}D_{i4} )\\right]^2.\\]optimization problem similar simple linear case, solved using calculus.\nequations deal now; seven equations coffee example, connected coefficient estimators need estimated.\nsolutions problem reached using matrices matrix calculus, regression coefficients introduced Chapter 6 Subsection 6.1.2.\ncalled least-squares estimators: \\(b_0\\) least-square estimator \\(\\beta_0\\), \\(b_1\\) least-square estimator \\(\\beta_1\\), etc.fitted values, residuals, estimator variance (\\(s^2\\)), standard deviation (\\(s\\)), direct extensions simple linear case. general case, \\(p\\) regressors, fitted values \\[\\widehat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \\dots + b_p x_{ip},\\]residuals \\(e_i = y_i - \\widehat{y}_i\\), model variance estimator \\[\ns^2 = \\frac{\\sum_{=1}^n \\left(y_i - \\widehat{y}_i\\right)^2}{n-p}\n\\]\\(p\\) number coefficients. applying formulas coffee scores example, fitted values \\[\\widehat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3}+ b_{02}D_{i2} + b_{03}D_{i3} + b_{04}D_{i4}\\,\\,,\\]variance estimator \\[\\begin{aligned}\ns^2 &= \\frac{\\sum_{=1}^n \\left[y_i - (b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3}+ b_{02}D_{i2} + b_{03}D_{i3} + b_{04}D_{i4} )\\right]^2}{n-7}\\\\\n&= \\frac{\\sum_{=1}^n \\left(y_i - \\widehat{y}_i\\right)^2}{n-7},\n\\end{aligned}\\]standard deviation estimator \\[\ns = \\sqrt{\\frac{\\sum_{=1}^n \\left(y_i - \\widehat{y}_i\\right)^2}{n-7}}.\n\\]think least-square estimators random variables depend random sample taken, properties estimators direct extension properties presented simple linear case:least-square estimators unbiased estimators parameters model.\nexample, choose \\(\\beta_1\\) (estimator partial slope aroma), expected value equal parameter, \\(E(b_1) = \\beta_1\\).\nmeans random samples estimated value \\(b_1\\) greater \\(\\beta_1\\), others less \\(\\beta_1\\); , average, \\(b_1\\) equal \\(\\beta_1\\).least-square estimators linear combinations observed responses \\(y_1\\), \\(y_2\\), \\(\\dots\\), \\(y_n\\).\n\\(b_3\\), example, known constants \\(c_1\\), \\(c_2\\), \\(\\dots\\), \\(c_n\\) \\(b_1 = \\sum_{=1}^n c_iy_i\\).using lm() function, necessary calculations done R. Table 10.8, coffee example, get:\nTABLE 10.8: Coffee example linear regression coefficients\nlinear model appropriate, can interpret coefficients Subsections 6.1.2 6.2.2.\nRecall numerical regressors’ coefficients (\\(b_1\\), \\(b_2\\), \\(b_3\\)) partial slopes, representing extra effect (additional effect) increasing corresponding regressor one unit keeping regressors fixed value.\nexample, using mod_mult, given observation increase flavor score one unit, keeping regressors fixed level, total_cup_points increase 4.32 units, average.\ninterpretation valid linear regression model mod_mult.\ndecide change regressors used, add remove others, model changes partial slope flavor different magnitude meaning; forget partial slope additional contribution flavor added model includes regressors particular model.addition, observe mod_mult model without interactions, similar model described Subsection 6.1.3. coefficients factor levels continent_of_origin (\\(b_{02}\\), \\(b_{03}\\), \\(b_{04}\\)) affect intercept model, based category observation question.\nRecall factor levels, order, Africa, Asia, North America, South America.\nexample, fifth observation’s continent origin South America (\\(D_{04} = 1\\) \\(D_{02} = D_{03} = 0\\)), regression formula given \\[\\begin{aligned}\n\\widehat{y}_5 &= b_0 + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}+ b_{02}D_{52} + b_{03}D_{53} + b_{04}D_{54}\\\\\n&= b_0 + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}+ b_{02}\\cdot 0 + b_{03}\\cdot 0 + b_{04}\\cdot 1\\\\\n&= (b_0 + b_{04}) + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}\\\\\n\\end{aligned}\\]regression intercept observation estimated \\[b_0 + b_{04} = 37.32 + (-0.48) = 36.84.\\]typically expect scores regressors zero, can always check range values regressors take.\nmod_mult range regressors can extracted using tidy_summary().\nfollowing code, using coffee_data dataset, select numerical regressors, use tidy_summary(), select column name, min, max get range regressorsAs see, moisture_percentage includes zero range, need numerical regressors include zero order intercept special meaning context problem.Observe decided use subset regressors construct model without interactions.\ndiscuss Subsection 10.5.5 determine best subset regressors use many available.\nnow ready discuss inference multiple linear regression.\nLearning check\n(LC10.15) multiple linear regression model, coefficient \\(\\beta_j\\) represent?. intercept model.B. standard error estimate.C. total variance explained model.D. partial slope related regressor \\(X_j\\), accounting regressors.(LC10.16) necessary convert continent_of_origin factor preparing coffee_data data frame regression analysis?. allow regression model interpret continent_of_origin numerical variable.B. create dummy variables represent different categories continent_of_origin.C. reduce number observations dataset.D. ensure variable included correlation matrix.(LC10.17) purpose creating scatterplot matrix context multiple linear regression?. identify outliers need removed dataset.B. test normality residuals.C. examine linear relationships variable pairs identify multicollinearity among regressors.D. determine appropriate number dummy variables.(LC10.18) multiple regression model coffee_data, role dummy variables continent_of_origin?. used predict values numerical regressors.B. modify intercept based specific category continent_of_origin.C. serve test independence residuals.D. indicate observations excluded model.","code":"\n# Fit regression model:\nmod_mult <- lm(\n  total_cup_points ~ aroma + flavor + moisture_percentage + continent_of_origin, \n  data = coffee_data\n)\n\n# Get the coefficients of the model\ncoef(mod_mult)\n\n# Get the standard deviation of the model\nsigma(mod_mult)\ncoffee_data |>\n  select(aroma, flavor, moisture_percentage) |>\n  tidy_summary() |>\n  select(column, min, max)"},{"path":"inference-for-regression.html","id":"theory-multiple-regression","chapter":"10 Inference for Regression","heading":"10.5 Theory-based inference for multiple linear regression","text":"section, introduce conceptual framework needed understand inference multiple linear regression.\nillustrate framework using coffee example R function get_regression_table() introduced Subsection 10.2.5.Inference multiple linear regression natural extension inference simple linear regression. Recall linear model, \\(\\)th observation given \\[y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\\,.\\]\nassume error term normally distributed expected value (mean) equal zero standard deviation equal \\(\\sigma\\):\\[\\epsilon_i \\sim Normal(0, \\sigma).\\]\nSince error term random element linear model, response \\(y_i\\) results sum constant\\[\\beta_0 + \\beta_1 \\cdot x_{i1} + \\dots + \\beta_p x_{ip}\\]random variable: error term \\(\\epsilon_i\\).\nUsing properties normal distribution, expected value, variance (standard deviation), can shown \\(y_i\\) also normally distributed mean equal \\(\\beta_0 + \\beta_1 \\cdot x_{i1} + \\dots + \\beta_p x_{ip}\\) standard deviation equal \\(\\sigma\\):\\[y_i \\sim Normal(\\beta_0 + \\beta_1 \\cdot x_{i1} + \\dots + \\beta_p x_{ip}\\,,\\, \\sigma)\\]\\(=1,\\dots,n\\).\nalso assume \\(\\epsilon_i\\) \\(\\epsilon_j\\) independent, \\(y_i\\) \\(y_j\\) also independent \\(\\ne j\\).\nMoreover, least-squares estimators (\\(b_0, b_1, \\dots, b_p\\)) linear combinations random variables \\(y_1, \\dots, y_n\\) normally distributed, shown .\n, following properties normal distribution, expected value, variance can shown thatthe (least-square) estimators follow normal distribution,estimators unbiased, meaning expected value estimator parameter estimating; example, \\(E(b_1) = \\beta_1\\), general \\(E(b_j) = \\beta_j\\) \\(j = 0,1,\\dots, p\\).variance standard deviation estimator (\\(b_j\\)), function \\(\\sigma\\), observed data explanatory variable (values regressors sample).\nsimplicity, standard deviation estimator \\(b_j\\) denoted \\(SD(b_j)\\) remember function standard deviation response (\\(\\sigma\\)).Using information , distribution (least-squares) estimator \\(b_j\\) given \\[b_j \\sim Normal(\\beta_j, SD(b_j))\\]\\(j = 1, \\dots, p\\). Note also \\(\\sigma\\) typically unknown, estimated using estimated standard deviation \\(s\\) instead \\(\\sigma\\). estimated standard deviation \\(b_j\\) called standard error \\(b_j\\) written \\(SE(b_j)\\). , remember standard error function \\(s\\).standard errors shown applying get_regression_table() function regression model. output model mod_mult Table 10.9:\nTABLE 10.9: regression table mod_mult\ncan see standard error numerical regressors \\(SE(b_1) = 0.22\\), \\(SE(b_2) = 0.23\\), \\(SE(b_3) = 0.03\\).","code":"\nget_regression_table(mod_mult)"},{"path":"inference-for-regression.html","id":"model-dependency","chapter":"10 Inference for Regression","heading":"10.5.1 Model dependency of estimators","text":"Many inference methods results multiple linear regression direct extension methods results discussed simple linear regression.\nimportant difference fact least-square estimators represent partial slopes values dependent regressors model.\nchange set regressors used model, least-squares estimates standard errors likely change well.\nchanges lead different confidence interval limits, different test statistics hypothesis tests, potentially different conclusions regressors.Using coffee_data example, suppose consider model:\\[y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\beta_2 \\cdot x_{i2} + \\beta_3 x_{i3} + \\epsilon_i\\,\\]\n\\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), parameters partial slopes aroma, flavor, moisture_precentage, respectively.\nRecall assume parameters \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) constants, unknown us, constants.\nuse multiple linear regression find least-square estimates \\(b_1\\), \\(b_2\\), \\(b_3\\), respectively. results using coffee_data calculated stored object mod_mult_1 shown Table 10.10:\nTABLE 10.10: Coffee example linear regression coefficients three regressors\nNow, assume decide instead construct model without regressor flavor, model \\[y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\beta_3 \\cdot x_{i3} + \\epsilon_i\\,.\\]\nUsing multiple regression compute least-square estimates \\(b'_1\\) \\(b'_3\\), respectively, \\('\\) denoting potentially different coefficient values different model.\nresults using coffee_data calculated stored object mod_mult_2 shown Table 10.11:\nTABLE 10.11: Coffee example linear regression coefficients two regressors\nfocus partial slope aroma using models.\nObserve model mod_mult_1, partial slope aroma \\(b_1 = 1.8\\).\nmodel mod_mult_2, partial slope aroma \\(b'_1 = 5.23\\).\nresults truly different models used case different.\nSimilarly, every coefficient found different, standard deviation estimate different, possible inferential results confidence intervals hypothesis tests different !results, conclusions, interpretations regressor valid model used.\nexample, interpretations conclusions made aroma effects influence total_cup_points entirely dependent whether used mod_mult_1, mod_mult_2, another model.\nNever assume conclusion using one model can translate different model.addition, important determine model adequate.\nClearly, mod_mult_1 mod_mult_2 can correct, like use one appropriate.\nneither mod_mult_1 mod_mult_2 adequate, use another model instead?\ntwo areas inferential statistics address questions.\nfirst area works comparisons two models, one using subset regressors , coffee example mod_mult_1 used regressors aroma, flavor moisture_percentage mod_mult_2 used two regressors: aroma moisture_percentage.\ndiscuss methods addressing comparison Subsection 10.5.3.\nsecond area called model selection variable selection uses alternative methods determine model, possible available adequate.","code":"\n# Fit regression model:\nmod_mult_1 <- lm(\n  total_cup_points ~ aroma + flavor + moisture_percentage, \n  data = coffee_data)\n\n# Get the coefficients of the model\ncoef(mod_mult_1)\nsigma(mod_mult_1)\n# Fit regression model:\nmod_mult_2 <- lm(\n  total_cup_points ~ aroma + moisture_percentage, data = coffee_data)\n\n# Get the coefficients of the model\ncoef(mod_mult_2)\nsigma(mod_mult_2)"},{"path":"inference-for-regression.html","id":"confidence-intervals-1","chapter":"10 Inference for Regression","heading":"10.5.2 Confidence intervals","text":"95% confidence interval coefficient multiple linear regression constructed exactly way simple linear regression, always interpret dependent model attained.example, formula 95% confidence interval \\(\\beta_1\\) given \\(b_1 \\pm q \\cdot SE_{b_1}(s)\\) critical value \\(q\\) determined level confidence required, sample size used (\\(n\\)), corresponding degrees freedom needed \\(t\\)-distribution (\\(n - p\\)).\ncoffee example, model mod_mult containsthree numerical regressors (aroma, flavor, moisture_content),one factor (continent_of_origin),confidence level 95%,sample size \\(n = 207\\),number regression coefficients \\(p = 7\\):one intercept (\\(b_0\\)),three partial slopes aroma, flavor moisture_content (\\(b_1, b_2\\), \\(b_3\\)), andthree coefficients factor levels model (\\(b_{02}\\), \\(b_{03}\\), \\(b_{04}\\)).degrees freedom \\(n - p = 207 - 7 =  200\\).95% confidence interval partial slope \\(b_1\\) aroma determined \\[\\begin{aligned}\nb_1 &\\pm q \\cdot SE(b_1)\\\\\n= 1.73 &\\pm 1.97\\cdot 0.22\\\\\n= (1.29 &, 2.17)\n\\end{aligned}\\]interpretation interval customary: “95% confident population partial slope aroma (\\(\\beta_1\\)) model mod_mult number 1.29 2.17.”find values using get_regression_table() model mod_mult.\ntime, however, add argument conf.level = 0.98 get 98% confidence intervals shown Table 10.12.\nTABLE 10.12: regression table mod_mult 98% level\ninterpretation coefficient flavor, example, : “98% confident value \\(\\beta_2\\) (population partial slope flavor) 3.79 4.85.”","code":"\nget_regression_table(mod_mult, conf.level = 0.98)"},{"path":"inference-for-regression.html","id":"hypo-test-mult-lm","chapter":"10 Inference for Regression","heading":"10.5.3 Hypothesis test for a single coefficient","text":"hypothesis test one coefficient, say \\(\\beta_1\\) model, similar one simple linear regression. general formulation two-sided test \\[\\begin{aligned}\nH_0: \\beta_1 = B\\quad \\text{}\\beta_0, \\beta_2, \\dots, \\beta_p \\text{ given arbitrary.}\\\\\nH_A: \\beta_1 \\ne B\\quad \\text{}\\beta_0, \\beta_2, \\dots, \\beta_p \\text{ given arbitrary.}\n\\end{aligned}\\]\\(B\\) hypothesized value \\(\\beta_1\\).\nmake emphasis stating \\(\\beta_0, \\beta_2, \\dots, \\beta_p\\) given arbitrary acknowledge test matters context appropriate model.\nAlso notice, can perform test \\(\\beta_1\\) parameter.simple linear regression, commonly used test one check \\(\\beta_j = 0\\) \\(j=0,1,\\dots,p\\).\n\\(\\beta_1\\) two-sided test :\\[\\begin{aligned}\nH_0: \\beta_1 = 0\\quad \\text{}\\beta_0, \\beta_2, \\dots, \\beta_p \\text{ given arbitrary}\\\\\nH_A: \\beta_1 \\ne 0\\quad \\text{}\\beta_0, \\beta_2, \\dots, \\beta_p \\text{ given arbitrary}\n\\end{aligned}\\]simple linear regression, testing \\(\\beta_1 = 0\\) testing determine linear relationship response regressor.\nNow, testing \\(\\beta_1 = 0\\) testing whether corresponding regressor part linear model already contains regressors.test can performed partial slope parameters.\nexample, use coffee example model mod_mult_1 (model three numerical regressors) perform test \\(\\beta_2\\) (population partial slope regressor flavor). hypotheses :\\[\\begin{aligned}\nH_0: \\beta_2 = 0\\quad \\text{}\\beta_0, \\beta_1, \\beta_3, \\beta_{02}, \\beta_{03}, \\beta_{04} \\text{ given arbitrary.}\\\\\nH_A: \\beta_2 \\ne 0\\quad \\text{}\\beta_0, \\beta_1, \\beta_3, \\beta_{02}, \\beta_{03}, \\beta_{04} \\text{ given arbitrary.}\n\\end{aligned}\\]relevant code shown follows output:\nTABLE 10.13: regression table total_cup_points aroma + flavor + moisture_percentage\nt-test statistic \\[t = \\frac{b_2 - 0}{SE(b_2)} = \\frac{4.28 - 0}{0.23} = 18.7\\]using test statistic, associated \\(p\\)-value near zero R shows output zero.\nenough evidence reject null hypothesis \\(\\beta_2 = 0\\).\nRecall reject null hypothesis say result statistically significant, enough evidence conclude alternative hypothesis (\\(\\beta_2 \\ne 0\\)).\nimplies changes flavor score provide information total_cup_points flavor added model already contains aroma moisture_percentage.Table 10.13 provides information coefficients. Observe, particular, test \\(\\beta_1\\) (aroma) also statistically significant test \\(\\beta_3\\) (moisture_percentage) (\\(p\\)-value = 0.62). latter, conclusion statistical evidence reject null hypothesis partial slope zero. words, found evidence adding moisture_percentage model already includes aroma flavor helps explaining changes response total_cup_points. can remove moisture_percentage regressor model.","code":"\nget_regression_table(mod_mult_1)"},{"path":"inference-for-regression.html","id":"hypo-test-mult-lm-1","chapter":"10 Inference for Regression","heading":"10.5.4 Hypothesis test for model comparison","text":"another hypothesis test can performed multiple linear regression, test compares two models, one given set regressors called full model subset regressors called reduced model.Using coffee_data example, suppose full model :\\[Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\beta_3 \\cdot X_3 + \\epsilon\\] \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) parameters partial slopes aroma, flavor, moisture_precentage, respectively. multiple linear regression outcomes using coffee_data dataset model stored object mod_mult_1. reduced model contain regressor flavor given \\[Y = \\beta_0 + \\beta_1 \\cdot X_1  + \\beta_3 \\cdot X_3 + \\epsilon.\\]multiple linear regression output using model stored object mod_mult_2. hypothesis test comparing full reduced models can written :\\[\\begin{aligned}\nH_0:\\quad &Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2  + \\epsilon\\\\\nH_A:\\quad &Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\beta_3 \\cdot X_3 + \\epsilon\n\\end{aligned}\\]words:\\[\\begin{aligned}\nH_0&:\\quad \\text{reduced model adequate}\\\\\nH_A&:\\quad \\text{full model needed}\n\\end{aligned}\\]test called ANOVA test \\(F\\)-test, distribution test statistic follows \\(F\\) distribution.\nway works test compares sum squared residuals full reduced models determines whether difference models large enough suggest full model needed.get result test R, use R function anova() enter reduced model followed full model Table 10.14 providing information test.\nTABLE 10.14: ANOVA test model comparison\ntest statistic given second row column F.\ntest statistic \\(F =349.6\\) associated \\(p\\)-value near zero.\nconclusion, reject null hypothesis conclude full model needed.ANOVA test can used test one regressor time.\nuseful factors (categorical variables) model, factor levels tested simultaneously.\nuse example , time making full model model factor continent_of_origin addition three numerical regressors, reduced model model without continent_of_origin.\nmodels computed already mod_mult mod_mult_1, respectively.\nANOVA test performed follows output given Table 10.15:\nTABLE 10.15: ANOVA test second model comparison\nObserve output degrees freedom 3, testing three coefficients time, \\(\\beta_{02}\\), \\(\\beta_{03}\\), \\(\\beta_{04}\\).\ntesting inclusion factor model, always need test factor levels .\nBased output, test statistic \\(F =4.83\\) associated \\(p\\)-value \\(0.003\\).\nreject null hypothesis conclude full model needed , alternatively, appropriate add factor continent_of_origin model already aroma, flavor, moisture_percentage.","code":"\nanova(mod_mult_2, mod_mult_1) \nanova(mod_mult_1, mod_mult) "},{"path":"inference-for-regression.html","id":"model-fit-mult","chapter":"10 Inference for Regression","heading":"10.5.5 Model fit and diagnostics","text":"simple linear regression use residuals determine fit model whether assumptions met.\nparticular, continue using plot residuals fitted values model violations present plot close null plot.treatment interpretations similar presented simple linear regression, plot residuals fitted values null plot, know sort violation one assumptions model.\nlonger clear reason , least one assumption met.\nhand, residuals--fitted-values plot appears close null plot, none assumptions broken can proceed use model.Let’s use coffee example.\nRecall shown regressors aroma, flavor, continent_of_origin statistically significant, moisture_percentage .\n, create model relevant regressors called mod_mult_final, determine residuals, create plot residuals fitted values QQ-plot using grid.arrange() function gridExtra package Figure 10.17:\nFIGURE 10.17: Residuals vs. fitted values plot QQ-plot multiple regression model.\nplot residuals fitted values (left) appears close null plot. result desirable supports Linearity condition patterns observed plot.\nEqual constant variance also holds vertical dispersion seems fairly uniform fitted values.\nSince assumed data collected random time sequences sequences consider, assumption Independence seems acceptable .\nFinally, QQ-plot suggests (exception one two observations) residuals follow approximately normal distribution. can conclude model seems good enough hold assumptions model.example concludes treatment theory-based inference. now proceed study simulation-based inference.\nLearning check\n(LC10.19) essential know estimators (\\(b_0, b_1, \\dots, b_p\\)) multiple linear regression unbiased?. ensures variance estimators always zero.B. means , average, estimators equal true population parameters estimate.C. implies estimators standard error zero.D. suggests regression model always perfect fit.(LC10.20) least-squares estimates coefficients change different sets regressors used multiple linear regression?. coefficients recalculated time, irrespective regressors.B. residuals always zero regressors changed.C. value coefficient depends specific combination regressors included model.D. models different regressors produce identical estimates.(LC10.21) 95% confidence interval coefficient multiple linear regression constructed?. using point estimate, critical value t-distribution, standard error coefficient.B. taking standard deviation coefficients .C. resampling data without replacement.D. calculating mean coefficients.(LC10.22) ANOVA test comparing two models multiple linear regression evaluate?. Whether regressors models coefficients.B. Whether reduced model adequate full model needed.C. Whether residuals two models follow normal distribution.D. Whether regression coefficients one model unbiased estimators.","code":"\n# Fit regression model:\nmod_mult_final <- lm(total_cup_points ~ aroma + flavor + continent_of_origin, \n                     coffee_data)\n# Get fitted values and residuals:\nfit_and_res_mult <- get_regression_points(mod_mult_final)\ng1 <- fit_and_res_mult |>\n  ggplot(aes(x = total_cup_points_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"fitted values (total cup points)\", y = \"residual\") +\n  geom_hline(yintercept = 0, col = \"blue\")\ng2 <- ggplot(fit_and_res_mult, aes(sample = residual)) +\n  geom_qq() +\n  geom_qq_line(col=\"blue\", linewidth = 0.5)\ngrid.arrange(g1, g2, ncol=2)"},{"path":"inference-for-regression.html","id":"simulation-based-inference-for-multiple-linear-regression","chapter":"10 Inference for Regression","heading":"10.6 Simulation-based Inference for multiple linear regression","text":"","code":""},{"path":"inference-for-regression.html","id":"confidence-intervals-for-the-partial-slopes-using-infer","chapter":"10 Inference for Regression","heading":"10.6.1 Confidence intervals for the partial slopes using infer","text":"’ll now use simulation-based methods previously learned Chapters 8 9 compute ranges plausible values partial slopes multiple linear regression. Recall simulation-based methods provide alternative theory-based methods rely assumptions normality large sample sizes. ’ll use infer package simple linear regression Section 10.3, time using fit() function.","code":""},{"path":"inference-for-regression.html","id":"getting-the-observed-fitted-model","chapter":"10 Inference for Regression","heading":"Getting the observed fitted model","text":"revisit using full model coffee_data factor continent_of_origin three numerical regressors aroma, flavor, moisture_percentage. hypothesis testing Chapter 9 Section 10.3.2, can retrieve observed statistic. case, get observed coefficients model using specify() function full model formula syntax combined fit():expect, values match values mod_mult_table given first two columns Table 10.16:\nTABLE 10.16: regression table mod_mult\nobserved_fit values point estimates partial slopes confidence intervals.","code":"\nobserved_fit <- coffee_data |> \n  specify(\n    total_cup_points ~ aroma + flavor + moisture_percentage + continent_of_origin\n  ) |>\n  fit()\nobserved_fit# A tibble: 7 × 2\n  term                                estimate\n  <chr>                                  <dbl>\n1 intercept                        37.3214    \n2 aroma                             1.73160   \n3 flavor                            4.31600   \n4 moisture_percentage              -0.00807976\n5 continent_of_originAsia          -0.392936  \n6 continent_of_originNorth America -0.273427  \n7 continent_of_originSouth America -0.478137  \nmod_mult_table"},{"path":"inference-for-regression.html","id":"bootstrap-distribution-for-the-partial-slopes","chapter":"10 Inference for Regression","heading":"Bootstrap distribution for the partial slopes","text":"now find bootstrap distribution partial slopes using infer workflow. Just like Section 10.3.1, now resampling entire rows values construct bootstrap distribution fitted partial slopes using full sample 207 coffees:specify() variables interest coffee_data formula: total_cup_points ~ aroma + flavor + moisture_percentage + continent_of_origin.generate() replicates using bootstrap resampling replacement\noriginal sample 207 coffees. generate\nreps = 1000 replicates using type = \"bootstrap\" total \n\\(207 \\cdot 1000 = 207,000\\)\nrows.Lastly, fit() models replicates boot_distribution_mlr variable. , mlr stands multiple linear regression.Since 7 coefficients model corresponding intercept, three levels continent_of_origin, aroma, flavor, moisture_percentage, 7 rows replicate. results total 7000 rows boot_distribution_mlr data frame. can visualize bootstrap distribution partial slopes Figure 10.18.\nFIGURE 10.18: Bootstrap distributions partial slopes.\n","code":"\ncoffee_data |>\n  specify(\n    total_cup_points ~ continent_of_origin + aroma + flavor + moisture_percentage\n  ) |>\n  generate(reps = 1000, type = \"bootstrap\")Response: total_cup_points (numeric)\nExplanatory: continent_of_origin (factor), aroma (numeric), flavor (numeric), mo...\n# A tibble: 207,000 × 6\n# Groups:   replicate [1,000]\n   replicate total_cup_points continent_of_origin aroma flavor moisture_percentage\n       <int>            <dbl> <fct>               <dbl>  <dbl>               <dbl>\n 1         1            83.25 Africa               7.92   7.67                10.4\n 2         1            83.67 Asia                 7.58   7.75                 9.2\n 3         1            85.5  Asia                 8.17   8.08                10.6\n 4         1            82    North America        7.42   7.42                11.5\n 5         1            84.42 Asia                 7.83   8                   10.1\n 6         1            86.08 Asia                 8.17   8.08                10.2\n 7         1            84.08 North America        7.67   7.83                11  \n 8         1            83.67 Asia                 7.83   7.83                10.2\n 9         1            82.75 North America        7.33   7.5                 11.8\n10         1            84.33 North America        7.83   7.83                10.3\n# ℹ 206,990 more rows\nboot_distribution_mlr <- coffee_quality |>\n  specify(\n    total_cup_points ~ continent_of_origin + aroma + flavor + moisture_percentage\n  ) |>\n  generate(reps = 1000, type = \"bootstrap\") |>\n  fit()\nboot_distribution_mlr# A tibble: 7,000 × 3\n# Groups:   replicate [1,000]\n   replicate term                                estimate\n       <int> <chr>                                  <dbl>\n 1         1 intercept                        37.4459    \n 2         1 continent_of_originAsia          -0.267451  \n 3         1 continent_of_originNorth America -0.203579  \n 4         1 continent_of_originSouth America -0.458541  \n 5         1 aroma                             1.80789   \n 6         1 flavor                            4.20492   \n 7         1 moisture_percentage              -0.00643938\n 8         2 intercept                        34.3890    \n 9         2 continent_of_originAsia          -0.425770  \n10         2 continent_of_originNorth America -0.237750  \n# ℹ 6,990 more rows\nvisualize(boot_distribution_mlr)"},{"path":"inference-for-regression.html","id":"confidence-intervals-for-the-partial-slopes","chapter":"10 Inference for Regression","heading":"Confidence intervals for the partial slopes","text":"Section 10.3.1, can construct 95% confidence intervals partial slopes. , focus using percentile-based method get_confidence_interval() function type = \"percentile\" construct intervals.reviewing confidence intervals, note confidence intervals aroma flavor include 0. suggests statistically significant. indicates aroma flavor meaningful reliable impact response variable presence predictors.also note 0 included confidence interval moisture_percentage. provides evidence might useful regressor multiple linear regression model. implies moisture_percentage significantly contribute explaining variability response variable accounting predictors model.can also visualize confidence intervals. done Figure 10.19.\nFIGURE 10.19: 95% confidence intervals partial slopes.\n","code":"\nconfidence_intervals_mlr <- boot_distribution_mlr |> \n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = observed_fit)\nconfidence_intervals_mlr# A tibble: 7 × 3\n  term                               lower_ci   upper_ci\n  <chr>                                 <dbl>      <dbl>\n1 aroma                             1.33584    2.13218  \n2 continent_of_originAsia          -0.657425  -0.143179 \n3 continent_of_originNorth America -0.557557   0.0131542\n4 continent_of_originSouth America -0.809466  -0.153730 \n5 flavor                            3.88508    4.74376  \n6 intercept                        34.5254    40.0449   \n7 moisture_percentage              -0.0924515  0.0417502\nvisualize(boot_distribution_mlr) +\n  shade_confidence_interval(endpoints = confidence_intervals_mlr)"},{"path":"inference-for-regression.html","id":"hypothesis-testing-for-the-partial-slopes-using-infer","chapter":"10 Inference for Regression","heading":"10.6.2 Hypothesis testing for the partial slopes using infer","text":"can also conduct hypothesis tests partial slopes multiple linear regression. use permutation test test null hypothesis \\(H_0: \\beta_i = 0\\) versus alternative hypothesis \\(H_A: \\beta_i \\neq 0\\) partial slopes. infer package constructs null distribution partial slopes null hypothesis independence.","code":""},{"path":"inference-for-regression.html","id":"null-distribution-for-the-partial-slopes","chapter":"10 Inference for Regression","heading":"Null distribution for the partial slopes","text":"can also compute null distribution partial slopes using infer workflow. shuffle values response variable total_cup_points across values regressors continent_of_origin, aroma, flavor, moisture_percentage coffee_data dataset. done assumption independence response regressors. syntax similar constructing bootstrap distribution, use type = \"permute\" set hypothesize null = \"independence\". set pseudo-random number generation seed 2024 order reader get results shuffling.","code":"\nset.seed(2024)\nnull_distribution_mlr <- coffee_quality |>\n  specify(total_cup_points ~ continent_of_origin + aroma + \n      flavor + moisture_percentage) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  fit()\nnull_distribution_mlr# A tibble: 7,000 × 3\n# Groups:   replicate [1,000]\n   replicate term                               estimate\n       <int> <chr>                                 <dbl>\n 1         1 intercept                        82.3301   \n 2         1 continent_of_originAsia          -0.193739 \n 3         1 continent_of_originNorth America -0.371403 \n 4         1 continent_of_originSouth America -0.0830341\n 5         1 aroma                            -1.21891  \n 6         1 flavor                            1.52397  \n 7         1 moisture_percentage              -0.0747986\n 8         2 intercept                        82.8068   \n 9         2 continent_of_originAsia          -0.239054 \n10         2 continent_of_originNorth America -0.617409 \n# ℹ 6,990 more rows"},{"path":"inference-for-regression.html","id":"hypothesis-tests-for-the-partial-slopes","chapter":"10 Inference for Regression","heading":"Hypothesis tests for the partial slopes","text":"can now conduct hypothesis tests partial slopes multiple linear regression. can use permutation test test null hypothesis \\(H_0: \\beta_i = 0\\) versus alternative hypothesis \\(H_A: \\beta_i \\neq 0\\) partial slopes. Let’s use significance level \\(\\alpha = 0.05\\). can visualize \\(p\\)-values null distribution comparing observed test statistics. adding shade_p_value() layer visualize().\nFIGURE 10.20: Shaded p-values partial slopes multiple regression.\nvisualizations Figure 10.20, can surmise aroma flavor statistically significant, observed test statistics fall far right null distribution. hand, moisture_percentage statistically significant, observed test statistic falls within null distribution. can also compute numerical \\(p\\)-values using get_p_value() function.results match findings visualizations shaded \\(p\\)-values null distribution regression table Table 10.13. reject null hypothesis \\(H_0: \\beta_i = 0\\) aroma flavor, fail reject moisture_percentage.\nLearning check\n(LC10.23) might one prefer use simulation-based methods (e.g., bootstrapping) inference multiple linear regression?. simulation-based methods require larger sample sizes theory-based methods.B. simulation-based methods always faster compute theory-based methods.C. simulation-based methods guarantee correct model used.D. simulation-based methods rely assumptions normality large sample sizes.(LC10.24) purpose constructing bootstrap distribution partial slopes multiple linear regression?. replace original data random numbers.B. approximate sampling distribution partial slopes resampling replacement.C. calculate exact values coefficients population.D. test model assumptions violated.(LC10.25) 95% confidence interval partial slope multiple linear regression includes 0, suggest variable?. variable statistically significant relationship response variable.B. variable statistically significant.C. variable’s coefficient estimate always negative.D. variable removed model bootstrapping.(LC10.26) hypothesis testing partial slopes using permutation tests, mean observed test statistic falls far right null distribution?. variable likely effect response.B. null hypothesis accepted.C. variable likely statistically significant, reject null.D. observed data discarded.","code":"\nvisualize(null_distribution_mlr) +\n  shade_p_value(obs_stat = observed_fit, direction = \"two-sided\")\nnull_distribution_mlr |>\n  get_p_value(obs_stat = observed_fit, direction = \"two-sided\")# A tibble: 7 × 2\n  term                             p_value\n  <chr>                              <dbl>\n1 aroma                              0.034\n2 continent_of_originAsia            0.332\n3 continent_of_originNorth America   0.56 \n4 continent_of_originSouth America   0.352\n5 flavor                             0    \n6 intercept                          0    \n7 moisture_percentage                0.918"},{"path":"inference-for-regression.html","id":"inference-conclusion","chapter":"10 Inference for Regression","heading":"10.7 Conclusion","text":"","code":""},{"path":"inference-for-regression.html","id":"summary-of-statistical-inference","chapter":"10 Inference for Regression","heading":"10.7.1 Summary of statistical inference","text":"’ve finished last two scenarios, re-display Table 10.17.\nTABLE 10.17: Scenarios sampling inference\nArmed regression modeling techniques learned Chapters 5 6, understanding sampling inference Chapter 7, tools statistical inference like confidence intervals hypothesis tests Chapters 8 9, ’re now equipped study significance relationships variables wide array data! Many ideas presented can extended multiple regression advanced modeling techniques.","code":""},{"path":"inference-for-regression.html","id":"additional-resources-8","chapter":"10 Inference for Regression","heading":"10.7.2 Additional resources","text":"R script file R code used chapter available .","code":""},{"path":"inference-for-regression.html","id":"whats-to-come-9","chapter":"10 Inference for Regression","heading":"10.7.3 What’s to come","text":"’ve now concluded last major part book “Statistical Inference infer.” closing Chapter 11 concludes book various short case studies involving real data, house prices city Seattle, Washington US. ’ll see principles book can help become great storyteller data!","code":""},{"path":"thinking-with-data.html","id":"thinking-with-data","chapter":"11 Tell Your Story with Data","heading":"11 Tell Your Story with Data","text":"Recall Preface end chapters throughout book, displayed “ModernDive flowchart” mapping journey book.\nFIGURE 11.1: ModernDive flowchart.\n","code":""},{"path":"thinking-with-data.html","id":"review","chapter":"11 Tell Your Story with Data","heading":"11.1 Review","text":"Let’s go refresher ’ve covered far. first got started data Chapter 1 learned difference R RStudio, started coding R, installed loaded first R packages, explored first dataset: domestic departure flights major New York City airport 2023. covered following three parts book (Parts 2 4 combined single portion):Data science tidyverse. assembled data science toolbox using tidyverse packages. particular, youCh.2: Visualized data using ggplot2 package.Ch.3: Wrangled data using dplyr package.Ch.4: Learned concept “tidy” data standardized data frame input output format packages tidyverse. Furthermore, learned import spreadsheet files R using readr package.Statistical/Data modeling moderndive. Using data science tools helper functions moderndive package, fit first data models. particular, youCh.5: Discovered basic regression models one explanatory variable.Ch.6: Examined multiple regression models one explanatory variable.Statistical inference infer. using newly acquired data science tools, unpacked statistical inference using infer package. particular, youCh.7: Learned role sampling variability plays statistical inference role sample size plays sampling variability.Ch.8: Constructed confidence intervals using bootstrapping learned theory-based approach confidence intervals.Ch.9: Conducted hypothesis tests using permutation.Statistical/Data modeling moderndive (revisited): Armed understanding statistical inference, revisited reviewed models constructed Ch.5 Ch.6. particular, youCh.10: Interpreted confidence intervals hypothesis tests regression setting using theory-based simulation-based approaches.’ve guided first experiences “thinking data,” expression originally coined Dr. Diane Lambert. philosophy underlying expression guided path flowchart Figure 11.1.philosophy also well-summarized “Practical Data Science Stats”: collection pre-prints focusing practical side data science workflows statistical analysis curated Dr. Jennifer Bryan Dr. Hadley Wickham. quote:many aspects day--day analytical work almost absent conventional statistics literature curriculum. yet activities account considerable share time effort data analysts applied statisticians. goal collection increase visibility adoption modern data analytical workflows. aim facilitate transfer tools frameworks industry academia, software engineering statistics computer science, across different domains.words, equipped “think data” 21st century beyond, analysts need practice going “data/science pipeline” saw Preface (re-displayed Figure 11.2). opinion , long, statistics education focused parts pipeline, instead going entirety.\nFIGURE 11.2: Data/science pipeline.\nconclude book, ’ll present additional case studies working data. Section 11.2 ’ll take full-pass “Data/Science Pipeline” order analyze sale price houses Seattle, Washington, USA. Section 11.3, ’ll present examples effective data storytelling drawn data journalism website, FiveThirtyEight.com. present case studies believe able “think data,” also able “tell story data.” Let’s explore !","code":""},{"path":"thinking-with-data.html","id":"story-packages","chapter":"11 Tell Your Story with Data","heading":"Needed packages","text":"Let’s load packages needed chapter (assumes ’ve already installed ). Read Section 1.3 information install load R packages.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(fivethirtyeight)\nlibrary(infer)"},{"path":"thinking-with-data.html","id":"seattle-house-prices","chapter":"11 Tell Your Story with Data","heading":"11.2 Case study: Seattle house prices","text":"Kaggle.com machine learning predictive modeling competition website hosts datasets uploaded companies, governmental organizations, individuals. One datasets “House Sales King County, USA”. consists sale prices homes sold May 2014 May 2015 King County, Washington, USA, includes greater Seattle metropolitan area. dataset house_prices data frame included moderndive package.dataset consists 21,613 houses 21 variables describing houses (full list description variables, see help file running ?house_prices console). case study, ’ll create multiple regression model :outcome variable \\(y\\) sale price houses.Two explanatory variables:numerical explanatory variable \\(x_1\\): house size sqft_living measured square feet living space. Note 1 square foot 0.09 square meters.categorical explanatory variable \\(x_2\\): house condition, categorical variable five levels 1 indicates “poor” 5 indicates “excellent.”","code":""},{"path":"thinking-with-data.html","id":"house-prices-EDA-I","chapter":"11 Tell Your Story with Data","heading":"11.2.1 Exploratory data analysis: part I","text":"’ve said numerous times throughout book, crucial first step presented data perform exploratory data analysis (EDA). Exploratory data analysis can give sense data, help identify issues data, bring light outliers, help inform model construction. Recall three common steps exploratory data analysis introduced Subsection 5.1.1:Looking raw data values.Computing summary statistics.Creating data visualizations.First, let’s look raw data using View() bring RStudio’s spreadsheet viewer glimpse() function dplyr package:questions can ask stage EDA: variables numerical? categorical? categorical variables, levels? Besides variables ’ll using regression model, variables think useful use model predicting house price?Observe, example, raw data condition variable values 1 5, saved R fct standing “factors.” Recall one R’s ways saving categorical variables. think “labels” 1 5 numerical values 1 5.Let’s now perform second step EDA: computing summary statistics. Recall Section 3.3 summary statistics single numerical values summarize large number values. Examples summary statistics include mean, median, standard deviation, various percentiles.Let’s use convenient tidy_summary() function moderndive package first used Subsection 6.1.1, sure select() variables interest model:\nTABLE 11.1: Summary house_prices variables.\nObserve mean price $540,088 larger median $450,000. small number expensive houses inflating average. words, “outlier” house prices dataset. (fact become even apparent create visualizations next.)However, median sensitive outlier house prices. news real estate market generally report median house prices mean/average house prices. say median robust outliers mean. Similarly, standard deviation interquartile-range (IQR) measures spread variability, IQR based quantiles Q3 - Q1 robust outliers.Let’s now perform last three common steps exploratory data analysis: creating data visualizations. Let’s first create univariate visualizations. plots focusing single variable time. Since price sqft_living numerical variables, can visualize distributions using geom_histogram() seen Section 2.5 histograms. hand, since condition categorical, can visualize distribution using geom_bar(). Recall Section 2.8 barplots since condition “pre-counted,” use geom_bar() geom_col().Figure 11.3, display three visualizations .\nFIGURE 11.3: Exploratory visualizations Seattle house prices data.\nFirst, observe bottom plot houses condition 3, conditions 4 5, almost none 1 2.Next, see histogram price (top-left plot) majority houses less two million dollars. Observe also x-axis stretches 8 million dollars, even though appear houses close price. small number houses prices closer 8 million noted tidy_summary(). outlier house prices mentioned earlier. say variable price right-skewed exhibited long right tail., histogram sqft_living middle plot shows houses appear less 5000 square feet living space. comparison, American football field US 57,600 square feet, whereas standard soccer/association football field 64,000 square feet. Observe also variable also right-skewed, although drastically price variable.price sqft_living variables, right-skew makes distinguishing houses lower end x-axis hard. scale x-axis compressed small number quite expensive immensely-sized houses.can skew? Let’s apply log10 transformation variables.\nunfamiliar transformations, highly recommend read Appendix online logarithmic (log) transformations.\nsummary, log transformations allow us alter scale variable focus multiplicative changes instead additive changes. words, shift view relative changes instead absolute changes. multiplicative/relative changes also called changes orders magnitude.Let’s create new log10 transformed versions right-skewed variable price sqft_living using mutate() function Section 3.5, ’ll give latter name log10_size, shorter easier understand name log10_sqft_living.Let’s display effects transformation variables first 10 rows house_prices:Observe particular houses sixth third rows. house sixth row price $1,225,000, just one million dollars. Since \\(10^6\\) one million, log10_price around 6.09. Contrast houses log10_price less six, since price less $1,000,000. house third row house sqft_living less 1000. Since \\(1000 = 10^3\\), ’s lone house log10_size less 3.Let’s now visualize effects transformation price Figure 11.4.\nFIGURE 11.4: House price log10 transformation.\nObserve transformation, distribution much less skewed, case, symmetric bell-shaped. Now can easily distinguish lower priced houses.Let’s house size, variable sqft_living log10 transformed log10_size.\nFIGURE 11.5: House size log10 transformation.\nObserve Figure 11.5 log10 transformation similar effect un-skewing variable. emphasize two cases resulting distributions symmetric bell-shaped, always necessarily case.Given now symmetric nature log10_price log10_size, going revise multiple regression model use new variables:outcome variable \\(y\\) sale log10_price houses.Two explanatory variables:numerical explanatory variable \\(x_1\\): house size log10_size measured log base 10 square feet living space.categorical explanatory variable \\(x_2\\): house condition, categorical variable five levels 1 indicates “poor” 5 indicates “excellent.”","code":"\nView(house_prices)\nglimpse(house_prices)Rows: 21,613\nColumns: 21\n$ id            <chr> \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", \"1954400510\", \"7237550310\", \"1321400060\"…\n$ date          <date> 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02-18, 2014-05-12, 2014-06-27, 2015-01-15,…\n$ price         <dbl> 221900, 538000, 180000, 604000, 510000, 1225000, 257500, 291850, 229500, 323000, 662500, 468000,…\n$ bedrooms      <int> 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2, 3, 4, 3, 5, 2, 3, 3, 3, 3, 3, 4, 3, 2, …\n$ bathrooms     <dbl> 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.50, 2.50, 1.00, 1.00, 1.75, 2.00, 3.00, …\n$ sqft_living   <int> 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 1890, 3560, 1160, 1430, 1370, 1810, 2950, 1…\n$ sqft_lot      <int> 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470, 6560, 9796, 6000, 19901, 9680, 4850, 50…\n$ floors        <dbl> 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.5, 1.0, 1.5, 2.0, 2.0, 1.5, 1.0, 1…\n$ waterfront    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ view          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ condition     <fct> 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 4, 5, 3, 5, 3, 3, 3, 3, …\n$ grade         <fct> 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 9, 8, 7, 8, 6, 8, 8, 7, 8, 8, 7,…\n$ sqft_above    <int> 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 1890, 1860, 860, 1430, 1370, 1810, 1980, 18…\n$ sqft_basement <int> 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, 0, 970, 0, 0, 0, 0, 760, 720, 0, 0, 0, 0…\n$ yr_built      <int> 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 2003, 1965, 1942, 1927, 1977, 1900, 1979, …\n$ yr_renovated  <int> 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ zipcode       <fct> 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, 98146, 98038, 98007, 98115, 98028, 98074…\n$ lat           <dbl> 47.5, 47.7, 47.7, 47.5, 47.6, 47.7, 47.3, 47.4, 47.5, 47.4, 47.6, 47.7, 47.8, 47.6, 47.7, 47.6, …\n$ long          <dbl> -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, -122, …\n$ sqft_living15 <int> 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 2390, 2210, 1330, 1780, 1370, 1360, 2140, …\n$ sqft_lot15    <int> 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, 7570, 8925, 6000, 12697, 10208, 4850, 40…\nhouse_prices |> \n  select(price, sqft_living, condition) |> \n  tidy_summary()\n# Histogram of house price:\nggplot(house_prices, aes(x = price)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"price (USD)\", title = \"House price\")\n\n# Histogram of sqft_living:\nggplot(house_prices, aes(x = sqft_living)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"living space (square feet)\", title = \"House size\")\n\n# Barplot of condition:\nggplot(house_prices, aes(x = condition)) +\n  geom_bar() +\n  labs(x = \"condition\", title = \"House condition\")\nhouse_prices <- house_prices |>\n  mutate(\n    log10_price = log10(price),\n    log10_size = log10(sqft_living)\n  )\nhouse_prices |> \n  select(price, log10_price, sqft_living, log10_size)# A tibble: 21,613 × 4\n     price log10_price sqft_living log10_size\n     <dbl>       <dbl>       <int>      <dbl>\n 1  221900     5.34616        1180    3.07188\n 2  538000     5.73078        2570    3.40993\n 3  180000     5.25527         770    2.88649\n 4  604000     5.78104        1960    3.29226\n 5  510000     5.70757        1680    3.22531\n 6 1225000     6.08814        5420    3.73400\n 7  257500     5.41078        1715    3.23426\n 8  291850     5.46516        1060    3.02531\n 9  229500     5.36078        1780    3.25042\n10  323000     5.50920        1890    3.27646\n# ℹ 21,603 more rows\n# Before log10 transformation:\nggplot(house_prices, aes(x = price)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"price (USD)\", title = \"House price: Before\")\n\n# After log10 transformation:\nggplot(house_prices, aes(x = log10_price)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"log10 price (USD)\", title = \"House price: After\")\n# Before log10 transformation:\nggplot(house_prices, aes(x = sqft_living)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"living space (square feet)\", title = \"House size: Before\")\n\n# After log10 transformation:\nggplot(house_prices, aes(x = log10_size)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"log10 living space (square feet)\", title = \"House size: After\")"},{"path":"thinking-with-data.html","id":"house-prices-EDA-II","chapter":"11 Tell Your Story with Data","heading":"11.2.2 Exploratory data analysis: part II","text":"Let’s now continue EDA creating multivariate visualizations. Unlike univariate histograms barplot earlier Figures 11.3, 11.4, 11.5, multivariate visualizations show relationships one variable. important step EDA perform since goal modeling explore relationships variables.Since model involves numerical outcome variable, numerical explanatory variable, categorical explanatory variable, similar regression modeling situation Section 6.1 studied UN member states dataset. Recall case numerical outcome variable fertility rate, numerical explanatory variable life expectancy, categorical explanatory variable income group.thus two choices models can fit: either (1) interaction model regression line condition level different slope different intercept (2) parallel slopes model regression line condition level slope different intercepts.Recall Subsection 6.1.3 geom_parallel_slopes() function special purpose function Evgeni Chasnovski created included moderndive package, since geom_smooth() method ggplot2 package convenient way plot parallel slopes models. plot resulting models Figure 11.6, interaction model left.\nFIGURE 11.6: Interaction parallel slopes models.\ncases, see positive relationship house price size, meaning houses larger size, tend expensive. Furthermore, plots seems houses condition 5 tend expensive house sizes evidenced fact line condition 5 highest, followed conditions 4 3. conditions 1 2, pattern isn’t clear. Recall univariate barplot condition Figure 11.3, houses condition 1 2.Let’s also show faceted version just interaction model Figure 11.7. now much apparent just houses condition 1 2.can checked using dplyr count() function:\nFIGURE 11.7: Faceted plot interaction model.\nexploratory visualization interaction model better, one left-hand plot Figure 11.6 faceted version Figure 11.7? universal right answer. need make choice depending want convey, choice, including discussing also option needed.","code":"\n# Plot interaction model\nggplot(house_prices, \n       aes(x = log10_size, y = log10_price, col = condition)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(y = \"log10 price\", \n       x = \"log10 size\", \n       title = \"House prices in Seattle\")\n# Plot parallel slopes model\nggplot(house_prices, \n       aes(x = log10_size, y = log10_price, col = condition)) +\n  geom_point(alpha = 0.05) +\n  geom_parallel_slopes(se = FALSE) +\n  labs(y = \"log10 price\", \n       x = \"log10 size\", \n       title = \"House prices in Seattle\")\nggplot(house_prices, \n       aes(x = log10_size, y = log10_price, col = condition)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(y = \"log10 price\", \n       x = \"log10 size\", \n       title = \"House prices in Seattle\") +\n  facet_wrap(~ condition)\nhouse_prices |> \n  count(condition)# A tibble: 5 × 2\n  condition     n\n  <fct>     <int>\n1 1            30\n2 2           172\n3 3         14031\n4 4          5679\n5 5          1701"},{"path":"thinking-with-data.html","id":"house-prices-regression","chapter":"11 Tell Your Story with Data","heading":"11.2.3 Regression modeling","text":"two models Figure 11.6 “better”? interaction model left-hand plot parallel slopes model right-hand plot?model selection, favor complex models additional complexity warranted. case, complex model interaction model since considers five intercepts five slopes total. contrast parallel slopes model considers five intercepts one common slope.additional complexity interaction model warranted? Looking left-hand plot Figure 11.6, ’re opinion , evidenced slight x-like (crossing) pattern lines. Therefore, ’ll focus rest analysis interaction model. (visual approach somewhat subjective, however, feel free disagree!) five different slopes five different intercepts interaction model? can get values regression table. Recall two-step process getting regression table:\nTABLE 11.2: Regression table interaction model\nRecall saw Subsection 6.1.2 interpret regression table numerical categorical explanatory variables. Let’s now 10 values estimate column Table 11.2.case, “baseline comparison” group categorical variable condition condition 1 houses, since “1” comes first alphanumerically. Thus, intercept log10_size values intercept slope log10_size baseline group. Next, condition2 condition5 terms offsets intercepts relative condition 1 intercept. Finally, log10_size:condition2 log10_size:condition5 offsets slopes log10_size relative condition 1 slope log10_size.Let’s simplify writing equation five regression lines using 10 estimate values. ’ll write line following format:\\[\n\\widehat{\\log10(\\text{price})} = \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{size}} \\cdot \\log10(\\text{size})\n\\]Condition 1:\\[\\widehat{\\log10(\\text{price})} = 3.33 + 0.69 \\cdot \\log10(\\text{size})\\]Condition 2:\\[\n\\begin{aligned}\n\\widehat{\\log10(\\text{price})} &= (3.33 + 0.047) + (0.69 - 0.024) \\cdot \\log10(\\text{size}) \\\\\n&= 3.377 + 0.666 \\cdot \\log10(\\text{size})\n\\end{aligned}\n\\]Condition 3:\\[\n\\begin{aligned}\n\\widehat{\\log10(\\text{price})} &= (3.33 - 0.367) + (0.69 + 0.133) \\cdot \\log10(\\text{size}) \\\\\n&= 2.963 + 0.823 \\cdot \\log10(\\text{size})\n\\end{aligned}\n\\]Condition 4:\\[\n\\begin{aligned}\n\\widehat{\\log10(\\text{price})} &= (3.33 - 0.398) + (0.69 + 0.146) \\cdot \\log10(\\text{size}) \\\\\n&= 2.932 + 0.836 \\cdot \\log10(\\text{size})\n\\end{aligned}\n\\]Condition 5:\\[\n\\begin{aligned}\n\\widehat{\\log10(\\text{price})} &= (3.33 - 0.883) + (0.69 + 0.31) \\cdot \\log10(\\text{size}) \\\\\n&= 2.447 + 1 \\cdot \\log10(\\text{size})\n\\end{aligned}\n\\]correspond regression lines left-hand plot Figure 11.6 faceted plot Figure 11.7. homes five condition types, size house increases, price increases. expect. However, rate increase price size fastest homes conditions 3, 4, 5 0.823, 0.836, 1, respectively. three largest slopes five.","code":"\nprice_interaction <- lm(log10_price ~ log10_size * condition, data = house_prices)\nget_regression_table(price_interaction)"},{"path":"thinking-with-data.html","id":"house-prices-making-predictions","chapter":"11 Tell Your Story with Data","heading":"11.2.4 Making predictions","text":"Say ’re realtor someone calls asking much home sell . tell ’s condition = 5 sized 1900 square feet. tell ? Let’s use interaction model fit make predictions!first make prediction visually Figure 11.8. predicted log10_price house marked black dot. following two lines intersect:regression line condition = 5 homes andThe vertical dashed black line log10_size equals 3.28, since predictor variable log10 transformed square feet living space \\(\\log10(1900) = 3.28\\).\nFIGURE 11.8: Interaction model prediction.\nEyeballing , seems predicted log10_price seems around 5.75. Let’s now find exact numerical value prediction using equation regression line condition = 5 houses, sure log10() square footage first.value close earlier visually made prediction 5.75. wait! prediction price house $5.75? , using log10_price outcome variable! want prediction dollar units price, need un-log taking power 10.\ndescribed Appendix online.predicted price home condition 5 size 1900 square feet $535,493.","code":"\n2.45 + 1 * log10(1900)[1] 5.73\n10^(2.45 + 1 * log10(1900))[1] 535493"},{"path":"thinking-with-data.html","id":"house-prices-inference-for-regression","chapter":"11 Tell Your Story with Data","heading":"11.2.5 Inference for multiple linear regression","text":"Let’s next check results multiple linear regression house prices using theory-based simulation-based methods hypothesis testing.","code":""},{"path":"thinking-with-data.html","id":"theory-based-hypothesis-testing-for-partial-slopes","chapter":"11 Tell Your Story with Data","heading":"Theory-based hypothesis testing for partial slopes","text":"Recall results theory-based inference shown interpreted estimate column get_regression_table(price_interaction) Table 11.2 shown Table 11.3. can now use values perform hypothesis tests partial slopes.\nTABLE 11.3: Regression table interaction model (shown )\nmodel, can perform hypothesis tests partial slopes \\(\\alpha\\) significance level set 0.05. example, can test null hypothesis partial slope log10_size zero. Looking \\(p\\)-value row corresponding log10_size see value 0 large statistic 4.652. \\(\\alpha = 0.05\\), log10_size regressor statistically significant relationship log10_price theory-based model.","code":"\nprice_interaction <- lm(log10_price ~ log10_size * condition, data = house_prices)\nget_regression_table(price_interaction)"},{"path":"thinking-with-data.html","id":"simulation-based-hypothesis-testing-for-partial-slopes","chapter":"11 Tell Your Story with Data","heading":"Simulation-based hypothesis testing for partial slopes","text":"can also perform hypothesis tests partial slopes using simulation-based methods. can use fit() function infer verbs check results theory-based inference.Let’s begin retrieving observed fit values price_interaction model:Next, build null distribution partial slopes using generate() function fit() function, setting hypothesize() null = \"independence\" shuffle values response variable log10_price.visualize null distribution shade observed fit values see statistically significant.regressors, appears log10_size statistically significant relationship log10_price. evidenced fact observed fit value log10_size far outside range null distribution.Lastly, can calculate \\(p\\)-value partial slopes using get_p_value() function.\\(p\\)-value matches conclusion. log10_size appears significant regressor predicting log10_price model. matches findings theory-based analysis model. Remember won’t always case depending distributions variables whether assumptions hold.\nLearning check\n(LC11.1) Check LINE conditions met inference made Seattle house prices example price_interaction <- lm(log10_price ~ log10_size * condition, data = house_prices).(LC11.2) Repeat regression modeling Subsection 11.2.3 prediction making just house condition 5 size 1900 square feet Subsection 11.2.4, using parallel slopes model visualized Figure 11.6.(LC11.3) Interpret results rows terms inference get_regression_table(price_interaction) output Table 11.2 interpret Subsection 11.2.5.(LC11.4) Create, visualize, interpret confidence intervals using theory-based simulation-based approaches mirror hypothesis testing done Subsection 11.2.5.","code":"\nobserved_fit_coefficients <- house_prices |>\n  specify(\n    log10_price ~ log10_size * condition\n    ) |>\n  fit()\nobserved_fit_coefficients# A tibble: 10 × 2\n   term                    estimate\n   <chr>                      <dbl>\n 1 intercept              3.33046  \n 2 log10_size             0.689534 \n 3 condition2             0.0469644\n 4 condition3            -0.367010 \n 5 condition4            -0.398174 \n 6 condition5            -0.883036 \n 7 log10_size:condition2 -0.0241635\n 8 log10_size:condition3  0.132593 \n 9 log10_size:condition4  0.145632 \n10 log10_size:condition5  0.309866 \nnull_distribution_housing <- house_prices |>\n  specify(\n    log10_price ~ log10_size * condition\n    ) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  fit()\nvisualize(null_distribution_housing) +\n  shade_p_value(obs_stat = observed_fit_coefficients, \n                direction = \"two-sided\")\nnull_distribution_housing |>\n  get_p_value(obs_stat = observed_fit_coefficients, direction = \"two-sided\")# A tibble: 10 × 2\n   term                  p_value\n   <chr>                   <dbl>\n 1 condition2              0.948\n 2 condition3              0.538\n 3 condition4              0.508\n 4 condition5              0.178\n 5 intercept               0    \n 6 log10_size              0.002\n 7 log10_size:condition2   0.916\n 8 log10_size:condition3   0.486\n 9 log10_size:condition4   0.454\n10 log10_size:condition5   0.148"},{"path":"thinking-with-data.html","id":"data-journalism","chapter":"11 Tell Your Story with Data","heading":"11.3 Case study: effective data storytelling","text":"’ve progressed throughout book, ’ve seen work data variety ways. ’ve learned effective strategies plotting data understanding types plots work best combinations variable types. ’ve summarized data spreadsheet form calculated summary statistics variety different variables. Furthermore, ’ve seen value statistical inference process come conclusions population using sampling. Lastly, ’ve explored fit linear regression models importance checking conditions required confidence intervals hypothesis tests valid interpretation. throughout, ’ve learned many computational techniques focused writing R code ’s reproducible.now present another set case studies, time “effective data storytelling” done data journalists around world. Great data stories don’t mislead reader, rather engulf understanding importance data plays lives storytelling.","code":""},{"path":"thinking-with-data.html","id":"bechdel-test-for-hollywood-gender-representation","chapter":"11 Tell Your Story with Data","heading":"11.3.1 Bechdel test for Hollywood gender representation","text":"recommend read analyze Walt Hickey’s FiveThirtyEight.com article, “Dollar--Cents Case Hollywood’s Exclusion Women.” , Walt completed multi-decade study many movies pass Bechdel test, informal test gender representation movie created Alison Bechdel.read article, think carefully Walt Hickey using data, graphics, analyses tell reader story. spirit reproducibility, FiveThirtyEight also shared data R code used article. can also find data used many articles GitHub page.ModernDive co-authors Chester Ismay Albert Y. Kim along Jennifer Chunn went one step creating fivethirtyeight package provides access datasets easily R. complete list 129 datasets included fivethirtyeight package, check package webpage https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html.Furthermore, example “vignettes” fully reproducible start--finish analyses data using dplyr, ggplot2, packages tidyverse available . example, vignette showing reproduce one plots end article Bechdel test available .","code":""},{"path":"thinking-with-data.html","id":"us-births-in-1999","chapter":"11 Tell Your Story with Data","heading":"11.3.2 US Births in 1999","text":"US_births_1994_2003 data frame included fivethirtyeight package provides information number daily births United States 1994 2003. information data frame including link original article FiveThirtyEight.com, check help file running ?US_births_1994_2003 console.’s always good idea preview data, either using RStudio’s spreadsheet View() function using glimpse() dplyr package:’ll focus number births date, births occurred 1999. Recall Section 3.2 can using filter() function dplyr package:discussed Section 2.4, since date notion time thus sequential ordering , linegraph appropriate visualization use scatterplot. words, use geom_line() instead geom_point(). Recall plots called time series plots.\nFIGURE 11.9: Number births US 1999.\nFigure 11.9, see big dip occurring just January 1st, 2000, likely due holiday season. However, large spike 14,000 births occurring just October 1st, 1999? reason anomalously high spike?Let’s sort rows US_births_1999 descending order number births. Recall Section 3.6 can use arrange() function dplyr function , making sure sort births descending order:date highest number births (14,540) fact 1999-09-09. write date month/day/year format (standard format US), date highest number births 9/9/99! nines! parents deliberately induced labor higher rate date? Maybe? Whatever cause may , fact makes fun story!\nLearning check\n(LC11.5) date 1994 2003 fewest number births US? story tell case?Time think data tell story data! statistical modeling help ? types statistical inference helpful? else can find can take analysis? assumptions make analysis? leave questions reader explore examine. Remember get touch us via contact info Preface. ’d love see come !Please check additional problem sets labs https://moderndive.com/labs.","code":"\nglimpse(US_births_1994_2003)Rows: 3,652\nColumns: 6\n$ year          <int> 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, …\n$ month         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ date_of_month <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2…\n$ date          <date> 1994-01-01, 1994-01-02, 1994-01-03, 1994-01-04, 1994-01-05, 1994-01-06, 1994-01-07, 1994-01-08,…\n$ day_of_week   <ord> Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, Mon, Tues,…\n$ births        <int> 8096, 7772, 10142, 11248, 11053, 11406, 11251, 8653, 7910, 10498, 11706, 11567, 11212, 11570, 86…\nUS_births_1999 <- US_births_1994_2003 |>\n  filter(year == 1999)\nggplot(US_births_1999, aes(x = date, y = births)) +\n  geom_line() +\n  labs(x = \"Date\", \n       y = \"Number of births\", \n       title = \"US Births in 1999\")\nUS_births_1999 |> \n  arrange(desc(births))# A tibble: 365 × 6\n    year month date_of_month date       day_of_week births\n   <int> <int>         <int> <date>     <ord>        <int>\n 1  1999     9             9 1999-09-09 Thurs        14540\n 2  1999    12            21 1999-12-21 Tues         13508\n 3  1999     9             8 1999-09-08 Wed          13437\n 4  1999     9            21 1999-09-21 Tues         13384\n 5  1999     9            28 1999-09-28 Tues         13358\n 6  1999     7             7 1999-07-07 Wed          13343\n 7  1999     7             8 1999-07-08 Thurs        13245\n 8  1999     8            17 1999-08-17 Tues         13201\n 9  1999     9            10 1999-09-10 Fri          13181\n10  1999    12            28 1999-12-28 Tues         13158\n# ℹ 355 more rows"},{"path":"thinking-with-data.html","id":"scripts-of-r-code","chapter":"11 Tell Your Story with Data","heading":"11.3.3 Scripts of R code","text":"R script file R code used chapter available .R code files saved *.R files relevant chapters throughout entire book given:","code":""},{"path":"thinking-with-data.html","id":"concluding-remarks","chapter":"11 Tell Your Story with Data","heading":"Concluding remarks","text":"Now ’ve made point book, suspect know thing two work data R! ’ve also gained lot knowledge use simulation-based techniques statistical inference techniques help build intuition traditional theory-based inferential methods.hope ’ve come appreciate power data respects, data wrangling, tidying datasets, data visualization, statistical/data modeling, statistical inference. opinion, important, data visualization may important tool citizen professional data scientist toolbox. can create truly beautiful graphics display information ways reader can clearly understand, great power tell tale data. Let’s hope skills help tell great stories data future. Thanks coming along journey dove modern data analysis using R tidyverse!","code":""},{"path":"appendixA.html","id":"appendixA","chapter":"A Statistical Background","heading":"A Statistical Background","text":"","code":""},{"path":"appendixA.html","id":"appendix-stat-terms","chapter":"A Statistical Background","heading":"A.1 Basic statistical terms","text":"Note following statistical terms apply numerical variables, except distribution can exist numerical categorical variables.","code":""},{"path":"appendixA.html","id":"mean","chapter":"A Statistical Background","heading":"A.1.1 Mean","text":"mean commonly reported measure center. commonly called average though term can little ambiguous. mean sum data elements divided many elements . \\(n\\) data points, mean given :\\[Mean = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\]","code":""},{"path":"appendixA.html","id":"median","chapter":"A Statistical Background","heading":"A.1.2 Median","text":"median calculated first sorting variable’s data smallest largest. sorting data, middle element list median. middle falls two values, median mean two middle values.","code":""},{"path":"appendixA.html","id":"appendix-sd-variance","chapter":"A Statistical Background","heading":"A.1.3 Standard deviation and variance","text":"next discuss standard deviation (\\(sd\\)) variable. formula can little intimidating first important remember essentially measure far expect given data value mean:\\[sd = \\sqrt{\\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}}\\]variance variable merely standard deviation squared.\\[variance = sd^2 = \\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}\\]","code":""},{"path":"appendixA.html","id":"five-number-summary","chapter":"A Statistical Background","heading":"A.1.4 Five-number summary","text":"five-number summary consists five summary statistics: minimum, first quantile AKA 25th percentile, second quantile AKA median 50th percentile, third quantile AKA 75th, maximum. five-number summary variable used constructing boxplots, seen Section 2.7.quantiles calculated asfirst quantile (\\(Q_1\\)): median first half sorted datathird quantile (\\(Q_3\\)): median second half sorted dataThe interquartile range (IQR) defined \\(Q_3 - Q_1\\) measure spread middle 50% values . IQR corresponds length box boxplot.median IQR influenced presence outliers ways mean standard deviation . , thus, recommended skewed datasets. say case median IQR robust outliers.","code":""},{"path":"appendixA.html","id":"distribution","chapter":"A Statistical Background","heading":"A.1.5 Distribution","text":"distribution variable shows frequently different values variable occur. Looking visualization distribution can show values centered, show values vary, give information typical value might fall. can also alert presence outliers.Recall Chapter 2 can visualize distribution numerical variable using binning histogram can visualize distribution categorical variable using barplot.","code":""},{"path":"appendixA.html","id":"outliers","chapter":"A Statistical Background","heading":"A.1.6 Outliers","text":"Outliers correspond values dataset fall far outside range “ordinary” values. context boxplot, default correspond values \\(Q_1 - (1.5 \\cdot IQR)\\) \\(Q_3 + (1.5 \\cdot IQR)\\).","code":""},{"path":"appendixA.html","id":"appendix-normal-curve","chapter":"A Statistical Background","heading":"A.2 Normal distribution","text":"Let’s next discuss one particular kind distribution: normal distributions. bell-shaped distributions defined two values: (1) mean \\(\\mu\\) (“mu”) locates center distribution (2) standard deviation \\(\\sigma\\) (“sigma”) determines variation distribution. Figure .1, plot three normal distributions :solid normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).dotted normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).dashed normal curve mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\nFIGURE .1: Three normal distributions.\nNotice solid dotted line normal curves center due common mean \\(\\mu\\) = 5. However, dotted line normal curve wider due larger standard deviation \\(\\sigma\\) = 5. hand, solid dashed line normal curves variation due common standard deviation \\(\\sigma\\) = 2. However, centered different locations.mean \\(\\mu\\) = 0 standard deviation \\(\\sigma\\) = 1, normal distribution special name. ’s called standard normal distribution \\(z\\)-curve.Furthermore, variable follows normal curve, three rules thumb can use:68% values lie within \\(\\pm\\) 1 standard deviation mean.95% values lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations mean.99.7% values lie within \\(\\pm\\) 3 standard deviations mean.Let’s illustrate standard normal curve Figure .2. dashed lines -3, -1.96, -1, 0, 1, 1.96, 3. 7 lines cut x-axis 8 segments. areas normal curve 8 segments marked add 100%. example:middle two segments represent interval -1 1. shaded area interval represents 34% + 34% = 68% area curve. words, 68% values.middle four segments represent interval -1.96 1.96. shaded area interval represents 13.5% + 34% + 34% + 13.5% = 95% area curve. words, 95% values.middle six segments represent interval -3 3. shaded area interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% area curve. words, 99.7% values.\nFIGURE .2: Rules thumb areas normal curves.\n\nLearning check\nSay normal distribution mean \\(\\mu = 6\\) standard deviation \\(\\sigma = 3\\).(LCA.1) proportion area normal curve less 3? Greater 12? 0 12?(LCA.2) 2.5th percentile area normal curve? 97.5th percentile? 100th percentile?","code":""},{"path":"appendixA.html","id":"additional-normal-calculations","chapter":"A Statistical Background","heading":"A.2.1 Additional normal calculations","text":"normal density curve, probabilities areas given interval can obtained using R function pnorm(). Think p name __p__robability __p__ercentage function finds area curve left given value probability observing number less equal value. possible indicate appropriate expected value standard deviation arguments function, default uses standard normal values, \\(\\mu = 0\\) \\(\\sigma = 1\\). example, probability observing value less equal 1 standard normal curve given :84%. probability observing value less equal one standard deviation mean.Similarly, probability observing standard value -1 1 given subtracting area left -1 area left 1. R, obtain probability follows:probability getting standard value -1 1, equivalently, probability observing value within one standard deviation mean 68%. Similarly, probability getting value within 2 standard deviations mean given byor 95%.Moreover, need restrict study areas within one two standard deviations mean. can find number standard deviations needed desired percentage around mean using R function qnorm(). q name stands \\(quantile\\) function can thought inverse complement pnorm(). finds value random variable given area curve left value. using standard normal, quantile also represents number standard deviations. example, learned area standard normal curve left standard value 1 approximately 84%. instead, want find standard value corresponds exactly area 84% curve left value, can use following syntax:words, exactly 84% chance observed standard value less equal 0.994. Similarly, exactly 95% chance obtaining value within q number standard deviations mean, need select appropriate value qnorm().want find standard value q area middle exactly 0.95 (95%). using qnorm() need provide total area curve left q. Since total area normal density curve 1, curve symmetric, area middle 0.95, total area tails 1 - 0.95 = 0.05 (5%), area tail 0.05/2 = 0.025 (2.5%). total area curve left q area middle area left tail 0.95 + 0.025 = 0.975. can now obtain standard value q using qnorm():probability observing value within 1.96 standard deviations mean exactly 95%.can follow method obtain number standard deviations needed area, probability, around mean. example, want area 98% around mean, area tails 1 - 0.98 = 0.02, 0.02/2 = 0.01 tail, area curve left desired q value 0.98 + 0.01 = 0.99 soThe area within 2.33 standard deviations mean 98%, 98% chance choosing value within 2.33 standard deviations mean. information useful us.","code":"\npnorm(1)[1] 0.841\npnorm(1) - pnorm(-1)[1] 0.683\npnorm(2) - pnorm(-2)[1] 0.954\nqnorm(0.84)[1] 0.994\nggplot(NULL, aes(c(-4,4))) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"grey100\", xlim = c(-4, -2)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"grey80\", xlim = c(-2, 2)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"grey100\", xlim = c(2, 4)) +\n  labs(x = \"z\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = NULL) + \n  annotate(geom=\"text\", x=2, y=-0.01, label=\"q\",\n           color=\"blue\")\nq <- qnorm(0.975)\nq[1] 1.96\nqnorm(0.99)[1] 2.33"},{"path":"appendixA.html","id":"the-t-distribution-calculations","chapter":"A Statistical Background","heading":"A.3 The t distribution calculations","text":"syntax R \\(t\\) distribution analogous standard normal distribution. use function pt() instead pnorm() qt() instead qnorm().\naddition, \\(t\\) distribution requires one additional parameter, degrees freedom. sample mean problems, degrees freedom needed exactly \\(n-1\\), size samples minus one.construct 95% confidence interval population mean, time using sample standard deviation estimate standard error \\(t\\) distribution determine wide confidence interval .start obtaining sample statistics:obtain number standard deviations \\(t\\) distribution account 95% values, proceed normal case: area middle 0.95, area tails 1-0.95 = 0.05. Since \\(t\\) distribution also symmetric, area tail 0.05/2 - 0.025. number standard deviation around center given value \\(q\\) area \\(t\\) curve left \\(q\\) exactly \\(0.95 + 0.025 = 0.975\\). Using R get:, order account 95% observations around mean, need take account values within 1.98 standard deviation mean. Compare number 1.96 obtained standard normal; difference due fact \\(t\\) curve thicker tails standard normal.\ncan now construct 95% confidence intervalWe 95% confident population mean weight almonds number 3.61 3.75 grams.","code":"\nalmonds_sample_100 |> \nsummarize(mean_weight = mean(weight),\nsd_weight = sd(weight),\nsample_size = n())# A tibble: 1 × 3\n  mean_weight sd_weight sample_size\n        <dbl>     <dbl>       <int>\n1       3.682  0.362199         100\nqt(0.975, df = 100 - 1)[1] 1.98\nxbar <- 3.682 \nse_xbar <- 0.362/sqrt(100)\nlower_bound <- xbar - 1.98 *  se_xbar\nupper_bound <- xbar + 1.98 *  se_xbar\nc(lower_bound, upper_bound)[1] 3.61 3.75"},{"path":"appendixA.html","id":"appendix-log10-transformations","chapter":"A Statistical Background","heading":"A.4 log10 transformations","text":"simplest, log10 transformations return base 10 logarithms. example, since \\(1000 = 10^3\\), running log10(1000) returns 3 R. undo log10 transformation, raise 10 value. example, undo previous log10 transformation return original value 1000, raise 10 power 3 running 10^(3) = 1000 R. Log transformations allow us focus changes orders magnitude. words, allow us focus multiplicative changes instead additive ones. Let’s illustrate idea Table .1 examples prices consumer goods 2019 US dollars.\nTABLE .1: TABLE .2: log10 transformed prices, orders magnitude, examples\nLet’s make remarks log10 transformations based Table .1:purchasing cup coffee, tend think prices ranging single dollars, $2 $3. However, purchasing mobile phone, don’t tend think prices units single dollars $313 $727. Instead, tend think prices units hundreds dollars like $300 $700. Thus, cups coffee mobile phones different orders magnitude price.Let’s say want know log10 transformed value $76. hard compute exactly without calculator. However, since $76 $10 $100 since log10(10) = 1 log10(100) = 2, know log10(76) 1 2. fact, log10(76) 1.880814.log10 transformations monotonic, meaning preserve orders. Price lower Price B, log10(Price ) also lower log10(Price B).importantly, increments one log10-scale correspond relative multiplicative changes original scale absolute additive changes. example, increasing log10(Price) 3 4 corresponds multiplicative increase factor 10: $100 $1000.","code":""},{"path":"appendixB.html","id":"appendixB","chapter":"B Inference Examples","heading":"B Inference Examples","text":"appendix designed provide examples five basic hypothesis tests corresponding confidence intervals. Traditional theory-based methods well computational-based methods presented. \nNote: appendix still construction. \nlike contribute, please check us GitHub https://github.com/moderndive/moderndive_book.\n","code":""},{"path":"appendixB.html","id":"needed-packages-1","chapter":"B Inference Examples","heading":"Needed packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(janitor)"},{"path":"appendixB.html","id":"inference-mind-map","chapter":"B Inference Examples","heading":"B.1 Inference mind map","text":"help better navigate choose appropriate analysis, ’ve created mind map http://coggle.available .\nFIGURE B.1: Mind map Inference.\n","code":""},{"path":"appendixB.html","id":"one-mean","chapter":"B Inference Examples","heading":"B.2 One mean","text":"","code":""},{"path":"appendixB.html","id":"problem-statement","chapter":"B Inference Examples","heading":"B.2.1 Problem statement","text":"National Survey Family Growth conducted \nCenters Disease Control gathers information family life, marriage divorce, pregnancy,\ninfertility, use contraception, men’s women’s health. One variables collected \nsurvey age first marriage. 5,534 randomly sampled US women 2006 2010 completed survey. women sampled married least . evidence mean age first marriage US women 2006 2010 greater 23 years? (Tweaked bit Diez, Barr, Çetinkaya-Rundel 2014 [Chapter 4])","code":""},{"path":"appendixB.html","id":"competing-hypotheses","chapter":"B Inference Examples","heading":"B.2.2 Competing hypotheses","text":"","code":""},{"path":"appendixB.html","id":"in-words","chapter":"B Inference Examples","heading":"In words","text":"Null hypothesis: mean age first marriage US women 2006 2010 equal 23 years.Alternative hypothesis: mean age first marriage US women 2006 2010 greater 23 years.","code":""},{"path":"appendixB.html","id":"in-symbols-with-annotations","chapter":"B Inference Examples","heading":"In symbols (with annotations)","text":"\\(H_0: \\mu = \\mu_{0}\\), \\(\\mu\\) represents mean age first marriage US women 2006 2010 \\(\\mu_0\\) 23.\\(H_A: \\mu > 23\\)","code":""},{"path":"appendixB.html","id":"set-alpha","chapter":"B Inference Examples","heading":"Set \\(\\alpha\\)","text":"’s important set significance level starting testing using data. Let’s set significance level 5% .","code":""},{"path":"appendixB.html","id":"exploring-the-sample-data","chapter":"B Inference Examples","heading":"B.2.3 Exploring the sample data","text":"histogram also shows distribution age.observed statistic interest sample mean:","code":"\nage_at_marriage <- read_csv(\"https://moderndive.com/data/ageAtMar.csv\")\nage_summ <- age_at_marriage |>\n  summarize(\n    sample_size = n(),\n    mean = mean(age),\n    sd = sd(age),\n    minimum = min(age),\n    lower_quartile = quantile(age, 0.25),\n    median = median(age),\n    upper_quartile = quantile(age, 0.75),\n    max = max(age)\n  )\nkable(age_summ) |>\n  kable_styling(\n    font_size = ifelse(is_latex_output(), 10, 16),\n    latex_options = c(\"hold_position\")\n  )\nggplot(data = age_at_marriage, mapping = aes(x = age)) +\n  geom_histogram(binwidth = 3, color = \"white\")\nx_bar <- age_at_marriage |>\n  specify(response = age) |>\n  calculate(stat = \"mean\")\nx_barResponse: age (numeric)\n# A tibble: 1 × 1\n     stat\n    <dbl>\n1 23.4402"},{"path":"appendixB.html","id":"guess-about-statistical-significance","chapter":"B Inference Examples","heading":"Guess about statistical significance","text":"looking see observed sample mean 23.44 statistically greater \\(\\mu_0 = 23\\). seem quite close, large sample size . Let’s guess large sample size lead us reject practically small difference.","code":""},{"path":"appendixB.html","id":"non-traditional-methods","chapter":"B Inference Examples","heading":"B.2.4 Non-traditional methods","text":"","code":""},{"path":"appendixB.html","id":"bootstrapping-for-hypothesis-test","chapter":"B Inference Examples","heading":"Bootstrapping for hypothesis test","text":"order look see observed sample mean 23.44 statistically greater \\(\\mu_0 = 23\\), need account sample size. also need determine process replicates original sample size 5534 selected.can use idea bootstrapping simulate population sample came generate samples simulated population account sampling variability. Recall bootstrapping apply context:Sample replacement original sample 5534 women repeat process 10,000 times,calculate mean 10,000 bootstrap samples created Step 1,combine bootstrap statistics calculated Step 2 boot_distn object, andshift center distribution null value 23. (needed since centered 23.44 via process bootstrapping.)can next use distribution observe \\(p\\)-value. Recall right-tailed test looking values greater equal 23.44 \\(p\\)-value.","code":"\nset.seed(2018)\nnull_distn_one_mean <- age_at_marriage |>\n  specify(response = age) |>\n  hypothesize(null = \"point\", mu = 23) |>\n  generate(reps = 10000) |>\n  calculate(stat = \"mean\")\nnull_distn_one_mean |> visualize()\nnull_distn_one_mean |>\n  visualize() +\n  shade_p_value(obs_stat = x_bar, direction = \"greater\")"},{"path":"appendixB.html","id":"calculate-p-value","chapter":"B Inference Examples","heading":"Calculate \\(p\\)-value","text":"\\(p\\)-value 0 reject null hypothesis 5% level. can also see histogram far tail null distribution.","code":"\npvalue <- null_distn_one_mean |>\n  get_pvalue(obs_stat = x_bar, direction = \"greater\")Warning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\npvalue# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0"},{"path":"appendixB.html","id":"bootstrapping-for-confidence-interval","chapter":"B Inference Examples","heading":"Bootstrapping for confidence interval","text":"can also create confidence interval unknown population parameter \\(\\mu\\) using sample data using bootstrapping. Note don’t need shift distribution since want center confidence interval point estimate \\(\\bar{x}_{obs} = 23.44\\).see 23 contained confidence interval plausible value \\(\\mu\\) (unknown population mean) entire interval larger 23. matches hypothesis test results rejecting null hypothesis favor alternative (\\(\\mu > 23\\)).Interpretation: 95% confident true mean age first marriage US women 2006 2010 23.315 23.567.","code":"\nboot_distn_one_mean <- age_at_marriage |>\n  specify(response = age) |>\n  generate(reps = 10000) |>\n  calculate(stat = \"mean\")\nci <- boot_distn_one_mean |>\n  get_ci()\nci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1  23.3148  23.5669\nboot_distn_one_mean |>\n  visualize() +\n  shade_ci(endpoints = ci)"},{"path":"appendixB.html","id":"traditional-methods","chapter":"B Inference Examples","heading":"B.2.5 Traditional methods","text":"","code":""},{"path":"appendixB.html","id":"check-conditions","chapter":"B Inference Examples","heading":"Check conditions","text":"Remember order use shortcut (formula-based, theoretical) approach, need check conditions met.Independent observations: observations collected independently.\ncases selected independently random sampling condition met.Independent observations: observations collected independently.cases selected independently random sampling condition met.Approximately normal: distribution response variable normal sample size least 30.\nhistogram sample show skew.Approximately normal: distribution response variable normal sample size least 30.histogram sample show skew.Q-Q plot also shows skew.sample size quite large though (\\(n = 5534\\)) conditions met.","code":"\nggplot(data = age_at_marriage, mapping = aes(sample = age)) +\n  stat_qq()"},{"path":"appendixB.html","id":"test-statistic","chapter":"B Inference Examples","heading":"Test statistic","text":"test statistic random variable based sample data. , want look way estimate population mean \\(\\mu\\). good guess sample mean \\(\\bar{X}\\). Recall sample mean actually random variable vary different samples (theoretically, ) collected. looking see likely us observed sample mean \\(\\bar{x}_{obs} = 23.44\\) larger assuming population mean 23 (assuming null hypothesis true). conditions met assuming \\(H_0\\) true, can “standardize” original test statistic \\(\\bar{X}\\) \\(T\\) statistic follows \\(t\\) distribution degrees freedom equal \\(df = n - 1\\):\\[ T =\\dfrac{ \\bar{X} - \\mu_0}{ S / \\sqrt{n} } \\sim t (df = n - 1) \\]\\(S\\) represents standard deviation sample \\(n\\) sample size.","code":""},{"path":"appendixB.html","id":"observed-test-statistic","chapter":"B Inference Examples","heading":"Observed test statistic","text":"one compute observed test statistic “hand”, focus set-problem understanding formula test statistic applies. can use t_test() function perform analysis us.see \\(t_{obs}\\) value 6.936.","code":"\nt_test_results <- age_at_marriage |>\n  t_test(\n    formula = age ~ NULL,\n    alternative = \"greater\",\n    mu = 23\n  )\nt_test_results# A tibble: 1 × 7\n  statistic  t_df     p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>       <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1   6.93570  5533 2.25216e-12 greater      23.4402  23.3358      Inf"},{"path":"appendixB.html","id":"compute-p-value","chapter":"B Inference Examples","heading":"Compute \\(p\\)-value","text":"\\(p\\)-value—probability observing \\(t_{obs}\\) value 6.936 null distribution \\(t\\) 5533 degrees freedom—essentially 0.","code":""},{"path":"appendixB.html","id":"state-conclusion","chapter":"B Inference Examples","heading":"State conclusion","text":", therefore, sufficient evidence reject null hypothesis. initial guess observed sample mean statistically greater hypothesized mean supporting evidence . Based sample, evidence mean age first marriage US women 2006 2010 greater 23 years.","code":""},{"path":"appendixB.html","id":"confidence-interval","chapter":"B Inference Examples","heading":"Confidence interval","text":"","code":"\nt.test(\n  x = age_at_marriage$age,\n  alternative = \"two.sided\",\n  mu = 23\n)$conf[1] 23.3 23.6\nattr(,\"conf.level\")\n[1] 0.95"},{"path":"appendixB.html","id":"comparing-results","chapter":"B Inference Examples","heading":"B.2.6 Comparing results","text":"Observing bootstrap distribution created, makes quite bit sense results similar traditional non-traditional methods terms \\(p\\)-value confidence interval since distributions look similar normal distributions. conditions also met (large sample size driver ) leads us better guess using methods whether traditional (formula-based) non-traditional (computational-based) lead similar results.","code":""},{"path":"appendixB.html","id":"one-proportion","chapter":"B Inference Examples","heading":"B.3 One proportion","text":"","code":""},{"path":"appendixB.html","id":"problem-statement-1","chapter":"B Inference Examples","heading":"B.3.1 Problem statement","text":"CEO large electric utility claims 80 percent 1,000,000 customers satisfied service receive. test claim, local newspaper surveyed 100 customers, using simple random sampling. 73 satisfied remaining unsatisfied. Based findings sample, can reject CEO’s hypothesis 80% customers satisfied? [Tweaked bit http://stattrek.com/hypothesis-test/proportion.aspx?Tutorial=AP]","code":""},{"path":"appendixB.html","id":"competing-hypotheses-1","chapter":"B Inference Examples","heading":"B.3.2 Competing hypotheses","text":"","code":""},{"path":"appendixB.html","id":"in-words-1","chapter":"B Inference Examples","heading":"In words","text":"Null hypothesis: proportion customers large electric utility satisfied service receive equal 0.80.Alternative hypothesis: proportion customers large electric utility satisfied service receive different 0.80.","code":""},{"path":"appendixB.html","id":"in-symbols-with-annotations-1","chapter":"B Inference Examples","heading":"In symbols (with annotations)","text":"\\(H_0: \\pi = p_{0}\\), \\(\\pi\\) represents proportion customers large electric utility satisfied service receive \\(p_0\\) 0.8.\\(H_A: \\pi \\ne 0.8\\)","code":""},{"path":"appendixB.html","id":"set-alpha-1","chapter":"B Inference Examples","heading":"Set \\(\\alpha\\)","text":"’s important set significance level starting testing using data. Let’s set significance level 5% .","code":""},{"path":"appendixB.html","id":"exploring-the-sample-data-1","chapter":"B Inference Examples","heading":"B.3.3 Exploring the sample data","text":"bar graph also shows distribution satisfy.observed statistic computed ","code":"\nelec <- c(rep(\"satisfied\", 73), rep(\"unsatisfied\", 27)) |>\n  enframe() |>\n  rename(satisfy = value)\nggplot(data = elec, aes(x = satisfy)) +\n  geom_bar()\np_hat <- elec |>\n  specify(response = satisfy, success = \"satisfied\") |>\n  calculate(stat = \"prop\")\np_hatResponse: satisfy (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  0.73"},{"path":"appendixB.html","id":"guess-about-statistical-significance-1","chapter":"B Inference Examples","heading":"Guess about statistical significance","text":"looking see sample proportion 0.73 statistically different \\(p_0 = 0.8\\) based sample. seem quite close, sample size huge (\\(n = 100\\)). Let’s guess evidence reject null hypothesis.","code":""},{"path":"appendixB.html","id":"non-traditional-methods-1","chapter":"B Inference Examples","heading":"B.3.4 Non-traditional methods","text":"","code":""},{"path":"appendixB.html","id":"simulation-for-hypothesis-test","chapter":"B Inference Examples","heading":"Simulation for hypothesis test","text":"order look see 0.73 statistically different 0.8, need account sample size. also need determine process replicates original sample size 100 selected. can use idea unfair coin simulate process. simulate flipping unfair coin (probability success 0.8 matching null hypothesis) 100 times. keep track many heads come 100 flips. simulated statistic matches calculated original statistic \\(\\hat{p}\\): number heads (satisfied) total sample 100. repeat process many times (say 10,000) create null distribution looking simulated proportions successes:can next use distribution observe \\(p\\)-value. Recall two-tailed test looking values 0.8 - 0.73 = 0.07 away 0.8 directions \\(p\\)-value:","code":"\nset.seed(2018)\nnull_distn_one_prop <- elec |>\n  specify(response = satisfy, success = \"satisfied\") |>\n  hypothesize(null = \"point\", p = 0.8) |>\n  generate(reps = 10000) |>\n  calculate(stat = \"prop\")\nnull_distn_one_prop |> visualize()\nnull_distn_one_prop |>\n  visualize() +\n  shade_p_value(obs_stat = p_hat, direction = \"both\")"},{"path":"appendixB.html","id":"calculate-p-value-1","chapter":"B Inference Examples","heading":"Calculate \\(p\\)-value","text":"\\(p\\)-value 0.114 fail reject null hypothesis 5% level.","code":"\npvalue <- null_distn_one_prop |>\n  get_pvalue(obs_stat = p_hat, direction = \"both\")\npvalue# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.1136"},{"path":"appendixB.html","id":"bootstrapping-for-confidence-interval-1","chapter":"B Inference Examples","heading":"Bootstrapping for confidence interval","text":"can also create confidence interval unknown population parameter \\(\\pi\\) using sample data. , use bootstrapping, involvessampling replacement original sample 100 survey respondents repeating process 10,000 times,calculating proportion successes 10,000 bootstrap samples created Step 1,combining bootstrap statistics calculated Step 2 boot_distn object,identifying 2.5th 97.5th percentiles distribution (corresponding 5% significance level chosen) find 95% confidence interval \\(\\pi\\), andinterpret confidence interval context problem.Just use mean function calculating mean numerical variable, can also use compute proportion successes categorical variable specify calling “success” ==. (Think formula calculating mean R handles logical statements satisfy == \"satisfied\" must true.)see 0.80 contained confidence interval plausible value \\(\\pi\\) (unknown population proportion). matches hypothesis test results failing reject null hypothesis.Interpretation: 95% confident true proportion customers satisfied service receive 0.64 0.81.","code":"\nboot_distn_one_prop <- elec |>\n  specify(response = satisfy, success = \"satisfied\") |>\n  generate(reps = 10000) |>\n  calculate(stat = \"prop\")\nci <- boot_distn_one_prop |>\n  get_ci()\nci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1     0.64     0.81\nboot_distn_one_prop |>\n  visualize() +\n  shade_ci(endpoints = ci)"},{"path":"appendixB.html","id":"traditional-methods-1","chapter":"B Inference Examples","heading":"B.3.5 Traditional methods","text":"","code":""},{"path":"appendixB.html","id":"check-conditions-1","chapter":"B Inference Examples","heading":"Check conditions","text":"Remember order use shortcut (formula-based, theoretical) approach, need check conditions met.Independent observations: observations collected independently.\ncases selected independently random sampling condition met.Independent observations: observations collected independently.cases selected independently random sampling condition met.Approximately normal: number expected successes expected failures least 10.\ncondition met since 73 27 greater 10.Approximately normal: number expected successes expected failures least 10.condition met since 73 27 greater 10.","code":""},{"path":"appendixB.html","id":"test-statistic-1","chapter":"B Inference Examples","heading":"Test statistic","text":"test statistic random variable based sample data. , want look way estimate population proportion \\(\\pi\\). good guess sample proportion \\(\\hat{P}\\). Recall sample proportion actually random variable vary different samples (theoretically, ) collected. looking see likely us observed sample proportion \\(\\hat{p}_{obs} = 0.73\\) extreme assuming population proportion 0.80 (assuming null hypothesis true). conditions met assuming \\(H_0\\) true, can standardize original test statistic \\(\\hat{P}\\) \\(Z\\) statistic follows \\(N(0, 1)\\) distribution.\\[ Z =\\dfrac{ \\hat{P} - p_0}{\\sqrt{\\dfrac{p_0(1 - p_0)}{n} }} \\sim N(0, 1) \\]","code":""},{"path":"appendixB.html","id":"observed-test-statistic-1","chapter":"B Inference Examples","heading":"Observed test statistic","text":"one compute observed test statistic “hand” plugging observed values formula, focus set-problem understanding formula test statistic applies. calculation done R completeness though:see \\(z_{obs}\\) value around -1.75. observed sample proportion 0.73 1.75 standard errors hypothesized parameter value 0.8.","code":"\np_hat <- 0.73\np0 <- 0.8\nn <- 100\n(z_obs <- (p_hat - p0) / sqrt((p0 * (1 - p0)) / n))[1] -1.75"},{"path":"appendixB.html","id":"visualize-and-compute-p-value","chapter":"B Inference Examples","heading":"Visualize and compute \\(p\\)-value","text":"\\(p\\)-value—probability observing \\(z_{obs}\\) value -1.75 extreme (directions) null distribution—around 8%.Note also test directly using prop.test function.prop.test \\(\\chi^2\\) test matches exactly expect: \\(x^2_{obs} = 3.06 = (-1.75)^2 = (z_{obs})^2\\) \\(p\\)-values focusing two-tailed test.Note 95 percent confidence interval given matches well one calculated using bootstrapping.","code":"\nelec |>\n  specify(response = satisfy, success = \"satisfied\") |>\n  hypothesize(null = \"point\", p = 0.8) |>\n  assume(distribution = \"z\") |>\n  visualize() +\n  shade_p_value(obs_stat = z_obs, direction = \"both\")\n2 * pnorm(z_obs)[1] 0.0801\nprop.test(\n  x = table(elec$satisfy),\n  n = length(elec$satisfy),\n  alternative = \"two.sided\",\n  p = 0.8,\n  correct = FALSE\n)\n    1-sample proportions test without continuity correction\n\ndata:  table(elec$satisfy), null probability 0.8\nX-squared = 3, df = 1, p-value = 0.08\nalternative hypothesis: true p is not equal to 0.8\n95 percent confidence interval:\n 0.636 0.807\nsample estimates:\n   p \n0.73 "},{"path":"appendixB.html","id":"state-conclusion-1","chapter":"B Inference Examples","heading":"State conclusion","text":", therefore, sufficient evidence reject null hypothesis. initial guess observed sample proportion statistically different hypothesized proportion invalidated. Based sample, evidence proportion customers large electric utility satisfied service receive different 0.80, 5% level.","code":""},{"path":"appendixB.html","id":"comparing-results-1","chapter":"B Inference Examples","heading":"B.3.6 Comparing results","text":"Observing bootstrap distribution null distribution created, makes quite bit sense results similar traditional non-traditional methods terms \\(p\\)-value confidence interval since distributions look similar normal distributions. conditions also met leads us better guess using methods whether traditional (formula-based) non-traditional (computational-based) lead similar results.","code":""},{"path":"appendixB.html","id":"two-proportions","chapter":"B Inference Examples","heading":"B.4 Two proportions","text":"","code":""},{"path":"appendixB.html","id":"problem-statement-2","chapter":"B Inference Examples","heading":"B.4.1 Problem statement","text":"2010 survey asked 827 randomly sampled registered voters\nCalifornia “support? oppose? Drilling oil natural gas Coast \nCalifornia? know enough say?” Conduct hypothesis test determine data\nprovide strong evidence proportion college\ngraduates opinion issue \ndifferent non-college graduates. (Tweaked bit Diez, Barr, Çetinkaya-Rundel 2014 [Chapter 6])","code":""},{"path":"appendixB.html","id":"competing-hypotheses-2","chapter":"B Inference Examples","heading":"B.4.2 Competing hypotheses","text":"","code":""},{"path":"appendixB.html","id":"in-words-2","chapter":"B Inference Examples","heading":"In words","text":"Null hypothesis: association opinion drilling college degree registered California voters 2010.Null hypothesis: association opinion drilling college degree registered California voters 2010.Alternative hypothesis: association opinion drilling college degree registered California voters 2010.Alternative hypothesis: association opinion drilling college degree registered California voters 2010.","code":""},{"path":"appendixB.html","id":"another-way-in-words","chapter":"B Inference Examples","heading":"Another way in words","text":"Null hypothesis: probability Californian voter 2010 opinion drilling college graduate non-college graduate.Null hypothesis: probability Californian voter 2010 opinion drilling college graduate non-college graduate.Alternative hypothesis: parameter probabilities different.Alternative hypothesis: parameter probabilities different.","code":""},{"path":"appendixB.html","id":"in-symbols-with-annotations-2","chapter":"B Inference Examples","heading":"In symbols (with annotations)","text":"\\(H_0: \\pi_{college} = \\pi_{\\_college}\\) \\(H_0: \\pi_{college} - \\pi_{\\_college} = 0\\), \\(\\pi\\) represents probability opinion drilling.\\(H_A: \\pi_{college} - \\pi_{\\_college} \\ne 0\\)","code":""},{"path":"appendixB.html","id":"set-alpha-2","chapter":"B Inference Examples","heading":"Set \\(\\alpha\\)","text":"’s important set significance level starting testing using data. Let’s set significance level 5% .","code":""},{"path":"appendixB.html","id":"exploring-the-sample-data-2","chapter":"B Inference Examples","heading":"B.4.3 Exploring the sample data","text":"Observe college graduates, proportion 104/(104 + 334) = 0.237 opinion drilling. hand, non-college graduates, proportion 131/(131 + 258) = 0.337 opinion drilling, whereas difference proportions 0.237 - 0.337 = -0.099.Let’s visualize barchart. However, first reverse order levels categorical variable response using fct_rev() function forcats package. default ordering levels factor alphanumeric. However, interested proportions opinion opinion. Thus need reverse default alphanumeric order.","code":"\noffshore <- read_csv(\"https://moderndive.com/data/offshore.csv\")\ncounts <- offshore |> tabyl(college_grad, response)\ncounts college_grad no opinion opinion\n           no        131     258\n          yes        104     334\noffshore <- offshore |>\n  mutate(response = fct_rev(response))\n\nggplot(offshore, aes(x = college_grad, fill = response)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"College grad?\", y = \"Proportion with no opinion on drilling\") +\n  coord_flip()"},{"path":"appendixB.html","id":"guess-about-statistical-significance-2","chapter":"B Inference Examples","heading":"Guess about statistical significance","text":"looking see difference exists size bars corresponding opinion plot. Based solely plot, little reason believe difference exists since bars seem size, …’s important use statistics see difference actually statistically significant!","code":""},{"path":"appendixB.html","id":"non-traditional-methods-2","chapter":"B Inference Examples","heading":"B.4.4 Non-traditional methods","text":"","code":""},{"path":"appendixB.html","id":"collecting-summary-info","chapter":"B Inference Examples","heading":"Collecting summary info","text":"observed statistic ","code":"\nd_hat <- offshore |>\n  specify(response ~ college_grad, success = \"no opinion\") |>\n  calculate(stat = \"diff in props\", order = c(\"yes\", \"no\"))\nd_hatResponse: response (factor)\nExplanatory: college_grad (factor)\n# A tibble: 1 × 1\n        stat\n       <dbl>\n1 -0.0993180"},{"path":"appendixB.html","id":"randomization-for-hypothesis-test","chapter":"B Inference Examples","heading":"Randomization for hypothesis test","text":"order ascertain observed sample proportion opinion college graduates 0.237 statistically different observed sample proportion opinion non-college graduates 0.337, need account sample sizes. Note ascertaining observed difference sample proportions -0.099 statistically different 0. also need determine process replicates original group sizes 389 438 selected.can use idea randomization testing (also known permutation testing) simulate population sample came (two groups different sizes) generate samples using shuffling simulated population account sampling variability.can next use distribution observe \\(p\\)-value. Recall two-tailed test looking values greater equal 0.099 less equal -0.099 \\(p\\)-value.","code":"\nset.seed(2018)\nnull_distn_two_props <- offshore |>\n  specify(response ~ college_grad, success = \"no opinion\") |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 10000) |>\n  calculate(stat = \"diff in props\", order = c(\"yes\", \"no\"))\nnull_distn_two_props |> visualize()\nnull_distn_two_props |>\n  visualize() +\n  shade_p_value(obs_stat = d_hat, direction = \"both\")"},{"path":"appendixB.html","id":"calculate-p-value-2","chapter":"B Inference Examples","heading":"Calculate \\(p\\)-value","text":"\\(p\\)-value 0.002 reject null hypothesis 5% level. can also see histogram far tails null distribution.","code":"\npvalue <- null_distn_two_props |>\n  get_pvalue(obs_stat = d_hat, direction = \"two_sided\")\npvalue# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.0024"},{"path":"appendixB.html","id":"bootstrapping-for-confidence-interval-2","chapter":"B Inference Examples","heading":"Bootstrapping for confidence interval","text":"can also create confidence interval unknown population parameter \\(\\pi_{college} - \\pi_{\\_college}\\) using sample data bootstrapping.see 0 contained confidence interval plausible value \\(\\pi_{college} - \\pi_{\\_college}\\) (unknown population parameter). matches hypothesis test results rejecting null hypothesis. Since zero plausible value population parameter, evidence proportion college graduates California opinion drilling different non-college graduates.Interpretation: 95% confident true proportion non-college graduates opinion offshore drilling California 0.16 dollars smaller 0.04 dollars smaller college graduates.","code":"\nboot_distn_two_props <- offshore |>\n  specify(response ~ college_grad, success = \"no opinion\") |>\n  generate(reps = 10000) |>\n  calculate(stat = \"diff in props\", order = c(\"yes\", \"no\"))\nci <- boot_distn_two_props |>\n  get_ci()\nci# A tibble: 1 × 2\n   lower_ci   upper_ci\n      <dbl>      <dbl>\n1 -0.160030 -0.0379112\nboot_distn_two_props |>\n  visualize() +\n  shade_ci(endpoints = ci)"},{"path":"appendixB.html","id":"traditional-methods-2","chapter":"B Inference Examples","heading":"B.4.5 Traditional methods","text":"","code":""},{"path":"appendixB.html","id":"check-conditions-2","chapter":"B Inference Examples","heading":"Check conditions","text":"Remember order use short-cut (formula-based, theoretical) approach, need check conditions met.Independent observations: case selected must independent cases selected.\ncondition met since cases selected random observe.Independent observations: case selected must independent cases selected.condition met since cases selected random observe.Sample size: number pooled successes pooled failures must least 10 group.\nneed first figure pooled success rate: \\[\\hat{p}_{obs} = \\dfrac{131 + 104}{827} = 0.28.\\] now determine expected (pooled) success failure counts:\n\\(0.28 \\cdot (131 + 258) = 108.92\\), \\(0.72 \\cdot (131 + 258) = 280.08\\)\n\\(0.28 \\cdot (104 + 334) = 122.64\\), \\(0.72 \\cdot (104 + 334) = 315.36\\)Sample size: number pooled successes pooled failures must least 10 group.need first figure pooled success rate: \\[\\hat{p}_{obs} = \\dfrac{131 + 104}{827} = 0.28.\\] now determine expected (pooled) success failure counts:\\(0.28 \\cdot (131 + 258) = 108.92\\), \\(0.72 \\cdot (131 + 258) = 280.08\\)\\(0.28 \\cdot (104 + 334) = 122.64\\), \\(0.72 \\cdot (104 + 334) = 315.36\\)Independent selection samples: cases paired meaningful way.\nreason suspect college graduate selected relationship non-college graduate selected.Independent selection samples: cases paired meaningful way.reason suspect college graduate selected relationship non-college graduate selected.","code":""},{"path":"appendixB.html","id":"test-statistic-2","chapter":"B Inference Examples","heading":"B.4.6 Test statistic","text":"test statistic random variable based sample data. , interested seeing observed difference sample proportions corresponding opinion drilling (\\(\\hat{p}_{college, obs} - \\hat{p}_{\\_college, obs}\\) = -0.092) statistically different 0. Assuming conditions met null hypothesis true, can use standard normal distribution standardize difference sample proportions (\\(\\hat{P}_{college} - \\hat{P}_{\\_college}\\)) using standard error \\(\\hat{P}_{college} - \\hat{P}_{\\_college}\\) pooled estimate:\\[ Z =\\dfrac{ (\\hat{P}_{college} - \\hat{P}_{no_college}) - 0}{\\sqrt{\\dfrac{\\hat{P}(1 - \\hat{P})}{n_1} + \\dfrac{\\hat{P}(1 - \\hat{P})}{n_2} }} \\sim N(0, 1) \\] \\(\\hat{P} = \\dfrac{\\text{total number successes} }{ \\text{total number cases}}.\\)","code":""},{"path":"appendixB.html","id":"observed-test-statistic-2","chapter":"B Inference Examples","heading":"Observed test statistic","text":"one compute observed test statistic “hand”, focus set-problem understanding formula test statistic applies. can use prop.test function perform analysis us.observed difference sample proportions 3.16 standard deviations smaller 0.\\(p\\)-value—probability observing \\(Z\\) value -3.16 extreme null distribution—0.0016. can also calculated R directly:","code":"\nz_hat <- offshore |>\n  specify(response ~ college_grad, success = \"no opinion\") |>\n  calculate(stat = \"z\", order = c(\"yes\", \"no\"))\nz_hatResponse: response (factor)\nExplanatory: college_grad (factor)\n# A tibble: 1 × 1\n      stat\n     <dbl>\n1 -3.16081\n2 * pnorm(-3.16, lower.tail = TRUE)[1] 0.00158"},{"path":"appendixB.html","id":"state-conclusion-2","chapter":"B Inference Examples","heading":"B.4.7 State conclusion","text":", therefore, sufficient evidence reject null hypothesis. initial guess statistically significant difference exist proportions opinion offshore drilling college educated non-college educated Californians validated. evidence suggest dependency college graduation position offshore drilling Californians.","code":""},{"path":"appendixB.html","id":"comparing-results-2","chapter":"B Inference Examples","heading":"B.4.8 Comparing results","text":"Observing bootstrap distribution null distribution created, makes quite bit sense results similar traditional non-traditional methods terms \\(p\\)-value confidence interval since distributions look similar normal distributions. conditions met since number pairs small, sample data highly skewed. Using methods whether traditional (formula-based) non-traditional (computational-based) lead similar results.","code":""},{"path":"appendixB.html","id":"two-means-independent-samples","chapter":"B Inference Examples","heading":"B.5 Two means (independent samples)","text":"","code":""},{"path":"appendixB.html","id":"problem-statement-3","chapter":"B Inference Examples","heading":"B.5.1 Problem statement","text":"Average income varies one region country \nanother, often reflects lifestyles regional living expenses. Suppose new graduate\nconsidering job two locations, Cleveland, OH Sacramento, CA, wants see\nwhether average income one cities higher . like conduct\nhypothesis test based two randomly selected samples 2000 Census. (Tweaked bit Diez, Barr, Çetinkaya-Rundel 2014 [Chapter 5])","code":""},{"path":"appendixB.html","id":"competing-hypotheses-3","chapter":"B Inference Examples","heading":"B.5.2 Competing hypotheses","text":"","code":""},{"path":"appendixB.html","id":"in-words-3","chapter":"B Inference Examples","heading":"In words","text":"Null hypothesis: association income location (Cleveland, OH Sacramento, CA).Alternative hypothesis: association income location (Cleveland, OH Sacramento, CA).","code":""},{"path":"appendixB.html","id":"another-way-in-words-1","chapter":"B Inference Examples","heading":"Another way in words","text":"Null hypothesis: mean income cities.Alternative hypothesis: mean income different two cities.","code":""},{"path":"appendixB.html","id":"in-symbols-with-annotations-3","chapter":"B Inference Examples","heading":"In symbols (with annotations)","text":"\\(H_0: \\mu_{sac} = \\mu_{cle}\\) \\(H_0: \\mu_{sac} - \\mu_{cle} = 0\\), \\(\\mu\\) represents average income.\\(H_A: \\mu_{sac} - \\mu_{cle} \\ne 0\\)","code":""},{"path":"appendixB.html","id":"set-alpha-3","chapter":"B Inference Examples","heading":"Set \\(\\alpha\\)","text":"’s important set significance level starting testing using data. Let’s set significance level 5% .","code":""},{"path":"appendixB.html","id":"exploring-the-sample-data-3","chapter":"B Inference Examples","heading":"B.5.3 Exploring the sample data","text":"boxplot also shows mean group highlighted red dots.","code":"\ncle_sac <- read.delim(\"https://moderndive.com/data/cleSac.txt\") |>\n  rename(\n    metro_area = Metropolitan_area_Detailed,\n    income = Total_personal_income\n  ) |>\n  na.omit()\ninc_summ <- cle_sac |>\n  group_by(metro_area) |>\n  summarize(\n    sample_size = n(),\n    mean = mean(income),\n    sd = sd(income),\n    minimum = min(income),\n    lower_quartile = quantile(income, 0.25),\n    median = median(income),\n    upper_quartile = quantile(income, 0.75),\n    max = max(income)\n  )\nkable(inc_summ) |>\n  kable_styling(\n    font_size = ifelse(is_latex_output(), 10, 16),\n    latex_options = c(\"hold_position\")\n  )\nggplot(cle_sac, aes(x = metro_area, y = income)) +\n  geom_boxplot() +\n  stat_summary(fun = \"mean\", geom = \"point\", color = \"red\")"},{"path":"appendixB.html","id":"guess-about-statistical-significance-3","chapter":"B Inference Examples","heading":"Guess about statistical significance","text":"looking see difference exists mean income two levels explanatory variable. Based solely boxplot, reason believe difference exists. distributions income seem similar means fall roughly place.","code":""},{"path":"appendixB.html","id":"non-traditional-methods-3","chapter":"B Inference Examples","heading":"B.5.4 Non-traditional methods","text":"","code":""},{"path":"appendixB.html","id":"collecting-summary-info-1","chapter":"B Inference Examples","heading":"Collecting summary info","text":"now compute observed statistic:","code":"\nd_hat <- cle_sac |>\n  specify(income ~ metro_area) |>\n  calculate(\n    stat = \"diff in means\",\n    order = c(\"Sacramento_ CA\", \"Cleveland_ OH\")\n  )\nd_hatResponse: income (numeric)\nExplanatory: metro_area (factor)\n# A tibble: 1 × 1\n     stat\n    <dbl>\n1 4960.48"},{"path":"appendixB.html","id":"randomization-for-hypothesis-test-1","chapter":"B Inference Examples","heading":"Randomization for hypothesis test","text":"order look see observed sample mean Sacramento 27467.066 statistically different Cleveland 32427.543, need account sample sizes. Note looking see \\(\\bar{x}_{sac} - \\bar{x}_{cle}\\) statistically different 0. also need determine process replicates original group sizes 212 175 selected.can use idea randomization testing (also known permutation testing) simulate population sample came (two groups different sizes) generate samples using shuffling simulated population account sampling variability.can next use distribution observe \\(p\\)-value. Recall two-tailed test looking values greater equal 4960.477 less equal -4960.477 \\(p\\)-value.","code":"\nset.seed(2018)\nnull_distn_two_means <- cle_sac |>\n  specify(income ~ metro_area) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 10000) |>\n  calculate(\n    stat = \"diff in means\",\n    order = c(\"Sacramento_ CA\", \"Cleveland_ OH\")\n  )\nnull_distn_two_means |> visualize()\nnull_distn_two_means |>\n  visualize() +\n  shade_p_value(obs_stat = d_hat, direction = \"both\")"},{"path":"appendixB.html","id":"calculate-p-value-3","chapter":"B Inference Examples","heading":"Calculate \\(p\\)-value","text":"\\(p\\)-value 0.126 fail reject null hypothesis 5% level. can also see histogram far tail null distribution.","code":"\npvalue <- null_distn_two_means |>\n  get_pvalue(obs_stat = d_hat, direction = \"both\")\npvalue# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.1262"},{"path":"appendixB.html","id":"bootstrapping-for-confidence-interval-3","chapter":"B Inference Examples","heading":"Bootstrapping for confidence interval","text":"can also create confidence interval unknown population parameter \\(\\mu_{sac} - \\mu_{cle}\\) using sample data bootstrapping. bootstrap samples replacement instead shuffling. done setting type argument generate function “bootstrap”.see 0 contained confidence interval plausible value \\(\\mu_{sac} - \\mu_{cle}\\) (unknown population parameter). matches hypothesis test results failing reject null hypothesis. Since zero plausible value population parameter, evidence Sacramento incomes different Cleveland incomes.Interpretation: 95% confident true mean yearly income living Sacramento 1359.5 dollars smaller 11499.69 dollars higher Cleveland.Note: also use null distribution based randomization shift center \\(\\bar{x}_{sac} - \\bar{x}_{cle} = \\$4960.48\\) instead 0 calculate percentiles. confidence interval produced via method comparable one done using bootstrapping .","code":"\nboot_distn_two_means <- cle_sac |>\n  specify(income ~ metro_area) |>\n  generate(reps = 10000) |>\n  calculate(\n    stat = \"diff in means\",\n    order = c(\"Sacramento_ CA\", \"Cleveland_ OH\")\n  )\nci <- boot_distn_two_means |>\n  get_ci()\nci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1 -1359.50  11499.7\nboot_distn_two_means |>\n  visualize() +\n  shade_ci(endpoints = ci)"},{"path":"appendixB.html","id":"traditional-methods-3","chapter":"B Inference Examples","heading":"B.5.5 Traditional methods","text":"","code":""},{"path":"appendixB.html","id":"check-conditions-3","chapter":"B Inference Examples","heading":"Check conditions","text":"Remember order use short-cut (formula-based, theoretical) approach, need check conditions met.Independent observations: observations independent groups.\nmetro_area variable met since cases randomly selected city.Independent observations: observations independent groups.metro_area variable met since cases randomly selected city.Approximately normal: distribution response group normal sample sizes least 30.Approximately normal: distribution response group normal sample sizes least 30.reason doubt normality assumption since histograms show deviation normal model fitting data well group. sample sizes group greater 100 though assumptions still apply.Independent samples: samples collected without natural pairing.\nmention relationship selected Cleveland Sacramento.Independent samples: samples collected without natural pairing.mention relationship selected Cleveland Sacramento.","code":"\nggplot(cle_sac, aes(x = income)) +\n  geom_histogram(color = \"white\", binwidth = 20000) +\n  facet_wrap(~metro_area)"},{"path":"appendixB.html","id":"test-statistic-3","chapter":"B Inference Examples","heading":"B.5.6 Test statistic","text":"test statistic random variable based sample data. , interested seeing observed difference sample means (\\(\\bar{x}_{sac, obs} - \\bar{x}_{cle, obs}\\) = 4960.477) statistically different 0. Assuming conditions met null hypothesis true, can use \\(t\\) distribution standardize difference sample means (\\(\\bar{X}_{sac} - \\bar{X}_{cle}\\)) using approximate standard error \\(\\bar{X}_{sac} - \\bar{X}_{cle}\\) (invoking \\(S_{sac}\\) \\(S_{cle}\\) estimates unknown \\(\\sigma_{sac}\\) \\(\\sigma_{cle}\\)).\\[ T =\\dfrac{ (\\bar{X}_1 - \\bar{X}_2) - 0}{ \\sqrt{\\dfrac{S_1^2}{n_1} + \\dfrac{S_2^2}{n_2}}  } \\sim t (df = min(n_1 - 1, n_2 - 1)) \\] 1 = Sacramento 2 = Cleveland \\(S_1^2\\) \\(S_2^2\\) sample variance incomes cities, respectively, \\(n_1 = 175\\) Sacramento \\(n_2 = 212\\) Cleveland.","code":""},{"path":"appendixB.html","id":"observed-test-statistic-3","chapter":"B Inference Examples","heading":"Observed test statistic","text":"Note also (ALMOST) test directly using t.test function. x y arguments expected numeric vectors ’ll need appropriately filter datasets.see observed test statistic value around -1.5. one compute observed test statistic “hand”, focus set-problem understanding formula test statistic applies.","code":"\ncle_sac |>\n  specify(income ~ metro_area) |>\n  calculate(\n    stat = \"t\",\n    order = c(\"Cleveland_ OH\", \"Sacramento_ CA\")\n  )Response: income (numeric)\nExplanatory: metro_area (factor)\n# A tibble: 1 × 1\n      stat\n     <dbl>\n1 -1.50062"},{"path":"appendixB.html","id":"compute-p-value-1","chapter":"B Inference Examples","heading":"B.5.7 Compute \\(p\\)-value","text":"\\(p\\)-value—probability observing \\(t_{174}\\) value -1.501 extreme (directions) null distribution—0.13. can also calculated R directly:can also approximate using standard normal curve:","code":"\n2 * pt(-1.501, df = min(212 - 1, 175 - 1), lower.tail = TRUE)[1] 0.135\n2 * pnorm(-1.501)[1] 0.133"},{"path":"appendixB.html","id":"state-conclusion-3","chapter":"B Inference Examples","heading":"B.5.8 State conclusion","text":", therefore, sufficient evidence reject null hypothesis. initial guess statistically significant difference existing means backed statistical analysis. evidence suggest true mean income differs Cleveland, OH Sacramento, CA based data.","code":""},{"path":"appendixB.html","id":"comparing-results-3","chapter":"B Inference Examples","heading":"B.5.9 Comparing results","text":"Observing bootstrap distribution null distribution created, makes quite bit sense results similar traditional non-traditional methods terms \\(p\\)-value confidence interval since distributions look similar normal distributions. conditions also met leads us better guess using methods whether traditional (formula-based) non-traditional (computational-based) lead similar results.","code":""},{"path":"appendixB.html","id":"two-means-paired-samples","chapter":"B Inference Examples","heading":"B.6 Two means (paired samples)","text":"","code":""},{"path":"appendixB.html","id":"problem-statement-4","chapter":"B Inference Examples","heading":"Problem statement","text":"Trace metals drinking water affect flavor unusually high concentration can pose health hazard. Ten pairs data taken measuring zinc concentration bottom water surface water 10 randomly selected locations stretch river. data suggest true average concentration surface water smaller bottom water? (Note units given.) [Tweaked bit Penn State’s STAT 500 course.]","code":""},{"path":"appendixB.html","id":"competing-hypotheses-4","chapter":"B Inference Examples","heading":"B.6.1 Competing hypotheses","text":"","code":""},{"path":"appendixB.html","id":"in-words-4","chapter":"B Inference Examples","heading":"In words","text":"Null hypothesis: mean concentration bottom water surface water different paired locations.Alternative hypothesis: mean concentration surface water smaller bottom water different paired locations.","code":""},{"path":"appendixB.html","id":"in-symbols-with-annotations-4","chapter":"B Inference Examples","heading":"In symbols (with annotations)","text":"\\(H_0: \\mu_{diff} = 0\\), \\(\\mu_{diff}\\) represents mean difference concentration surface water minus bottom water.\\(H_A: \\mu_{diff} < 0\\)","code":""},{"path":"appendixB.html","id":"set-alpha-4","chapter":"B Inference Examples","heading":"Set \\(\\alpha\\)","text":"’s important set significance level starting testing using data. Let’s set significance level 5% .","code":""},{"path":"appendixB.html","id":"exploring-the-sample-data-4","chapter":"B Inference Examples","heading":"B.6.2 Exploring the sample data","text":"want look differences surface - bottom location:Next calculate mean difference observed statistic:histogram also shows distribution pair_diff.","code":"\nzinc_tidy <- read_csv(\"https://moderndive.com/data/zinc_tidy.csv\")\nzinc_diff <- zinc_tidy |>\n  group_by(loc_id) |>\n  summarize(pair_diff = diff(concentration)) |>\n  ungroup()\nd_hat <- zinc_diff |>\n  specify(response = pair_diff) |>\n  calculate(stat = \"mean\")\nd_hatResponse: pair_diff (numeric)\n# A tibble: 1 × 1\n     stat\n    <dbl>\n1 -0.0804\nggplot(zinc_diff, aes(x = pair_diff)) +\n  geom_histogram(binwidth = 0.04, color = \"white\")"},{"path":"appendixB.html","id":"guess-about-statistical-significance-4","chapter":"B Inference Examples","heading":"Guess about statistical significance","text":"looking see sample paired mean difference -0.08 statistically less 0. seem quite close, small number pairs . Let’s guess fail reject null hypothesis.","code":""},{"path":"appendixB.html","id":"non-traditional-methods-4","chapter":"B Inference Examples","heading":"B.6.3 Non-traditional methods","text":"","code":""},{"path":"appendixB.html","id":"bootstrapping-for-hypothesis-test-1","chapter":"B Inference Examples","heading":"Bootstrapping for hypothesis test","text":"order look see observed sample mean difference \\(\\bar{x}_{diff} = -0.08\\) statistically less 0, need account number pairs. also need determine process replicates paired data selected way similar calculated original difference sample means.Treating differences data interest, next use process bootstrapping build simulated samples calculate mean bootstrap samples. hypothesize mean difference zero.process similar comparing One Mean example seen , using differences two groups single sample hypothesized mean difference 0.can next use distribution observe \\(p\\)-value. Recall left-tailed test looking values less equal -0.08 \\(p\\)-value.","code":"\nset.seed(2018)\nnull_distn_paired_means <- zinc_diff |>\n  specify(response = pair_diff) |>\n  hypothesize(null = \"point\", mu = 0) |>\n  generate(reps = 10000) |>\n  calculate(stat = \"mean\")\nnull_distn_paired_means |> visualize()\nnull_distn_paired_means |>\n  visualize() +\n  shade_p_value(obs_stat = d_hat, direction = \"less\")"},{"path":"appendixB.html","id":"calculate-p-value-4","chapter":"B Inference Examples","heading":"Calculate \\(p\\)-value","text":"\\(p\\)-value essentially 0 reject null hypothesis 5% level. can also see histogram far left tail null distribution.","code":"\npvalue <- null_distn_paired_means |>\n  get_pvalue(obs_stat = d_hat, direction = \"less\")Warning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\npvalue# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0"},{"path":"appendixB.html","id":"bootstrapping-for-confidence-interval-4","chapter":"B Inference Examples","heading":"Bootstrapping for confidence interval","text":"can also create confidence interval unknown population parameter \\(\\mu_{diff}\\) using sample data (calculated differences) bootstrapping. similar bootstrapping done one sample mean case, except now data differences instead raw numerical data.\nNote code identical pipeline shown hypothesis test except hypothesize() function called.see 0 contained confidence interval plausible value \\(\\mu_{diff}\\) (unknown population parameter). matches hypothesis test results rejecting null hypothesis. Since zero plausible value population parameter since entire confidence interval falls zero, evidence surface zinc concentration levels lower, average, bottom level zinc concentrations.Interpretation: 95% confident true mean zinc concentration surface 0.11 units smaller 0.05 units smaller bottom.","code":"\nboot_distn_paired_means <- zinc_diff |>\n  specify(response = pair_diff) |>\n  generate(reps = 10000) |>\n  calculate(stat = \"mean\")\nci <- boot_distn_paired_means |>\n  get_ci()\nci# A tibble: 1 × 2\n  lower_ci   upper_ci\n     <dbl>      <dbl>\n1  -0.1116 -0.0501975\nboot_distn_paired_means |>\n  visualize() +\n  shade_ci(endpoints = ci)"},{"path":"appendixB.html","id":"traditional-methods-4","chapter":"B Inference Examples","heading":"B.6.4 Traditional methods","text":"","code":""},{"path":"appendixB.html","id":"check-conditions-4","chapter":"B Inference Examples","heading":"Check conditions","text":"Remember order use shortcut (formula-based, theoretical) approach, need check conditions met.Independent observations: observations among pairs independent.\nlocations selected independently random sampling condition met.Independent observations: observations among pairs independent.locations selected independently random sampling condition met.Approximately normal: distribution population differences normal number pairs least 30.\nhistogram show skew reason doubt population normal based sample. also 10 pairs fewer 30 needed. theory-based test may valid .Approximately normal: distribution population differences normal number pairs least 30.histogram show skew reason doubt population normal based sample. also 10 pairs fewer 30 needed. theory-based test may valid .","code":""},{"path":"appendixB.html","id":"test-statistic-4","chapter":"B Inference Examples","heading":"Test statistic","text":"test statistic random variable based sample data. , want look way estimate population mean difference \\(\\mu_{diff}\\). good guess sample mean difference \\(\\bar{X}_{diff}\\). Recall sample mean actually random variable vary different samples (theoretically, ) collected. looking see likely us observed sample mean \\(\\bar{x}_{diff} = -0.08\\) smaller assuming population mean difference 0 (assuming null hypothesis true). conditions met assuming \\(H_0\\) true, can “standardize” original test statistic \\(\\bar{X}_{diff}\\) \\(T\\) statistic follows \\(t\\) distribution degrees freedom equal \\(df = n - 1\\):\\[ T =\\dfrac{ \\bar{X}_{diff} - 0}{ S_{diff} / \\sqrt{n} } \\sim t (df = n - 1) \\]\\(S\\) represents standard deviation sample differences \\(n\\) number pairs.","code":""},{"path":"appendixB.html","id":"observed-test-statistic-4","chapter":"B Inference Examples","heading":"Observed test statistic","text":"one compute observed test statistic “hand”, focus set-problem understanding formula test statistic applies. can use t_test function differences perform analysis us.see \\(t_{obs}\\) value -4.864.","code":"\nt_test_results <- zinc_diff |>\n  t_test(\n    formula = pair_diff ~ NULL,\n    alternative = \"less\",\n    mu = 0\n  )\nt_test_results# A tibble: 1 × 7\n  statistic  t_df     p_value alternative estimate lower_ci   upper_ci\n      <dbl> <dbl>       <dbl> <chr>          <dbl>    <dbl>      <dbl>\n1  -4.86381     9 0.000445558 less         -0.0804     -Inf -0.0500982"},{"path":"appendixB.html","id":"compute-p-value-2","chapter":"B Inference Examples","heading":"Compute \\(p\\)-value","text":"\\(p\\)-value—probability observing \\(t_{obs}\\) value -4.864 less null distribution \\(t\\) 9 degrees freedom—0. can also calculated R directly:","code":"\npt(-4.8638, df = nrow(zinc_diff) - 1, lower.tail = TRUE)[1] 0.000446"},{"path":"appendixB.html","id":"state-conclusion-4","chapter":"B Inference Examples","heading":"State conclusion","text":", therefore, sufficient evidence reject null hypothesis. initial guess observed sample mean difference statistically less hypothesized mean 0 invalidated . Based sample, evidence mean concentration bottom water greater surface water different paired locations.","code":""},{"path":"appendixB.html","id":"comparing-results-4","chapter":"B Inference Examples","heading":"B.6.5 Comparing results","text":"Observing bootstrap distribution null distribution created, makes quite bit sense results similar traditional non-traditional methods terms \\(p\\)-value confidence interval since distributions look similar normal distributions. conditions met since number pairs small, sample data highly skewed. Using methods whether traditional (formula-based) non-traditional (computational-based) lead similar results .","code":""},{"path":"appendixC.html","id":"appendixC","chapter":"C Tips and Tricks","heading":"C Tips and Tricks","text":"\nNote: appendix still construction update\nmaterials First Edition Second Edition. \nlike contribute, please check us GitHub https://github.com/moderndive/moderndive_book.\n","code":""},{"path":"appendixC.html","id":"needed-packages-2","chapter":"C Tips and Tricks","heading":"Needed packages","text":"Let’s load packages needed chapter (assumes ’ve already installed ). Recall discussion Section 4.4 loading tidyverse package running library(tidyverse) loads following commonly used data science packages :ggplot2 data visualizationdplyr data wranglingtidyr converting data “tidy” formatreadr importing spreadsheet data RAs well advanced purrr, tibble, stringr, forcats packages.needed, read Section 1.3 information install load R packages.","code":"\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(janitor)\nlibrary(dygraphs)\nlibrary(nycflights23)"},{"path":"appendixC.html","id":"data-wrangling","chapter":"C Tips and Tricks","heading":"C.1 Data wrangling","text":"Section, address common data wrangling questions ’ve encountered student projects (shout Dr. Jenny Smetzer work setting !):C.1.1: Dealing missing valuesC.1.2: Reordering bars barplotC.1.3: Showing money axisC.1.4: Changing values inside cellsC.1.5: Converting numerical variable categorical oneC.1.6: Computing proportionsC.1.7: Dealing %, commas, $Let’s load example movies dataset, pare rows columns bit, show first 10 rows using slice().","code":"\nmovies_ex <- read_csv(\"https://moderndive.com/data/movies.csv\") %>%\n  filter(type %in% c(\"action\", \"comedy\", \"drama\", \"animated\", \"fantasy\", \"rom comedy\")) %>%\n  select(-over200)\n\nmovies_ex %>%\n  slice(1:10)# A tibble: 10 × 5\n   name               score rating type        millions\n   <chr>              <dbl> <chr>  <chr>          <dbl>\n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA      \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545  \n 3 A Man Apart      42.9000 R      action      26.2480 \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781  \n 5 Agent Cody Banks 57.9000 PG     action      47.8110 \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219  \n 7 American Wedding 50.7000 R      comedy     104.441  \n 8 Anger Management 62.6000 PG-13  comedy     134.404  \n 9 Anything Else    63.3000 R      rom comedy   3.21200\n10 Bad Boys II      38.1000 R      action     138.397  "},{"path":"appendixC.html","id":"appendix-missing-values","chapter":"C Tips and Tricks","heading":"C.1.1 Dealing with missing values","text":"see revenue millions value movie “2 Fast 2 Furious” NA (missing). following occurs computing median revenue:always think data value might missing missingness may mean. example, imagine conducting study effects smoking lung cancer lot patients’ data missing died lung cancer. just “sweep patients rug” ignore , clearly biasing results.statistical methods deal missing data beyond reach class. easiest thing remove missing cases, always least report reader , removing missing values may biasing results.can na.rm = TRUE argument like :decide want remove row missing data, can use filter function like :see “2 Fast 2 Furious” now gone.","code":"\nmovies_ex %>%\n  summarize(mean_profit = median(millions))# A tibble: 1 × 1\n  mean_profit\n        <dbl>\n1          NA\nmovies_ex %>%\n  summarize(mean_profit = median(millions, na.rm = TRUE))# A tibble: 1 × 1\n  mean_profit\n        <dbl>\n1     43.4270\nmovies_no_missing <- movies_ex %>%\n  filter(!is.na(millions))\n\nmovies_no_missing %>%\n  slice(1:10)# A tibble: 10 × 5\n   name               score rating type        millions\n   <chr>              <dbl> <chr>  <chr>          <dbl>\n 1 A Guy Thing      39.5    PG-13  rom comedy  15.545  \n 2 A Man Apart      42.9000 R      action      26.2480 \n 3 A Mighty Wind    79.9000 PG-13  comedy      17.781  \n 4 Agent Cody Banks 57.9000 PG     action      47.8110 \n 5 Alex & Emma      35.1000 PG-13  rom comedy  14.219  \n 6 American Wedding 50.7000 R      comedy     104.441  \n 7 Anger Management 62.6000 PG-13  comedy     134.404  \n 8 Anything Else    63.3000 R      rom comedy   3.21200\n 9 Bad Boys II      38.1000 R      action     138.397  \n10 Bad Santa        75.8000 R      comedy      59.5230 "},{"path":"appendixC.html","id":"appendix-reordering-bars","chapter":"C Tips and Tricks","heading":"C.1.2 Reordering bars in a barplot","text":"Let’s compute total revenue movie type plot barplot.Say want reorder categorical variable type bars show different order. can reorder bars manually defining order levels factor() command:want reorder type ascending order total_revenue, use reorder():want reorder type descending order total_revenue, just put\n- sign front -total_revenue reorder():advanced categorical variable (.e. factor) manipulations, check \nforcats package. Note: forcats anagram factors.","code":"\nrevenue_by_type <- movies_ex %>%\n  group_by(type) %>%\n  summarize(total_revenue = sum(millions))\nrevenue_by_type# A tibble: 6 × 2\n  type       total_revenue\n  <chr>              <dbl>\n1 action            NA    \n2 animated         561.306\n3 comedy          2286.81 \n4 drama            840.038\n5 fantasy          508.580\n6 rom comedy       492.282\nggplot(revenue_by_type, aes(x = type, y = total_revenue)) +\n  geom_col() +\n  labs(x = \"Movie genre\", y = \"Total box office revenue (in millions of $)\")\ntype_levels <- c(\"rom comedy\", \"action\", \"drama\", \"animated\", \"comedy\", \"fantasy\")\n\nrevenue_by_type <- revenue_by_type %>%\n  mutate(type = factor(type, levels = type_levels))\n\nggplot(revenue_by_type, aes(x = type, y = total_revenue)) +\n  geom_col() +\n  labs(x = \"Movie genre\", y = \"Total boxoffice revenue (in millions of $)\")\nrevenue_by_type <- revenue_by_type %>%\n  mutate(type = reorder(type, total_revenue))\n\nggplot(revenue_by_type, aes(x = type, y = total_revenue)) +\n  geom_col() +\n  labs(\n    x = \"Movie genre\", y = \"Total boxoffice revenue (in millions of $)\"\n  )\nrevenue_by_type <- revenue_by_type %>%\n  mutate(type = reorder(type, -total_revenue))\n\nggplot(revenue_by_type, aes(x = type, y = total_revenue)) +\n  geom_col() +\n  labs(\n    x = \"Movie genre\", y = \"Total boxoffice revenue (in millions of $)\"\n  )"},{"path":"appendixC.html","id":"appendix-money-on-axis","chapter":"C Tips and Tricks","heading":"C.1.3 Showing money on an axis","text":"Google “ggplot2 axis scale dollars” click first link search word “dollars”. ’ll find:","code":"\nmovies_ex <- movies_ex %>%\n  mutate(revenue = millions * 10^6)\n\nggplot(data = movies_ex, aes(x = rating, y = revenue)) +\n  geom_boxplot() +\n  labs(x = \"rating\", y = \"Revenue in $\", title = \"Profits for different movie ratings\")\n# Don't forget to load the scales package first!\nlibrary(scales)\n\nggplot(data = movies_ex, aes(x = rating, y = revenue)) +\n  geom_boxplot() +\n  labs(x = \"rating\", y = \"Revenue in $\", title = \"Profits for different movie ratings\") +\n  scale_y_continuous(labels = dollar)"},{"path":"appendixC.html","id":"appendix-changing-values","chapter":"C Tips and Tricks","heading":"C.1.4 Changing values inside cells","text":"rename() function dplyr package renames column/variable names. “rename” values inside cells particular column, need mutate() column using one three functions . might ones , three ’ve seen . examples, ’ll change values variable type.if_else()recode()case_when()","code":""},{"path":"appendixC.html","id":"if_else","chapter":"C Tips and Tricks","heading":"if_else()","text":"Switch instances rom comedy romantic comedy using if_else() dplyr package. particular row type == \"rom comedy\", return \"romantic comedy\", else return whatever originally type. Save everything new variable type_new:, return \"romantic comedy\" type \"rom comedy\" time overwrite original type variable:","code":"\nmovies_ex %>%\n  mutate(type_new = if_else(type == \"rom comedy\", \"romantic comedy\", type)) %>%\n  slice(1:10)# A tibble: 10 × 7\n   name               score rating type        millions    revenue type_new       \n   <chr>              <dbl> <chr>  <chr>          <dbl>      <dbl> <chr>          \n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA              NA  action         \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545    15545000  romantic comedy\n 3 A Man Apart      42.9000 R      action      26.2480   26247999  action         \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781    17781000  comedy         \n 5 Agent Cody Banks 57.9000 PG     action      47.8110   47811001  action         \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219    14219000  romantic comedy\n 7 American Wedding 50.7000 R      comedy     104.441   104441000  comedy         \n 8 Anger Management 62.6000 PG-13  comedy     134.404   134404010  comedy         \n 9 Anything Else    63.3000 R      rom comedy   3.21200   3212000. romantic comedy\n10 Bad Boys II      38.1000 R      action     138.397   138397000  action         \nmovies_ex %>%\n  mutate(type = if_else(type == \"rom comedy\", \"romantic comedy\", \"not romantic comedy\")) %>%\n  slice(1:10)# A tibble: 10 × 6\n   name               score rating type                 millions    revenue\n   <chr>              <dbl> <chr>  <chr>                   <dbl>      <dbl>\n 1 2 Fast 2 Furious 48.9000 PG-13  not romantic comedy  NA              NA \n 2 A Guy Thing      39.5    PG-13  romantic comedy      15.545    15545000 \n 3 A Man Apart      42.9000 R      not romantic comedy  26.2480   26247999 \n 4 A Mighty Wind    79.9000 PG-13  not romantic comedy  17.781    17781000 \n 5 Agent Cody Banks 57.9000 PG     not romantic comedy  47.8110   47811001 \n 6 Alex & Emma      35.1000 PG-13  romantic comedy      14.219    14219000 \n 7 American Wedding 50.7000 R      not romantic comedy 104.441   104441000 \n 8 Anger Management 62.6000 PG-13  not romantic comedy 134.404   134404010 \n 9 Anything Else    63.3000 R      romantic comedy       3.21200   3212000.\n10 Bad Boys II      38.1000 R      not romantic comedy 138.397   138397000 "},{"path":"appendixC.html","id":"recode","chapter":"C Tips and Tricks","heading":"recode()","text":"if_else() rather limited however. want “rename” type start uppercase? Use recode():","code":"\nmovies_ex %>%\n  mutate(type_new = recode(type,\n    \"action\" = \"Action\",\n    \"animated\" = \"Animated\",\n    \"comedy\" = \"Comedy\",\n    \"drama\" = \"Drama\",\n    \"fantasy\" = \"Fantasy\",\n    \"rom comedy\" = \"Romantic Comedy\"\n  )) %>%\n  slice(1:10)# A tibble: 10 × 7\n   name               score rating type        millions    revenue type_new       \n   <chr>              <dbl> <chr>  <chr>          <dbl>      <dbl> <chr>          \n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA              NA  Action         \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545    15545000  Romantic Comedy\n 3 A Man Apart      42.9000 R      action      26.2480   26247999  Action         \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781    17781000  Comedy         \n 5 Agent Cody Banks 57.9000 PG     action      47.8110   47811001  Action         \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219    14219000  Romantic Comedy\n 7 American Wedding 50.7000 R      comedy     104.441   104441000  Comedy         \n 8 Anger Management 62.6000 PG-13  comedy     134.404   134404010  Comedy         \n 9 Anything Else    63.3000 R      rom comedy   3.21200   3212000. Romantic Comedy\n10 Bad Boys II      38.1000 R      action     138.397   138397000  Action         "},{"path":"appendixC.html","id":"case_when","chapter":"C Tips and Tricks","heading":"case_when()","text":"case_when() little trickier, allows evaluate boolean operations using ==, >, >=, &, |, etc:","code":"\nmovies_ex %>%\n  mutate(\n    type_new =\n      case_when(\n        type == \"action\" & millions > 40 ~ \"Big budget action\",\n        type == \"rom comedy\" & millions < 40 ~ \"Small budget romcom\",\n        # Need this for everything else that aren't the two cases above:\n        TRUE ~ \"Rest\"\n      )\n  )# A tibble: 108 × 7\n   name               score rating type        millions    revenue type_new           \n   <chr>              <dbl> <chr>  <chr>          <dbl>      <dbl> <chr>              \n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA              NA  Rest               \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545    15545000  Small budget romcom\n 3 A Man Apart      42.9000 R      action      26.2480   26247999  Rest               \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781    17781000  Rest               \n 5 Agent Cody Banks 57.9000 PG     action      47.8110   47811001  Big budget action  \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219    14219000  Small budget romcom\n 7 American Wedding 50.7000 R      comedy     104.441   104441000  Rest               \n 8 Anger Management 62.6000 PG-13  comedy     134.404   134404010  Rest               \n 9 Anything Else    63.3000 R      rom comedy   3.21200   3212000. Small budget romcom\n10 Bad Boys II      38.1000 R      action     138.397   138397000  Big budget action  \n# ℹ 98 more rows"},{"path":"appendixC.html","id":"appendix-convert-numerical-categorical","chapter":"C Tips and Tricks","heading":"C.1.5 Converting a numerical variable to a categorical one","text":"Sometimes want turn numerical, continuous variable categorical variable. instance, wanted variable tells us movie made one hundred million dollars . say, can create binary variable, thing categorical variable 2 levels. can use mutate() function:want convert numerical variable categorical variable 2 levels? One way use cut() command. instance, , cut() score variable, recode 4 categories:0 - 40 = bad40.1 - 60 = -so60.1 - 80 = good80.1+ = greatWe set breaking points cutting numerical variable c(0, 40, 60, 80, 100) part, set labels bins labels = c(\"bad\", \"-\", \"good\", \"great\") part. action happens inside mutate() command, new categorical variable score_categ added data frame.options cut function:default, value exactly upper bound interval, ’s\nincluded lessor category (e.g. 60.0 ‘-’ ‘good’), \nflip , include argument right = FALSE.also R equally divide variable balanced\nnumber groups. example, specifying breaks = 3 create 3 groups \napproximately number values group.","code":"\nmovies_ex %>%\n  mutate(big_budget = millions > 100) %>%\n  slice(1:10)# A tibble: 10 × 7\n   name               score rating type        millions    revenue big_budget\n   <chr>              <dbl> <chr>  <chr>          <dbl>      <dbl> <lgl>     \n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA              NA  NA        \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545    15545000  FALSE     \n 3 A Man Apart      42.9000 R      action      26.2480   26247999  FALSE     \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781    17781000  FALSE     \n 5 Agent Cody Banks 57.9000 PG     action      47.8110   47811001  FALSE     \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219    14219000  FALSE     \n 7 American Wedding 50.7000 R      comedy     104.441   104441000  TRUE      \n 8 Anger Management 62.6000 PG-13  comedy     134.404   134404010  TRUE      \n 9 Anything Else    63.3000 R      rom comedy   3.21200   3212000. FALSE     \n10 Bad Boys II      38.1000 R      action     138.397   138397000  TRUE      \nmovies_ex %>%\n  mutate(score_categ = cut(score,\n    breaks = c(0, 40, 60, 80, 100),\n    labels = c(\"bad\", \"so-so\", \"good\", \"great\")\n  )) %>%\n  slice(1:10)# A tibble: 10 × 7\n   name               score rating type        millions    revenue score_categ\n   <chr>              <dbl> <chr>  <chr>          <dbl>      <dbl> <fct>      \n 1 2 Fast 2 Furious 48.9000 PG-13  action      NA              NA  so-so      \n 2 A Guy Thing      39.5    PG-13  rom comedy  15.545    15545000  bad        \n 3 A Man Apart      42.9000 R      action      26.2480   26247999  so-so      \n 4 A Mighty Wind    79.9000 PG-13  comedy      17.781    17781000  good       \n 5 Agent Cody Banks 57.9000 PG     action      47.8110   47811001  so-so      \n 6 Alex & Emma      35.1000 PG-13  rom comedy  14.219    14219000  bad        \n 7 American Wedding 50.7000 R      comedy     104.441   104441000  so-so      \n 8 Anger Management 62.6000 PG-13  comedy     134.404   134404010  good       \n 9 Anything Else    63.3000 R      rom comedy   3.21200   3212000. good       \n10 Bad Boys II      38.1000 R      action     138.397   138397000  bad        "},{"path":"appendixC.html","id":"appendix-prop","chapter":"C Tips and Tricks","heading":"C.1.6 Computing proportions","text":"using group_by() followed summarize() often case, rather mutate(). say compute total revenue millions movie rating type:Say within movie rating (G, PG, PG-13, R), want know proportion total_millions made movie type (animated, action, comedy, etc). can:example, 4 proportions corresponding R rated movies 0.596 + 0.142 + 0.213 + 0.0491 = 1.","code":"\nrating_by_type_millions <- movies_ex %>%\n  group_by(rating, type) %>%\n  summarize(millions = sum(millions)) %>%\n  arrange(rating, type)\n\nrating_by_type_millions# A tibble: 15 × 3\n# Groups:   rating [4]\n   rating type        millions\n   <chr>  <chr>          <dbl>\n 1 G      animated    495.594 \n 2 PG     action       47.8110\n 3 PG     animated     65.712 \n 4 PG     comedy      829.616 \n 5 PG     drama       160.873 \n 6 PG     fantasy     147.461 \n 7 PG-13  action       NA     \n 8 PG-13  comedy     1208.31  \n 9 PG-13  drama       306.26  \n10 PG-13  fantasy     361.119 \n11 PG-13  rom comedy  406.251 \n12 R      action     1044.82  \n13 R      comedy      248.876 \n14 R      drama       372.905 \n15 R      rom comedy   86.0310\nrating_by_type_millions %>%\n  group_by(rating) %>%\n  mutate(\n    # Compute a new column of the sum of millions split by rating:\n    total_millions = sum(millions),\n    # Compute the proportion within each rating:\n    prop = millions / total_millions\n  )# A tibble: 15 × 5\n# Groups:   rating [4]\n   rating type        millions total_millions       prop\n   <chr>  <chr>          <dbl>          <dbl>      <dbl>\n 1 G      animated    495.594         495.594  1        \n 2 PG     action       47.8110       1251.47   0.0382038\n 3 PG     animated     65.712        1251.47   0.0525077\n 4 PG     comedy      829.616        1251.47   0.662912 \n 5 PG     drama       160.873        1251.47   0.128547 \n 6 PG     fantasy     147.461        1251.47   0.117830 \n 7 PG-13  action       NA              NA     NA        \n 8 PG-13  comedy     1208.31           NA     NA        \n 9 PG-13  drama       306.26           NA     NA        \n10 PG-13  fantasy     361.119          NA     NA        \n11 PG-13  rom comedy  406.251          NA     NA        \n12 R      action     1044.82         1752.63   0.596143 \n13 R      comedy      248.876        1752.63   0.142001 \n14 R      drama       372.905        1752.63   0.212769 \n15 R      rom comedy   86.0310       1752.63   0.0490868"},{"path":"appendixC.html","id":"appendix-commas","chapter":"C Tips and Tricks","heading":"C.1.7 Dealing with %, commas, and $","text":"Say numerical data recorded percentages, commas, dollar form hence character strings. convert numerical values? Using parse_number() function readr package inside mutate()! Shout Stack Overflow.way around? Use scales package!Congratulations. now R Ninja!","code":"\nlibrary(readr)\nparse_number(\"10.5%\")[1] 10.5\nparse_number(\"145,897\")[1] 145897\nparse_number(\"$1,234.5\")[1] 1234\nlibrary(scales)\npercent(0.105)[1] \"10%\"\ncomma(145897)[1] \"145,897\"\ndollar(1234.5)[1] \"$1,234.50\""},{"path":"appendixC.html","id":"interactive-graphics","chapter":"C Tips and Tricks","heading":"C.2 Interactive graphics","text":"","code":""},{"path":"appendixC.html","id":"interactive-linegraphs","chapter":"C Tips and Tricks","heading":"C.2.1 Interactive linegraphs","text":"Another useful tool viewing linegraphs dygraph function dygraphs package combination dyRangeSelector function. allows us zoom selected range get interactive plot us work :syntax little different covered far. dygraph function expecting dates given rownames object. convert data data frame (since tibbles don’t allow easy row name manipulation), remove date variable flights_summarized data frame since accounted rownames. Lastly, run dygraph function new data frame contains median arrival delay column provide ability selector zoom interactive plot via dyRangeSelector. (Note plot interactive HTML version book.)","code":"\nlibrary(dygraphs)\nlibrary(nycflights23)\nflights_day <- mutate(flights, date = as.Date(time_hour))\nflights_summarized <- flights_day %>%\n  group_by(date) %>%\n  summarize(median_arr_delay = median(arr_delay, na.rm = TRUE)) %>%\n  as.data.frame()\nrownames(flights_summarized) <- flights_summarized$date\nflights_summarized <- select(flights_summarized, -date)\ndyRangeSelector(dygraph(flights_summarized))"},{"path":"appendixD.html","id":"appendixD","chapter":"D Learning Check Solutions","heading":"D Learning Check Solutions","text":"detect errors suggestions regarding solutions, please file issue ModernDive GitHub repository , ideally, create Pull Request GitHub repository making suggested changes (v2 branch) us review. Thanks!","code":""},{"path":"appendixD.html","id":"chapter-1-solutions","chapter":"D Learning Check Solutions","heading":"D.1 Chapter 1 Solutions","text":"(LC1.1) Repeat earlier installation steps … dplyr, nycflights23, knitr.Solution: RStudio → Packages tab → Install → enter dplyr, nycflights23, knitr → Install.\n(run install.packages(c(\"dplyr\",\"nycflights23\",\"knitr\")) Console.)(LC1.2) “Load” dplyr, nycflights23, knitr packages well repeating earlier steps.Solution:(LC1.3) ONE row flights dataset refer ?Solution: B. Data flight (one departure) NYC airport 2023.(LC1.4) examples dataset categorical variables? makes different quantitative variables?Solution: Hint: Type ?flights console see variables mean!Categorical:\ncarrier company\ndest destination\nflight flight number. Even though number, simply label. Example United 1545 less United 1714\ncarrier companydest destinationflight flight number. Even though number, simply label. Example United 1545 less United 1714Quantitative:\ndistance distance miles\ntime_hour time\ndistance distance milestime_hour time(LC1.5) lat, lon, alt, tz, dst, tzone describe airports? (Best guess.)Solution:lat = latitude, lon = longitudealt = airport altitude (likely feet)tz = time zone offset UTC (hours)dst = daylight savings time code/indicatortzone = time zone name (IANA string)(LC1.6) Provide variables data frame least one ID variable least two .Solution: weather data frame, combination origin, year, month, day, hour identification variables identify observation question.\n* Anything else pertains observations: temp, humid, wind_speed, etc.(LC1.7) Look help file airports. Revise earlier guesses lat, lon, alt, tz, dst, tzone needed.Solution:lat, lon: geographic latitude/longitude airportalt: altitude (feet)tz: hours offset UTC (e.g., −5 Eastern Standard Time)dst: daylight saving time indicator (e.g., “” = observes DST, etc., per help file coding)tzone: IANA time zone name (e.g., \"America/New_York\")documentation also available package website .","code":"\nlibrary(dplyr)\nlibrary(nycflights23)\nlibrary(knitr)"},{"path":"appendixD.html","id":"chapter-2-solutions","chapter":"D Learning Check Solutions","heading":"D.2 Chapter 2 Solutions","text":"Needed packages(LC2.1) Take look flights data frame nycflights23 package envoy_flights data frame moderndive package running View(flights) View(envoy_flights). respect data frames differ? example, think number rows dataset.Solution: envoy_flights subset flights containing rows carrier == \"MQ\" (Envoy Air). columns fewer rows flights.(LC2.2) practical reasons dep_delay arr_delay positive relationship?Solution: plane leaves late, often loses departure slot, encounters downstream congestion/spacing, arrives late well. Crewing/turnaround buffers can’t fully absorb big late departures, late often leads late (though time can made flight).(LC2.3) variables weather expect negative correlation dep_delay? ?Solution: Visibility (visib) pressure (pressure)—higher values usually mean clearer, calmer weather thus fewer delays (negative relationship). (contrast, precip, wind_speed, wind_gust tend positively related delays.)(LC2.4) believe cluster points near (0, 0)? (0, 0) correspond terms Envoy Air flights?Solution: (0, 0) -time departure -time arrival. Many flights leave arrive close schedule, producing dense cluster around origin.(LC2.5) features plot stand ?Solution: Different people answer one differently. One answer flights depart arrive less hour late.(LC2.6) Create new scatterplot different variables envoy_flights.Solution: Many possibilities one, see plot . pattern departure delay depending flight scheduled depart? Interestingly, seems blocks time flights depart Envoy.(LC2.7) setting alpha useful scatterplots?Solution: Lower alpha reveals point density overplotting—darker regions indicate many points overlap, regular opaque scatterplot hides.(LC2.8) viewing Figure 2.4, give approximate range arrival delays departure delays occur frequently. region changed compared observed plot without alpha = 0.2 set Figure 2.2?Solution: densest region near zero (roughly within -25 minutes departure arrival). transparency, high-density core much clearer opaque plot.(LC2.9) Take look weather data frame nycflights23 package early_january_2023_weather data frame moderndive package running View(weather) View(early_january_2023_weather). respect data frames differ?Solution: early_january_2023_weather subset: origin == \"EWR\", month == 1, day <= 15. variables; fewer rows.(LC2.10) View() flights data frame . time_hour variable uniquely identify hour measurement, whereas hour variable ?Solution: time_hour encodes date + hour (POSIXct timestamp R). hour alone repeats 0–23 every day (across airports), ’s unique.(LC2.11) linegraphs avoided clear ordering horizontal axis?Solution: Connecting points implies meaningful sequence/continuity. Without real order, lines suggest patterns don’t exist can mislead.(LC2.12) linegraphs frequently used time explanatory variable x-axis?Solution: Time inherently ordered; connecting adjacent times shows trends changes time smoothly.(LC2.13) Plot time series variable wind_speed Newark Airport first 15 days January 2023. Try select variable doesn’t lot missing (NA) values.Solution:One example visibility.(LC2.14) changing number bins 30 20 tell us distribution wind speeds?Solution: Fewer, wider bins smooth histogram, making overall pattern clearer (observations lower speeds), sacrificing fine detail.(LC2.15) classify distribution wind speeds symmetric skewed one direction another?Solution: Right-skewed—concentration low speeds tail extending higher wind speeds.(LC2.16) guess “center” value distribution? make choice?Solution: Around ~10 mph. tallest bins cluster roughly 5–15 mph range, typical value near 10.(LC2.17) data spread greatly center close? ?Solution: Moderate spread: values 0–20 mph, relatively beyond 30 mph (right tail extremely wide).(LC2.18) things notice faceted plot? faceted plot help us see relationships two variables?Solution: Distributions look similar across months, months slightly windier. Faceting puts months comparable panels shared scales, making cross-month contrasts easy.(LC2.19) 1–12 correspond ? 10, 20, 30?Solution: 1–12 = months year. 10, 20, 30 = wind speed tick marks (mph) x-axis.(LC2.20) types datasets faceted plots work well comparing relationships variables? Give example describing nature variables important characteristics.Solution: facetting variable many levels (e.g., hundreds ZIP codes) observations per level, producing tiny, sparse panels hard compare.(LC2.21) wind_speed variable weather dataset lot variability? say ?Solution: , mostly concentrated: many observations 0–20 mph, fewer high-speed outliers. variability exists right tail, bulk low--moderate.(LC2.22) dots top plot January correspond ? Explain might occurred January produce points.Solution: Outliers (unusually high winds). Likely stormy gusty winter days causing much higher wind speeds.(LC2.23) months highest variability wind speed? ?Solution: Winter months (e.g., Jan–Mar) show wider spread likely due seasonal systems bringing stronger, variable winds.(LC2.24) looked distribution numerical variable wind_speed split numerical variable month converted using factor() function order make side--side boxplot. boxplot wind_speed split numerical variable pressure similarly converted categorical variable using factor() informative?Solution: pressure continuous many distinct values; turning factor yields lots tiny groups points . leads clutter hard interpret.(LC2.25) Boxplots provide simple way identify outliers. may outliers easier identify looking boxplot instead faceted histogram?Solution: Boxplots explicitly mark outliers points; histograms spread counts across bins, rare extreme values don’t stand clearly.(LC2.26) histograms inappropriate categorical variables?Solution: Histograms assume numeric, ordered x-axis contiguous bins. Categorical levels aren’t numeric ordered sense.(LC2.27) difference histograms barplots?Solution: Histograms: numeric variable, contiguous bins (gaps), y = counts/density. Barplots: categorical variable, separate bars gaps, y = counts (given frequencies).(LC2.28) many Alaska Air flights departed NYC 2023?Solution: 7,843 flights (carrier == \"\").(LC2.29) 7th highest airline departed flights NYC 2023? better present table get answer quickly?Solution: Sorted departures, 7th highest NK (Spirit Airlines) 15,189 flights.\ndescending sort table ordered barplot makes rankings clearer glance.(LC2.30) pie charts avoided replaced barplots?Solution: Humans compare lengths better angles/areas. Pie charts distort judgments (angles > 90 degrees overestimated, < 90 degrees underestimated); barplots enable clean, one-axis comparisons.(LC2.31) think people continue use pie charts?Solution: Familiar defaults plotting software, perceived simplicity/aesthetics, stakeholder expectations (even though ’re harder read accurately).(LC2.32) kinds questions easily answered looking Figure 2.23?Solution: Comparing origins across carriers hard (segments lack common baseline). ’s also hard see small differences within stacked segments.(LC2.33) can say, anything, relationship airline airport NYC 2023 regard number departing flights?Solution: Carriers show airport specialization patterns. airlines many departures one NYC airport others.(LC2.34) might side--side barplot preferable stacked barplot case?Solution: give origin common baseline within carrier, making cross-origin comparisons straightforward.(LC2.35) disadvantages using dodged barplot, general?Solution: Can clutter many categories; labels/legend get busy; totals harder read; bars become thin harder compare levels proliferate.(LC2.36) faceted barplot preferred side--side stacked barplots case?Solution: Separate panels per origin reduce clutter keep shared scales, making easier rank carriers within airport compare patterns across airports.(LC2.37) information different carriers different airports easily seen faceted barplot?Solution: carriers dominate airport, clear rankings within airport, presence/absence specific carriers specific airports, without visual interference stacking dodging.","code":"\nlibrary(nycflights23)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(tibble)\nggplot(data = envoy_flights, mapping = aes(x = dep_time, y = dep_delay)) +\n  geom_point()\nggplot(early_january_2023_weather, aes(x = time_hour, y = visib)) +\n  geom_line()"},{"path":"appendixD.html","id":"chapter-3-solutions","chapter":"D Learning Check Solutions","heading":"D.3 Chapter 3 Solutions","text":"Needed packages(LC3.1) ’s another way using “” operator ! filter rows going Burlington, VT Seattle, WA flights data frame? Test using previous code.Solution:(LC3.2) Say doctor studying effect smoking lung cancer large number patients records measured five-year intervals. notices large number patients missing data points patient died, chooses ignore patients analysis. wrong doctor’s approach?Solution: introduces survivorship bias / informative dropout. Missingness random (death related smoking/cancer). Excluding biases effects toward healthier survivors underestimates harm.(LC3.3) Modify summarize function create summary_temp also use n() summary function: summarize(count = n()). returned value correspond ?Solution:count number rows summarized (per group grouped). ignore NAs unless pre-filter.(LC3.4) doesn’t following code work? Run code line--line instead , look data. words, select run summary_windspeed <- weather |> summarize(mean = mean(wind_speed, na.rm = TRUE)) first.Solution: first summarize() 1-row data frame mean; wind_speed longer exists, second summarize(sd(wind_speed)) errors. Compute one step fix :(LC3.5) Recall Chapter 2 looked wind speeds months NYC. standard deviation column summary_monthly_temp data frame tell us temperatures NYC throughout year?Solution: SD shows within-month variability temperatures. largest spread given month February second highest April. lead us expect different temperatures different days months compared months like July August smallest standard deviations.(LC3.6) code required get mean standard deviation wind speed day 2023 NYC?Solution:Note: group_by(day) enough, day value 1-31. need group_by(year, month, day).(LC3.7) Recreate by_monthly_origin, instead grouping via group_by(origin, month), group variables different order group_by(month, origin). differs resulting dataset?by_monthly_origin month column now first rows sorted month instead origin. compare values count by_origin_monthly by_monthly_origin using View() function, ’ll see values actually , just presented different order.Solution: counts identical; grouping structure column order grouping keys result differ (potentially default post-summarize grouping retained/dropped message).(LC3.8) identify many flights left three airports carrier?Solution:Note: n() function counts rows, whereas sum(VARIABLE_NAME) function sums values certain numerical variable VARIABLE_NAME.(LC3.9) filter() operation differ group_by() followed summarize()?Solution: filter() keeps/discards rows based conditions (aggregation). group_by()+summarize() collapses rows group-level statistics.(LC3.10) positive values gain variable flights correspond ? negative values? zero value?Solution: gain = dep_delay - arr_delay.\nPositive ⇒ made time (arrived earlier/less late departure delay).\nNegative ⇒ lost time (arrived even later relative departure delay).\nZero ⇒ change.Say flight departed 20 minutes late, .e. dep_delay = 20.arrived 10 minutes late, .e. arr_delay = 10.gain = dep_delay - arr_delay = 20 - 10  = 10 positive, “made /gained time air.”0 means departure arrival time , time made air. see cases gain near 0 minutes.never understood . pilot says “’re going make time air”\ndelay flying faster, don’t always just fly faster begin\n?(LC3.11) create dep_delay arr_delay columns simply subtracting dep_time sched_dep_time similarly arrivals? Try code explain differences result actually appears flights.Solution: reliably. dep_time/sched_dep_time clock times (hhmm) can cross midnight; simple subtraction ignores rollovers/time parsing. provided dep_delay/arr_delay already account logic. can’t direct arithmetic times. difference time 12:03 11:59 4 minutes, 1203-1159 = 44.(LC3.12) can say distribution gain? Describe sentences using plot gain_summary data frame values.Solution: histogram centered near 0 flights small gains/losses, long tails directions. Median near 0 11; flights gain lose lot minutes, ’s uncommon.(LC3.13) Looking Figure 3.7, joining flights weather (, words, matching hourly weather values flight), need join year, month, day, hour, origin, just hour?Solution: hour repeats every day three origins. need year, month, day, hour, origin uniquely identify correct hourly record correct airport. hour simply value 0 23; identify specific hour, need know year, month, day airport.(LC3.14) surprises top 10 destinations NYC 2023?Solution: Answers vary. Example: heavy traffic Florida hubs ORD/ATL; west-coast volumes lower/higher intuition. Also, high number flights Boston; wouldn’t easier quicker take train?(LC3.15) advantages data normal forms? disadvantages?Solution: Pros: less redundancy, consistent updates, smaller storage, clean joins. Cons: Need joins, queries can harder/slower, less convenient quick, denormalized reporting.(LC3.16) ways select three dest, air_time, distance variables flights? Give code showing least three different ways.Solution:(LC3.17) one use starts_with, ends_with, contains select columns flights data frame? Provide three different examples total: one starts_with, one ends_with, one contains.Solution:(LC3.18) might want use select() function data frame?Solution: focus relevant variables, declutter views, speed downstream operations/joins, reduce memory.(LC3.19) Create new data frame shows top 5 airports largest arrival delays NYC 2023.Solution:(LC3.20) Using datasets included nycflights23 package, compute available seat miles airline sorted descending order. completing necessary data wrangling steps, resulting data frame 13 rows (one airline) 2 columns (airline name available seat miles). hints:Crucial: Unless confident , worthwhile starting coding right away, rather first sketch paper necessary data wrangling steps using exact code, rather high-level pseudocode informal yet detailed enough articulate . way won’t confuse trying (algorithm) going (writing dplyr code).Take close look datasets using View() function: flights, weather, planes, airports, airlines identify variables necessary compute available seat miles.Figure 3.7 showing various datasets can joined also useful.Consider data wrangling verbs Table 3.2 toolbox!Solution: examples student-written pseudocode. Based pseudocode, let’s first display entire solution.Let’s now break step--step. compute available seat miles given flight, need distance variable flights data frame seats variable planes data frame, necessitating join key variable tailnum illustrated Figure 3.7. keep resulting data frame easy view, ’ll select() two variables carrier:Now flight can compute available seat miles ASM multiplying number seats distance via mutate():Next want sum ASM carrier. achieve first grouping carrier summarizing using sum() function:However, certain carriers certain flights missing NA values, resulting table also returns NA’s. can eliminate adding na.rm = TRUE argument sum(), telling R want remove NA’s sum. saw Section 3.3:Finally, arrange() data desc()ending order ASM.data frame correct, IATA carrier code always useful. example, carrier WN? can address joining airlines dataset using carrier key variable. step absolutely required, goes long way making table easier make sense . important empathetic ultimate consumers presented data!","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(nycflights23)\n# Original in book\nnot_BTV_SEA <- flights |>\n  filter(!(dest == \"BTV\" | dest == \"SEA\"))\n\n# Alternative way\nnot_BTV_SEA <- flights |>\n  filter(!dest == \"BTV\" & !dest == \"SEA\")\n\n# Yet another way\nnot_BTV_SEA <- flights |>\n  filter(dest != \"BTV\", dest != \"SEA\")\nsummary_windspeed <- weather |>\n  summarize(mean = mean(wind_speed, na.rm = TRUE),\n            std_dev = sd(wind_speed, na.rm = TRUE),\n            count = n())\nsummary_windspeed <- weather |>   \n  summarize(mean = mean(wind_speed, na.rm = TRUE)) |> \n  summarize(std_dev = sd(wind_speed, na.rm = TRUE))\nweather |> summarize(mean = mean(wind_speed, na.rm = TRUE),\n                     std_dev = sd(wind_speed, na.rm = TRUE))\nweather |>\n  group_by(year, month, day) |>\n  summarize(mean_ws = mean(wind_speed, na.rm = TRUE),\n            sd_ws = sd(wind_speed, na.rm = TRUE),\n            .groups = \"drop\")# A tibble: 364 × 5\n    year month   day  mean_ws   sd_ws\n   <int> <int> <int>    <dbl>   <dbl>\n 1  2023     1     1  8.39110 4.56829\n 2  2023     1     2  5.45022 2.49590\n 3  2023     1     3  4.76295 3.29083\n 4  2023     1     4  5.30637 2.84225\n 5  2023     1     5  5.57809 2.93970\n 6  2023     1     6  6.24937 3.76913\n 7  2023     1     7 10.8525  2.81767\n 8  2023     1     8  8.34316 3.37724\n 9  2023     1     9  8.26324 4.19758\n10  2023     1    10  8.49308 2.53893\n# ℹ 354 more rows\nby_monthly_origin <- flights |>\n  group_by(month, origin) |>\n  summarize(count = n())\nby_monthly_origin\ncount_flights_by_airport <- flights |> \n  group_by(origin, carrier) |> \n  summarize(num = n(), .groups = \"drop\") |>\n  arrange(origin, desc(num))\ncount_flights_by_airport\nflights |> select(dest, air_time, distance)\nflights |> select(c(dest, air_time, distance))\nflights |> select(dest:distance) \n# If you check out the dplyr help pages you'll also see `any_of()`\nflights |> select(any_of(c(\"dest\",\"air_time\",\"distance\")))\nflights |> select(starts_with(\"sched_\"))     # e.g., sched_dep_time, sched_arr_time\nflights |> select(ends_with(\"_delay\"))       # dep_delay, arr_delay\nflights |> select(contains(\"time\"))          # dep_time, arr_time, sched_*_time, etc.\ntop5_arr_delay <- flights |>\n  group_by(dest) |>\n  summarize(mean_arr_delay = mean(arr_delay, na.rm = TRUE), .groups=\"drop\") |>\n  arrange(desc(mean_arr_delay)) |>\n  top_n(n = 5) |>\n  # Useful for looking up the name of the airports!\n  inner_join(airports, by = c(\"dest\" = \"faa\")) |>\n  # Can rename with select too!\n  select(dest, airport_name = name, mean_arr_delay)\ntop5_arr_delay# A tibble: 5 × 3\n  dest  airport_name                                mean_arr_delay\n  <chr> <chr>                                                <dbl>\n1 PSE   Mercedita Airport                                  37.5549\n2 ANC   Ted Stevens Anchorage International Airport        36.4505\n3 RNO   Reno Tahoe International Airport                   34.4341\n4 ABQ   Albuquerque International Sunport                  26.7156\n5 ONT   Ontario International Airport                      26.1388\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  mutate(ASM = seats * distance) |>\n  group_by(carrier) |>\n  summarize(ASM = sum(ASM, na.rm = TRUE)) |>\n  arrange(desc(ASM))# A tibble: 13 × 2\n   carrier         ASM\n   <chr>         <dbl>\n 1 UA      18753552904\n 2 B6      18302094316\n 3 DL      15282765973\n 4 AA      11173950579\n 5 NK       3880566315\n 6 YX       3512338372\n 7 AS       2974953367\n 8 9E       2501279760\n 9 WN       1986242879\n10 HA        683807124\n11 OO        325167024\n12 F9        230428926\n13 MQ         16311790\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance)# A tibble: 424,068 × 3\n   carrier seats distance\n   <chr>   <int>    <dbl>\n 1 UA        149     2500\n 2 DL        222      760\n 3 B6        200     1576\n 4 B6         20      636\n 5 UA        149      488\n 6 AA        162     1085\n 7 B6        246     1576\n 8 AA        162      719\n 9 UA        178     1400\n10 NK        190     1065\n# ℹ 424,058 more rows\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  # Added:\n  mutate(ASM = seats * distance)# A tibble: 424,068 × 4\n   carrier seats distance    ASM\n   <chr>   <int>    <dbl>  <dbl>\n 1 UA        149     2500 372500\n 2 DL        222      760 168720\n 3 B6        200     1576 315200\n 4 B6         20      636  12720\n 5 UA        149      488  72712\n 6 AA        162     1085 175770\n 7 B6        246     1576 387696\n 8 AA        162      719 116478\n 9 UA        178     1400 249200\n10 NK        190     1065 202350\n# ℹ 424,058 more rows\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  mutate(ASM = seats * distance) |>\n  # Added:\n  group_by(carrier) |>\n  summarize(ASM = sum(ASM))# A tibble: 13 × 2\n   carrier         ASM\n   <chr>         <dbl>\n 1 9E       2501279760\n 2 AA      11173950579\n 3 AS       2974953367\n 4 B6      18302094316\n 5 DL      15282765973\n 6 F9        230428926\n 7 HA        683807124\n 8 MQ         16311790\n 9 NK       3880566315\n10 OO        325167024\n11 UA      18753552904\n12 WN       1986242879\n13 YX       3512338372\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  mutate(ASM = seats * distance) |>\n  group_by(carrier) |>\n  # Modified:\n  summarize(ASM = sum(ASM, na.rm = TRUE))# A tibble: 13 × 2\n   carrier         ASM\n   <chr>         <dbl>\n 1 9E       2501279760\n 2 AA      11173950579\n 3 AS       2974953367\n 4 B6      18302094316\n 5 DL      15282765973\n 6 F9        230428926\n 7 HA        683807124\n 8 MQ         16311790\n 9 NK       3880566315\n10 OO        325167024\n11 UA      18753552904\n12 WN       1986242879\n13 YX       3512338372\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  mutate(ASM = seats * distance) |>\n  group_by(carrier) |>\n  summarize(ASM = sum(ASM, na.rm = TRUE)) |>\n  # Added:\n  arrange(desc(ASM))# A tibble: 13 × 2\n   carrier         ASM\n   <chr>         <dbl>\n 1 UA      18753552904\n 2 B6      18302094316\n 3 DL      15282765973\n 4 AA      11173950579\n 5 NK       3880566315\n 6 YX       3512338372\n 7 AS       2974953367\n 8 9E       2501279760\n 9 WN       1986242879\n10 HA        683807124\n11 OO        325167024\n12 F9        230428926\n13 MQ         16311790\nflights |>\n  inner_join(planes, by = \"tailnum\") |>\n  select(carrier, seats, distance) |>\n  mutate(ASM = seats * distance) |>\n  group_by(carrier) |>\n  summarize(ASM = sum(ASM, na.rm = TRUE)) |>\n  arrange(desc(ASM)) |>\n  # Added:\n  inner_join(airlines, by = \"carrier\") |> \n  select(-carrier)# A tibble: 13 × 2\n           ASM name                  \n         <dbl> <chr>                 \n 1 18753552904 United Air Lines Inc. \n 2 18302094316 JetBlue Airways       \n 3 15282765973 Delta Air Lines Inc.  \n 4 11173950579 American Airlines Inc.\n 5  3880566315 Spirit Air Lines      \n 6  3512338372 Republic Airline      \n 7  2974953367 Alaska Airlines Inc.  \n 8  2501279760 Endeavor Air Inc.     \n 9  1986242879 Southwest Airlines Co.\n10   683807124 Hawaiian Airlines Inc.\n11   325167024 SkyWest Airlines Inc. \n12   230428926 Frontier Airlines Inc.\n13    16311790 Envoy Air             "},{"path":"appendixD.html","id":"chapter-4-solutions","chapter":"D Learning Check Solutions","heading":"D.4 Chapter 4 Solutions","text":"Needed packages(LC4.1) common characteristics “tidy” datasets?Solution: Rows correspond observations, columns correspond variables. type observational unit stored table.(LC4.2) makes “tidy” datasets useful organizing data?Solution: Tidy datasets organized way viewing data. format required ggplot2 dplyr packages data visualization wrangling. provide consistent, standardized structure functions work seamlessly together across packages. uniform format makes easier wrangle, visualize, analyze data without reshaping repeatedly.(LC4.3) Take look airline_safety data frame included fivethirtyeight data. Run following:reading help file running ?airline_safety, see airline_safety data frame containing information different airlines companies’ safety records. data originally reported data journalism website FiveThirtyEight.com Nate Silver’s article “Travelers Avoid Flying Airlines Crashes Past?”. Let’s ignore incl_reg_subsidiaries avail_seat_km_per_week variables simplicity:data frame “tidy” format. convert data frame “tidy” format, particular variable incident_type_years indicating incident type/year variable count counts?Solution:can done using pivot_longer() function tidyr package:look resulting airline_safety_smaller_tidy data frame spreadsheet viewer, ’ll see variable incident_type_years 6 possible values: \"incidents_85_99\", \"fatal_accidents_85_99\", \"fatalities_85_99\",  \"incidents_00_14\", \"fatal_accidents_00_14\", \"fatalities_00_14\" corresponding 6 columns airline_safety_smaller tidied.Note prior tidyr version 1.0.0 released CRAN September 2019, also done using gather() function tidyr package. gather() function still works, development stopped favor pivot_longer().(LC4.4) Convert dem_score data frame \ntidy data frame assign name dem_score_tidy resulting long-formatted data frame.Solution: Running following console:Let’s now compare dem_score dem_score_tidy. dem_score democracy score information year columns, whereas dem_score_tidy explicit variables year democracy_score. representations data contain information, can use ggplot() create plots using dem_score_tidy data frame.(LC4.5) Read life expectancy data stored https://moderndive.com/v2/data/le_mess.csv convert tidy data frame.Solution: code similar:observe structure respect year life_expectancy vs life_expectancy_tidy dem_score vs dem_score_tidy:","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(nycflights23)\nlibrary(fivethirtyeight)\nairline_safety\nairline_safety_smaller <- airline_safety |>\n  select(-c(incl_reg_subsidiaries, avail_seat_km_per_week))\nairline_safety_smaller# A tibble: 56 × 7\n   airline incidents_85_99 fatal_accidents_85_99 fatalities_85_99 incidents_00_14 fatal_accidents_00_14 fatalities_00_14\n   <chr>             <int>                 <int>            <int>           <int>                 <int>            <int>\n 1 Aer Li…               2                     0                0               0                     0                0\n 2 Aerofl…              76                    14              128               6                     1               88\n 3 Aeroli…               6                     0                0               1                     0                0\n 4 Aerome…               3                     1               64               5                     0                0\n 5 Air Ca…               2                     0                0               2                     0                0\n 6 Air Fr…              14                     4               79               6                     2              337\n 7 Air In…               2                     1              329               4                     1              158\n 8 Air Ne…               3                     0                0               5                     1                7\n 9 Alaska…               5                     0                0               5                     1               88\n10 Alital…               7                     2               50               4                     0                0\n# ℹ 46 more rows\nairline_safety_smaller_tidy <- airline_safety_smaller |>\n  pivot_longer(\n    names_to = \"incident_type_years\",\n    values_to = \"count\",\n    cols = -airline\n  )\nairline_safety_smaller_tidy# A tibble: 336 × 3\n   airline    incident_type_years   count\n   <chr>      <chr>                 <int>\n 1 Aer Lingus incidents_85_99           2\n 2 Aer Lingus fatal_accidents_85_99     0\n 3 Aer Lingus fatalities_85_99          0\n 4 Aer Lingus incidents_00_14           0\n 5 Aer Lingus fatal_accidents_00_14     0\n 6 Aer Lingus fatalities_00_14          0\n 7 Aeroflot   incidents_85_99          76\n 8 Aeroflot   fatal_accidents_85_99    14\n 9 Aeroflot   fatalities_85_99        128\n10 Aeroflot   incidents_00_14           6\n# ℹ 326 more rows\nairline_safety_smaller_tidy <- airline_safety_smaller |>\n  gather(key = incident_type_years, value = count, -airline)\nairline_safety_smaller_tidy# A tibble: 336 × 3\n   airline               incident_type_years count\n   <chr>                 <chr>               <int>\n 1 Aer Lingus            incidents_85_99         2\n 2 Aeroflot              incidents_85_99        76\n 3 Aerolineas Argentinas incidents_85_99         6\n 4 Aeromexico            incidents_85_99         3\n 5 Air Canada            incidents_85_99         2\n 6 Air France            incidents_85_99        14\n 7 Air India             incidents_85_99         2\n 8 Air New Zealand       incidents_85_99         3\n 9 Alaska Airlines       incidents_85_99         5\n10 Alitalia              incidents_85_99         7\n# ℹ 326 more rows\ndem_score_tidy <- dem_score |>\n  pivot_longer(\n    names_to = \"year\", values_to = \"democracy_score\",\n    cols = -country\n  )\ndem_score# A tibble: 96 × 10\n   country    `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`\n   <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 Albania        -9     -9     -9     -9     -9     -9     -9     -9      5\n 2 Argentina      -9     -1     -1     -9     -9     -9     -8      8      7\n 3 Armenia        -9     -7     -7     -7     -7     -7     -7     -7      7\n 4 Australia      10     10     10     10     10     10     10     10     10\n 5 Austria        10     10     10     10     10     10     10     10     10\n 6 Azerbaijan     -9     -7     -7     -7     -7     -7     -7     -7      1\n 7 Belarus        -9     -7     -7     -7     -7     -7     -7     -7      7\n 8 Belgium        10     10     10     10     10     10     10     10     10\n 9 Bhutan        -10    -10    -10    -10    -10    -10    -10    -10    -10\n10 Bolivia        -4     -3     -3     -4     -7     -7      8      9      9\n# ℹ 86 more rows\ndem_score_tidy# A tibble: 864 × 3\n   country   year  democracy_score\n   <chr>     <chr>           <dbl>\n 1 Albania   1952               -9\n 2 Albania   1957               -9\n 3 Albania   1962               -9\n 4 Albania   1967               -9\n 5 Albania   1972               -9\n 6 Albania   1977               -9\n 7 Albania   1982               -9\n 8 Albania   1987               -9\n 9 Albania   1992                5\n10 Argentina 1952               -9\n# ℹ 854 more rows\nlife_expectancy <- read_csv(\"https://moderndive.com/data/le_mess.csv\")\nlife_expectancy_tidy <- life_expectancy |>\n  pivot_longer(\n    names_to = \"year\",\n    values_to = \"life_expectancy\",\n    cols = -country\n  )\nlife_expectancy# A tibble: 202 × 67\n   country      `1951` `1952` `1953` `1954` `1955` `1956` `1957` `1958` `1959` `1960` `1961` `1962` `1963` `1964` `1965`\n   <chr>         <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 Afghanistan   27.13  27.67  28.19  28.73  29.27  29.8   30.34  30.86  31.4   31.94  32.47  33.01  33.53  34.07  34.6 \n 2 Albania       54.72  55.23  55.85  56.59  57.45  58.42  59.48  60.6   61.75  62.87  63.92  64.84  65.6   66.18  66.59\n 3 Algeria       43.03  43.5   43.96  44.44  44.93  45.44  45.94  46.45  46.97  47.5   48.02  48.55  49.07  49.58  50.09\n 4 Angola        31.05  31.59  32.14  32.69  33.24  33.78  34.33  34.88  35.43  35.98  36.53  37.08  37.63  38.18  38.74\n 5 Antigua and…  58.26  58.8   59.34  59.87  60.41  60.93  61.45  61.97  62.48  62.97  63.46  63.93  64.38  64.81  65.23\n 6 Argentina     61.93  62.54  63.1   63.59  64.03  64.41  64.73  65     65.22  65.39  65.53  65.64  65.74  65.84  65.95\n 7 Armenia       62.67  63.13  63.6   64.07  64.54  65     65.45  65.92  66.39  66.86  67.33  67.82  68.3   68.78  69.26\n 8 Aruba         58.96  60.01  60.98  61.87  62.69  63.42  64.09  64.68  65.2   65.66  66.07  66.44  66.79  67.11  67.44\n 9 Australia     68.71  69.11  69.69  69.84  70.16  70.03  70.31  70.86  70.43  70.87  71.14  70.91  70.97  70.63  70.96\n10 Austria       65.24  66.78  67.27  67.3   67.58  67.7   67.46  68.46  68.39  68.75  69.72  69.51  69.64  70.13  69.92\n# ℹ 192 more rows\n# ℹ 51 more variables: `1966` <dbl>, `1967` <dbl>, `1968` <dbl>, `1969` <dbl>, `1970` <dbl>, `1971` <dbl>,\n#   `1972` <dbl>, `1973` <dbl>, `1974` <dbl>, `1975` <dbl>, `1976` <dbl>, `1977` <dbl>, `1978` <dbl>, `1979` <dbl>,\n#   `1980` <dbl>, `1981` <dbl>, `1982` <dbl>, `1983` <dbl>, `1984` <dbl>, `1985` <dbl>, `1986` <dbl>, `1987` <dbl>,\n#   `1988` <dbl>, `1989` <dbl>, `1990` <dbl>, `1991` <dbl>, `1992` <dbl>, `1993` <dbl>, `1994` <dbl>, `1995` <dbl>,\n#   `1996` <dbl>, `1997` <dbl>, `1998` <dbl>, `1999` <dbl>, `2000` <dbl>, `2001` <dbl>, `2002` <dbl>, `2003` <dbl>,\n#   `2004` <dbl>, `2005` <dbl>, `2006` <dbl>, `2007` <dbl>, `2008` <dbl>, `2009` <dbl>, `2010` <dbl>, `2011` <dbl>, …\nlife_expectancy_tidy# A tibble: 13,332 × 3\n   country     year  life_expectancy\n   <chr>       <chr>           <dbl>\n 1 Afghanistan 1951            27.13\n 2 Afghanistan 1952            27.67\n 3 Afghanistan 1953            28.19\n 4 Afghanistan 1954            28.73\n 5 Afghanistan 1955            29.27\n 6 Afghanistan 1956            29.8 \n 7 Afghanistan 1957            30.34\n 8 Afghanistan 1958            30.86\n 9 Afghanistan 1959            31.4 \n10 Afghanistan 1960            31.94\n# ℹ 13,322 more rows"},{"path":"appendixD.html","id":"chapter-5-solutions","chapter":"D Learning Check Solutions","heading":"D.5 Chapter 5 Solutions","text":"Needed packages(LC5.1) Conduct new EDA y = fert_rate x = obes_rate. can say relationship?Solution:EDA = raw data + summaries + plot. scatterplot regression line correlation quantify direction/strength. data shows negative association obesity rate fertility rate (points slope ; negative correlation): obesity rate increases, fertility rate tends decrease.(LC5.2) Main purpose EDA?Solution: B.\nEDA helps understand relationships, spot outliers/missingness, check assumptions modeling, predict fabricate variables.(LC5.3) correct correlation coefficient?Solution: C.\nPearson’s correlation bounded [-1, 1] measures linear association strength/direction.(LC5.4) Fit simple linear regression using lm(fert_rate ~ obes_rate, data = UN_data_ch5) obes_rate new explanatory variable \\(x\\). Learn “best-fitting” line regression coefficients applying coef() function. regression results match earlier exploratory data analysis?Solution:slope sign match EDA: typically negative, meaning higher obesity rate associated lower fertility. magnitude tells much fertility changes per 1-point increase obesity (average). plot/correlation looked negative, negative fitted slope confirms .(LC5.5) intercept \\(b_0\\) represent?Solution: B.\\(b_0\\) predicted response \\(x = 0\\). may outside data’s range (often substantively meaningful), ’s definition.(LC5.6) best describes “slope”?Solution: C.\nslope change outcome one-unit increase explanatory variable, average.(LC5.7) negative slope indicate?Solution: .\n\\(x\\) increases, predicted \\(y\\) decreases (downward trend).(LC5.8) “wrapper function”?Solution: B.\nWrapper functions combine functions simpler interface (e.g., moderndive::get_regression_points() wraps broom::augment() cleaning).(LC5.9) Create data frame residuals obesity model.Solution:get_regression_points() returns observed \\(y\\), fitted \\(\\hat y\\), residuals \\(y-\\hat y\\) tidy frame, perfect diagnostics sorting.(LC5.10) True statement regression line?Solution: B.\nOLS chooses line minimizes sum squared residuals (squared differences observed fitted).(LC5.11) EDA x = continent, y = gdp_per_capita.Solution:Boxplots/facets reveal distribution differences. Notice higher median Europe lower Africa, Europe showing widest spread.(LC5.12) Baseline group categorical regression?Solution: B.\nbaseline reference category others compared (’s necessarily largest/smallest statistic unless set ).(LC5.13) Fit lm(gdp_per_capita ~ continent, data = gapminder2022) compare EDA.Solution:intercept baseline continent’s mean GDP per capita. continent coefficient offset baseline mean. Signs/magnitudes mirror boxplot: positive richer--baseline continents, negative poorer--baseline.(LC5.14) many “offsets” 4 levels?Solution: C. (3)\n\\(k\\) categories, get \\(k-1\\) offsets plus 1 intercept baseline.(LC5.15) Positive coefficient categorical model means?Solution: C.\npositive coefficient means group’s mean response higher baseline’s (coefficient amount).(LC5.16) true residuals?Solution: .\nresidual \\(y - \\hat y\\). can positive negative crucial model fit checks.(LC5.17) Find five negative residuals; interpret.Solution:Negative residuals continent’s mean. life expectancy lower continent’s average residual (magnitude) years.(LC5.18) Find five positive residuals; interpret.Solution:Positive residuals continent’s mean (higher life expectancy continent average residual years).(LC5.19) Compute sum squared residuals (SSR) three lines toy example show regression line smallest.Solution:“best” fitting solid regression line blue:\\[\n\\sum_{=1}^{n}(y_i - \\widehat{y}_i)^2 = (2.0-1.5)^2+(1.0-2.0)^2+(3.0-2.5)^2 = 1.5\n\\]arbitrarily chosen dotted red line:\\[\n\\sum_{=1}^{n}(y_i - \\widehat{y}_i)^2 = (2.0-2.5)^2+(1.00-2.5)^2+(3.0-2.5)^2 = 2.75\n\\]Another arbitrarily chosen dashed green line:\\[\n\\sum_{=1}^{n}(y_i - \\widehat{y}_i)^2 = (2.0-2.0)^2+(1.0-1.5)^2+(3.0-1.0)^2 = 4.25\n\\]calculated, \\(1.5 < 2.75 < 4.25\\). Therefore, show regression line blue smallest value residual sum squares.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nUN_data_ch5 <- un_member_states_2024 |>\n  select(iso, \n         life_exp = life_expectancy_2022, \n         fert_rate = fertility_rate_2022, \n         obes_rate = obesity_rate_2016)|>\n  na.omit()\n# 1) Look at raw values\nglimpse(UN_data_ch5)Rows: 181\nColumns: 4\n$ iso       <chr> \"AFG\", \"ALB\", \"DZA\", \"AGO\", \"ATG\", \"ARG\", \"ARM\", \"AUS\", \"AUT\", \"AZE\", \"BHS\", \"BHR\", \"BGD\", \"BRB\", \"B…\n$ life_exp  <dbl> 53.6, 79.5, 78.0, 62.1, 77.8, 78.3, 76.1, 83.1, 82.3, 74.2, 76.1, 79.9, 74.7, 78.5, 74.3, 81.9, 75.8…\n$ fert_rate <dbl> 4.3, 1.4, 2.7, 5.0, 1.6, 1.9, 1.6, 1.6, 1.5, 1.6, 1.4, 1.8, 1.9, 1.6, 1.5, 1.6, 2.0, 4.7, 1.4, 2.5, …\n$ obes_rate <dbl> 5.5, 21.7, 27.4, 8.2, 18.9, 28.3, 20.2, 29.0, 20.1, 19.9, 31.6, 29.8, 3.6, 23.1, 24.5, 22.1, 24.1, 9…\n# 2) Summary statistics\nUN_data_ch5 |>\n  select(fert_rate, obes_rate) |>\n  moderndive::tidy_summary()# A tibble: 2 × 11\n  column        n group type      min    Q1     mean median    Q3   max      sd\n  <chr>     <int> <chr> <chr>   <dbl> <dbl>    <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1 fert_rate   181 <NA>  numeric   1.1   1.6  2.50110    2     3.2   6.6 1.15041\n2 obes_rate   181 <NA>  numeric   2.1   9.6 19.2785    20.6  25.2  51.6 9.98920\n# 3) Visualizations\nggplot(UN_data_ch5, aes(x = obes_rate, y = fert_rate)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Obesity rate (2016)\", y = \"Fertility rate (2022)\")\n# Optional: correlation\nUN_data_ch5 |>\n  moderndive::get_correlation(fert_rate ~ obes_rate)# A tibble: 1 × 1\n        cor\n      <dbl>\n1 -0.435236\nm_obesity <- lm(fert_rate ~ obes_rate, data = UN_data_ch5)\ncoef(m_obesity)(Intercept)   obes_rate \n     3.4674     -0.0501 \n# or a tidy table:\nmoderndive::get_regression_table(m_obesity)# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept    3.467     0.168    20.618       0    3.136    3.799\n2 obes_rate   -0.05      0.008    -6.468       0   -0.065   -0.035\nm_obesity <- lm(fert_rate ~ obes_rate, data = UN_data_ch5)\nresids_df <- moderndive::get_regression_points(m_obesity)\nresids_df |> select(obes_rate, fert_rate, fert_rate_hat, residual) |> head()# A tibble: 6 × 4\n  obes_rate fert_rate fert_rate_hat residual\n      <dbl>     <dbl>         <dbl>    <dbl>\n1       5.5       4.3         3.192    1.108\n2      21.7       1.4         2.38    -0.98 \n3      27.4       2.7         2.094    0.606\n4       8.2       5           3.056    1.944\n5      18.9       1.6         2.52    -0.92 \n6      28.3       1.9         2.049   -0.149\ngapminder2022 <- un_member_states_2024 |>\n  select(country, life_exp = life_expectancy_2022, continent, gdp_per_capita) |> \n  na.omit()\n\n# Raw look\nglimpse(gapminder2022)Rows: 188\nColumns: 4\n$ country        <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\", \"Argentina\", \"…\n$ life_exp       <dbl> 53.6, 79.5, 78.0, 83.4, 62.1, 77.8, 78.3, 76.1, 83.1, 82.3, 74.2, 76.1, 79.9, 74.7, 78.5, 74.3,…\n$ continent      <fct> Asia, Europe, Africa, Europe, Africa, North America, South America, Asia, Oceania, Europe, Asia…\n$ gdp_per_capita <dbl> 356, 6810, 4343, 41993, 3000, 19920, 13651, 7018, 65100, 52085, 7762, 31458, 30147, 2688, 20239…\n# Summaries\ngapminder2022 |>\n  select(gdp_per_capita, continent) |>\n  moderndive::tidy_summary()# A tibble: 7 × 11\n  column             n group         type        min      Q1    mean  median      Q3     max      sd\n  <chr>          <int> <chr>         <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 gdp_per_capita   188 <NA>          numeric 259.025 2255.36 18472.8 6741.29 20395.5 240862. 30858.0\n2 continent         52 Africa        factor   NA       NA       NA     NA       NA       NA     NA  \n3 continent         44 Asia          factor   NA       NA       NA     NA       NA       NA     NA  \n4 continent         43 Europe        factor   NA       NA       NA     NA       NA       NA     NA  \n5 continent         23 North America factor   NA       NA       NA     NA       NA       NA     NA  \n6 continent         14 Oceania       factor   NA       NA       NA     NA       NA       NA     NA  \n7 continent         12 South America factor   NA       NA       NA     NA       NA       NA     NA  \n# Visualization\nggplot(gapminder2022, aes(x = continent, y = gdp_per_capita)) +\n  geom_boxplot() +\n  # Helpful with longer names/categories\n  coord_flip() +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Continent\", y = \"GDP per capita (USD)\")\nm_gdp <- lm(gdp_per_capita ~ continent, data = gapminder2022)\nmoderndive::get_regression_table(m_gdp)# A tibble: 6 × 7\n  term                     estimate std_error statistic p_value  lower_ci upper_ci\n  <chr>                       <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>\n1 intercept                 2637.1    3727.63     0.707   0.48  -4717.83   9992.03\n2 continent: Asia          13014.3    5506.08     2.364   0.019  2150.33  23878.2 \n3 continent: Europe        43061.4    5540.65     7.772   0     32129.2   53993.5 \n4 continent: North America 13713.5    6731.31     2.037   0.043   432.033 26994.9 \n5 continent: Oceania       10031.0    8093.58     1.239   0.217 -5938.28  26000.4 \n6 continent: South America  8083.51   8608.60     0.939   0.349 -8901.98  25069.0 \nlife_exp_model <- lm(life_exp ~ continent, data = gapminder2022)\nrp <- moderndive::get_regression_points(life_exp_model, ID = \"country\")\n\nrp |>\n  arrange(residual) |>\n  slice(1:5) |>\n  select(country, continent, life_exp, life_exp_hat, residual)# A tibble: 5 × 5\n  country                  continent     life_exp life_exp_hat residual\n  <chr>                    <fct>            <dbl>        <dbl>    <dbl>\n1 Afghanistan              Asia             53.65       74.95   -21.3  \n2 Central African Republic Africa           55.52       66.31   -10.79 \n3 Somalia                  Africa           55.72       66.31   -10.59 \n4 Haiti                    North America    65.95       76.295  -10.345\n5 Mozambique               Africa           57.1        66.31    -9.21 \nrp |>\n  arrange(desc(residual)) |>\n  top_n(n = 5) |>\n  select(country, continent, life_exp, life_exp_hat, residual)# A tibble: 5 × 5\n  country   continent life_exp life_exp_hat residual\n  <chr>     <fct>        <dbl>        <dbl>    <dbl>\n1 Algeria   Africa       78.03        66.31    11.72\n2 Singapore Asia         86.43        74.95    11.48\n3 Libya     Africa       77.18        66.31    10.87\n4 Tunisia   Africa       76.82        66.31    10.51\n5 Japan     Asia         84.91        74.95     9.96"},{"path":"appendixD.html","id":"chapter-6-solutions","chapter":"D Learning Check Solutions","heading":"D.6 Chapter 6 Solutions","text":"Needed packages(LC6.1) goal including interaction term?Solution: B.\nlets effect one explanatory variable depend level another.(LC6.2) main effects plus interactions change coefficient interpretation?Solution: C.\nCoefficients become conditional effects depend interacting variable(s).(LC6.3) statement dummy variables correct?Solution: B.\nencode categorical variables least two levels.(LC6.4) Model one categorical one numerical regressor, interactions: interpretation?Solution: B.\ncategories share slope; intercepts differ.(LC6.5) Compute observed responses, fitted values, residuals interaction model.Solution: Use fitted interaction model generate regression points:Interpretation: row, compare fert_rate fert_rate_hat; residual difference.(LC6.6) Main benefit visualizing fitted values residuals?Solution: B.\nassess model assumptions like linearity constant variance.(LC6.7) EDA debt credit_rating age. can say?Solution: Make scatterplots summaries; compute correlations:Patterns look : higher credit ratings often associate lower debt; age can show weak nonlinear association. plots correlations guide final statement.(LC6.8) Fit lm(debt ~ credit_rating + age, data = credit_ch6) compare EDA.Solution: Fit read coefficients, check signs magnitudes plots.Interpretation: coefficient credit_rating partial effect holding age fixed; age. EDA showed debt decreasing rating increases, expect negative coefficient credit_rating. age showed weak relationship, expect small coefficient consider adding nonlinear terms residual plots suggest curvature.(LC6.9) Interpretation regression coefficient multiple regression.Solution: .\nadditional effect regressor/predictor variable response accounting regressors.(LC6.10) Characteristic best fitting plane two numerical regressors.Solution: C.\nminimizes sum squared residuals observations.(LC6.11) intercept represent two explanatory variables?Solution: D.\nexpected response regressors zero.(LC6.12) partial slope?Solution: .\nadditional effect regressor response others taken account.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(ISLR2)\nUN_data_ch6 <- un_member_states_2024 |>\n  select(country, \n         life_expectancy_2022, \n         fertility_rate_2022, \n         income_group_2024)|>\n  na.omit()|>\n  rename(life_exp = life_expectancy_2022, \n         fert_rate = fertility_rate_2022, \n         income = income_group_2024)|>\n  mutate(income = factor(income, \n                         levels = c(\"Low income\", \"Lower middle income\", \n                                    \"Upper middle income\", \"High income\")))\ncredit_ch6 <- Credit |> as_tibble() |> \n  select(debt = Balance, credit_limit = Limit, \n         income = Income, credit_rating = Rating, age = Age)\nmodel_no_int <- lm(fert_rate ~ life_exp + income, data = UN_data_ch6)\npts_no_int <- get_regression_points(model_no_int)\n# Columns include fert_rate (observed y), fert_rate_hat (fitted ŷ), and residual = y - ŷ\nhead(pts_no_int)# A tibble: 6 × 6\n     ID fert_rate life_exp income              fert_rate_hat residual\n  <int>     <dbl>    <dbl> <fct>                       <dbl>    <dbl>\n1     1       4.3    53.65 Low income                  5.369   -1.069\n2     2       1.4    79.47 Upper middle income         1.531   -0.131\n3     3       2.7    78.03 Lower middle income         2.197    0.503\n4     4       5      62.11 Lower middle income         3.799    1.201\n5     5       1.6    77.8  High income                 1.872   -0.272\n6     6       1.9    78.31 Upper middle income         1.648    0.252\ncredit_ch6 |> \n  select(debt, credit_rating, age) |> \n  tidy_summary()# A tibble: 3 × 11\n  column            n group type      min     Q1     mean median     Q3   max       sd\n  <chr>         <int> <chr> <chr>   <dbl>  <dbl>    <dbl>  <dbl>  <dbl> <dbl>    <dbl>\n1 debt            400 <NA>  numeric     0  68.75 520.015   459.5 863     1999 459.759 \n2 credit_rating   400 <NA>  numeric    93 247.25 354.94    344   437.25   982 154.724 \n3 age             400 <NA>  numeric    23  41.75  55.6675   56    70       98  17.2498\nggplot(credit_ch6, aes(credit_rating, debt)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Credit rating\", y = \"Debt ($)\")\nggplot(credit_ch6, aes(age, debt)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Age\", y = \"Debt ($)\")\ncredit_ch6 |> select(debt, credit_rating, age) |> cor()                 debt credit_rating     age\ndebt          1.00000         0.864 0.00184\ncredit_rating 0.86363         1.000 0.10316\nage           0.00184         0.103 1.00000\nm_ca <- lm(debt ~ credit_rating + age, data = credit_ch6)\nget_regression_table(m_ca)# A tibble: 3 × 7\n  term          estimate std_error statistic p_value lower_ci upper_ci\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept     -269.581    44.806    -6.017       0 -357.668 -181.494\n2 credit_rating    2.593     0.074    34.84        0    2.447    2.74 \n3 age             -2.351     0.668    -3.521       0   -3.663   -1.038"},{"path":"appendixD.html","id":"chapter-7-solutions","chapter":"D Learning Check Solutions","heading":"D.7 Chapter 7 Solutions","text":"Needed packages(LC7.1) important mix balls new sample?Solution: randomize selection reduce dependence bias samples.(LC7.2) didn’t students get sample proportion?Solution: Sampling variation. Different random samples population yield different statistics.(LC7.3) can’t study sampling variation one virtual sample?Solution: One sample shows one outcome. Many samples needed observe variability form sampling distribution.(LC7.4) didn’t take 1000 samples 50 balls hand?Solution: impractical time consuming. Simulation fast, scalable, reproducible.(LC7.5) 1000-sample histogram, 30% red \\(n = 50\\) likely? 10%?Solution: 30% plausible though less common. 10% unlikely.(LC7.6) Figure 7.12 comparing sample sizes, center histograms?Solution: C. center histogram seems , regardless sample size.(LC7.7) sample size increases, histogram gets narrower. happens sample proportions?Solution: . vary less.(LC7.8) use random sampling constructing sampling distributions?Solution: B. minimize bias make inferences population.(LC7.9) construct histogram sample means proportions simulation study?Solution: . visualize distribution assess normality patterns.(LC7.10) bowl activity, population parameter? know value? can know exactly?Solution: population parameter true proportion red balls, \\(p\\). can know exactly counting balls computing proportion full population.(LC7.11) ensure shovel samples random?Solution: mixing bowl samples sampling replacement ball equal chance draw.(LC7.12) expected value sample mean context sampling distributions?Solution: B. population mean.(LC7.13) role Central Limit Theorem inference?Solution: B. says sampling distribution sample mean approaches normal sample size becomes large, regardless population distribution.(LC7.14) “sampling variation” refer ?Solution: B. Differences sample statistics due random sampling.(LC7.15) increasing sample size affect standard error sample mean?Solution: B. decreases standard error.(LC7.16) sampling distribution sample mean approximate normal?Solution:population normal: yes, \\(n\\).sample size large: yes, CLT.sample size sufficiently large regardless population: yes, CLT.population uniform: approximately normal \\(n\\) large, necessarily small \\(n\\).(LC7.17) comparing two samples, add variances finding SE difference?Solution: B. Adding variances reflects total uncertainty contributed independent samples.(LC7.18) large samples, shape sampling distribution difference sample proportions?Solution: B. Bell shaped, approximating normal distribution.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)"},{"path":"appendixD.html","id":"chapter-8-solutions","chapter":"D Learning Check Solutions","heading":"D.8 Chapter 8 Solutions","text":"Needed packages(LC8.1) expected value sample mean weight almonds large sample according sampling distribution theory?Solution: D.\naverage, sample mean equals population mean (unbiased), though single sample can vary.(LC8.2) point estimate differ interval estimate?Solution: B.\npoint estimate one number (e.g., \\(\\bar x\\)). interval estimate gives plausible range accounts sampling variability.(LC8.3) population mean (\\(\\mu\\)) represent almond activity?Solution: C.\n’s average weight 5,000 almonds (full population), just sample.(LC8.4) best describes population standard deviation (\\(\\sigma\\)) almond activity?Solution: B.\n\\(\\sigma\\) measures spread around population mean, sample mean.(LC8.5) use sample mean estimate population mean?Solution: B.\n\\(\\bar x\\) unbiased estimator \\(\\mu\\); across repeated samples hits \\(\\mu\\) average.(LC8.6) standard error sample mean calculated ?Solution: B.\n\\(SE(\\bar x)=\\sigma/\\sqrt{n}\\); larger \\(n\\) shrinks variability \\(\\bar x\\).(LC8.7) 95% confidence interval represent ?Solution: C.\nsampling, procedure yields intervals capture \\(\\mu\\) 95% time; numerically ’s \\(\\bar x \\pm 1.96\\times SE\\).(LC8.8) \\(t\\) distribution thicker tails standard normal?Solution: D.\nUsing \\(s\\) instead \\(\\sigma\\) adds uncertainty; \\(t\\) accounts heavier tails.(LC8.9) Effect increasing degrees freedom \\(t\\) distribution?Solution: B.\nTails thin distribution approaches standard normal df increases.(LC8.10) chief difference bootstrap distribution sampling distribution?Solution: bootstrap distribution resamples original sample (replacement), sampling distribution resamples entire population.(LC8.11) Looking bootstrap distribution sample mean Figure 8.14, two values say values lie?Solution: Roughly 3.60 3.75 grams.(LC8.12) following true confidence level constructing confidence interval?Solution: . confidence level sets interval’s width affects likely contain true parameter.(LC8.13) increasing sample size affect width confidence interval given confidence level?Solution: C. decreases width, making estimate precise.(LC8.14) Construct 95% confidence interval median weight almonds percentile method. appropriate also use standard-error method?Solution: percentile method appropriate median.standard-error method ideal, SE-based formulas assume symmetry/normality, may hold medians. standard-error method appropriate , bootstrap distribution bell-shaped:(LC8.15) advantages using infer building confidence intervals?Solution: Clearer, step--step workflow; consistent syntax across CI hypothesis testing; easier extension multi-variable cases; built-visualization tools.(LC8.16) main purpose bootstrapping statistical inference?Solution: B. generate multiple samples original data estimating parameters.(LC8.17) function denotes variables interest inference?Solution: C. specify().(LC8.18) key difference percentile method standard error method constructing confidence intervals using bootstrap samples?Solution: B. Percentile method takes middle 95% bootstrap statistics; SE method centers original sample mean adds/subtracts 1.96 × bootstrap SE.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)\nbootstrap_distribution <- almonds_sample |>\n  specify(response = weight) |>\n  generate(reps = 1000) |>\n  calculate(stat = \"median\")\npercentile_ci <- bootstrap_distribution |>\n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1      3.3        4\nvisualize(bootstrap_distribution)"},{"path":"appendixD.html","id":"chapter-9-solutions","chapter":"D Learning Check Solutions","heading":"D.9 Chapter 9 Solutions","text":"Needed packages(LC9.1) given code produce error stat = \"diff means\"?Solution:\nresponse variable popular_or_not categorical, numerical. statistic \"diff means\" requires numeric response. categorical responses, must use \"diff props\".(LC9.2) relatively confident distributions sample proportions approximate population distributions?Solution:\nsample sizes large (1000 songs per genre). large, representative random samples, sample proportions good approximations population proportions due Law Large Numbers Central Limit Theorem.(LC9.3) Using definition p-value, p-value represent Spotify test?Solution:\nprobability, assuming true difference popularity metal deep house, obtaining sample difference proportions large larger observed one favor metal.(LC9.4) Allen Downey’s diagram help us conclude whether difference existed popularity rates?Solution:\nfollowed steps: (1) specify variables hypotheses, (2) assume null universe difference, (3) simulate data null permutation, (4) calculate observed test statistic compare null distribution, (5) compute p-value. Since observed difference lay far tail null distribution, p-value small, leading us reject null conclude metal songs popular.(LC9.5) wrong saying, “defendant innocent” US trial?Solution:\ncourt can conclude “guilty.” means insufficient evidence convict, proof innocence. Similarly, hypothesis testing, “fail reject” \\(H_0\\) never “accept” true.(LC9.6) purpose hypothesis testing?Solution:\nassess whether sample evidence provides enough support reject null hypothesis population parameter, controlled risk error.(LC9.7) flaws hypothesis testing, alleviate ?Solution:\nFlaws:Overreliance arbitrary cutoffs (e.g., \\(p < 0.05\\)).Misinterpretation “failing reject” proof \\(H_0\\).Sensitivity sample size (tiny effects significant large \\(n\\)).Alleviations: Report effect sizes, confidence intervals, use context-specific significance levels rather fixed thresholds.(LC9.8) Comparing \\(\\alpha = 0.1\\) \\(\\alpha = 0.01\\), higher chance Type Error?Solution:\n\\(\\alpha = 0.1\\). larger \\(\\alpha\\) increases probability rejecting \\(H_0\\) actually true.(LC9.9) Conduct analysis comparing action versus romance movies using median rating instead mean. different ?Solution:(LC9.10) conclusions can make viewing faceted histogram rating versus genre see boxplot?Solution:\nfaceted histogram, can also see comparison rating versus genre year, conclude boxplot.(LC9.11) Describe used Allen Downey’s diagram conclude statistical difference existed.Solution:\n(1) stated variables hypotheses, (2) assumed null universe action romance mean, (3) generated shuffled datasets null, (4) calculated difference sample means , (5) compared observed difference null distribution, (6) computed \\(p\\)-value. \\(p\\)-value larger \\(\\alpha\\), failed reject \\(H_0\\); otherwise, reject \\(H_0\\) conclude statistical difference existed.(LC9.12) relatively confident distributions sample ratings approximate population distributions?Solution:\nsample movies taken randomly full IMDb database reasonable size, making representative. Central Limit Theorem, sufficiently large \\(n\\), sample-based statistics approximate population parameters well.(LC9.13) Using definition \\(p\\)-value, \\(p\\)-value represent ?Solution:\nprobability, assuming true difference mean ratings action romance movies, obtaining sample mean difference least extreme (either direction) observed difference.(LC9.14) value \\(p\\)-value two-sided test?Solution:\n\\(p\\)-value \\(0.004\\).(LC9.15) Test wrangling EDA skills:Create action/romance-data frame movies.Make boxplots faceted histograms IMDb rating genre.Compare sample plots.Solution:Use dplyr tidyr create necessary data frame focused action romance movies () movies data frame ggplot2movies package.Note :filter() command (Action == 1 | Romance == 1) picks movies either action romance !(Action == 1 & Romance == 1) leaves movies classified .mutate() use ifelse() create new variable genre whose values either \"Action\" \"Romance\".Make boxplot faceted histogram population data comparing ratings action romance movies IMDb.results look similar previous plots. outliers shown Romance boxplot done .","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(moderndive)\nlibrary(nycflights23)\nlibrary(ggplot2movies)\n# In calculate() step replace \"diff in means\" with \"diff in medians\"\nnull_distribution_movies_median <- movies_sample |> \n  specify(formula = rating ~ genre) |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in medians\", order = c(\"Action\", \"Romance\"))\n\n# compute observed \"diff in medians\"\nobs_diff_medians <- movies_sample |> \n  specify(formula = rating ~ genre) |> \n  calculate(stat = \"diff in medians\", order = c(\"Action\", \"Romance\"))\nobs_diff_mediansResponse: rating (numeric)\nExplanatory: genre (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 -1.55\n# Visualize p-value. Observing this difference in medians under H0\n# is very unlikely! Suggesting H0 is false, similarly to when we used\n# \"diff in means\" as the test statistic. \nvisualize(null_distribution_movies_median, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_medians, direction = \"both\")\n# p-value is very small, just like when we used \"diff in means\"\n# as the test statistic. \nnull_distribution_movies_median |> \n  get_p_value(obs_stat = obs_diff_medians, direction = \"both\")# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.008\naction_romance <- movies |>\n  select(title, year, rating, Action, Romance) |>\n  filter((Action == 1 | Romance == 1) & !(Action == 1 & Romance == 1)) |> \n  mutate(genre = ifelse(Action == 1, \"Action\", \"Romance\"))\n# Faceted histogram\nggplot(action_romance, aes(rating)) +\n  geom_histogram() +\n  facet_wrap(~genre)\n# Boxplot\nggplot(action_romance, aes(y = rating, x = genre)) +\n  geom_boxplot()"},{"path":"appendixD.html","id":"chapter-10-solutions","chapter":"D Learning Check Solutions","heading":"D.10 Chapter 10 Solutions","text":"Needed packages(LC10.1) Meaning error term \\(\\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).Solution: C.\npart response explained line.(LC10.2) key property least squares estimates \\(b_0, b_1\\).Solution: B.\nlinear combinations observed responses \\(y_1, y_2, \\ldots, y_n\\).(LC10.3) encode two-group difference means regression.Solution: C.\nInclude dummy variable groups; coefficient equals mean difference.(LC10.4) Meaning null hypothesis \\(H_0: \\beta_1 = 0\\).Solution: .\nlinear association explanatory variable response.(LC10.5) assumption required linear regression model.Solution: .\nerror terms normally distributed mean zero constant variance.(LC10.6) slope estimate \\(b_1\\) random variable.Solution: C.\nvaries sample sample due sampling variation.(LC10.7) Residual analysis UN data (\\(y =\\) fertility, \\(x =\\) life expectancy).Solution:\nCompute fitted values residuals get_regression_points().\nFIGURE D.1: Histogram residuals.\nresiduals appear follow Normal distribution pretty closely. one bin near 0.5 bit high, overall histogram looks reasonably symmetric mound-shaped.\nFIGURE D.2: Plot residuals HDI.\nplot seems fit equality variance. doesn’t appear strong “fan formation” graph.(LC10.8) Interpretation near-zero \\(p\\)-value slope.Solution: B.\nstrong evidence \\(H_0: \\beta_1 = 0\\), indicating linear relationship explanatory response variables.(LC10.9) assumptions residual plot helps assess.Solution:\nResidual plots help check linearity equal variance. directly check independence (need residuals vs time sequential data). sufficient normality, requires histograms QQ-plots.(LC10.10) Meaning U-shaped residual plot.Solution: B.\nsuggests violation linearity assumption.(LC10.11) Simulation-based inference correlation coefficient.Solution:\nUse infer workflow stat = \"correlation\" waiting ~ duration.CI: specify() |> generate(reps, type = \"bootstrap\") |> calculate(stat = \"correlation\") |> get_confidence_interval(type = \"percentile\", level = 0.95).Test \\(H_0:\\rho=0\\): specify() |> hypothesize(null = \"independence\") |> generate(reps, type = \"permute\") |> calculate(stat = \"correlation\"), compare observed correlation shade_p_value() / get_p_value(direction = \"\").(LC10.12) bootstrap percentile method appropriate \\(\\beta_1\\) .Solution: D.\nrequire bootstrap distribution normally shaped.(LC10.13) Role permutation test \\(\\beta_1\\).Solution: B.\nassesses whether observed slope occur chance \\(H_0\\) relationship.(LC10.14) \\(p\\)-value near 0 null distribution slopes.Solution: C.\nobserved slope significantly different 0, suggesting meaningful relationship.(LC10.15) Meaning \\(\\beta_j\\) multiple regression.Solution: D.\npartial slope \\(X_j\\), accounting regressors.(LC10.16) convert continent_of_origin factor.Solution: B.\ncreate dummy variables representing categories.(LC10.17) Purpose scatterplot matrix .Solution: C.\nExamine pairwise linear relationships spot multicollinearity among regressors.(LC10.18) Role dummy variables continent_of_origin.Solution: B.\nshift intercept according category.(LC10.19) matters estimators unbiased.Solution: B.\naverage, estimates equal true population parameters.(LC10.20) coefficients change different regressor sets.Solution: C.\ncoefficient depends specific combination regressors model.(LC10.21) Constructing 95% CI coefficient MLR.Solution: .\nPoint estimate \\(\\pm\\) (critical \\(t\\) value \\(\\times\\) standard error).(LC10.22) ANOVA model-comparison test evaluates.Solution: B.\nWhether reduced model adequate full model needed.(LC10.23) use simulation-based methods MLR.Solution: D.\ndon’t rely normality assumptions large sample sizes.(LC10.24) Purpose bootstrap distribution partial slopes.Solution: B.\napproximate sampling distribution resampling replacement.(LC10.25) CI partial slope includes 0.Solution: .\nstatistically significant relationship response (chosen level).(LC10.26) Observed test statistic far right null distribution.Solution: C.\nLikely statistically significant; reject null.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(infer)\nlibrary(gridExtra)\nlibrary(GGally)\nsimple_model <- lm(fertility_rate_2022 ~ hdi_2022, \n                   data = na.omit(un_member_states_2024))\nregression_points <- get_regression_points(simple_model, ID = \"country\")\nregression_points# A tibble: 107 × 5\n   country      fertility_rate_2022 hdi_2022 fertility_rate_2022_hat residual\n   <chr>                      <dbl>    <dbl>                   <dbl>    <dbl>\n 1 Afghanistan                  4.3    0.462                   4.035    0.265\n 2 Algeria                      2.7    0.745                   2.394    0.306\n 3 Argentina                    1.9    0.849                   1.791    0.109\n 4 Armenia                      1.6    0.786                   2.157   -0.557\n 5 Australia                    1.6    0.946                   1.229    0.371\n 6 Austria                      1.5    0.926                   1.345    0.155\n 7 Azerbaijan                   1.6    0.76                    2.307   -0.707\n 8 Bahamas, The                 1.4    0.82                    1.959   -0.559\n 9 Bahrain                      1.8    0.888                   1.565    0.235\n10 Barbados                     1.6    0.809                   2.023   -0.423\n# ℹ 97 more rows\nggplot(regression_points, aes(x = hdi_2022, y = residual)) +\n  geom_point() +\n  labs(x = \"HDI\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)"},{"path":"appendixD.html","id":"chapter-11-solutions","chapter":"D Learning Check Solutions","heading":"D.11 Chapter 11 Solutions","text":"Needed packages(LC11.1) Check LINE conditions met inference Seattle house prices example \nprice_interaction <- lm(log10_price ~ log10_size * condition, data = house_prices).Solution:Plot log10 size log10 price condition:\nFIGURE D.3: Log10 size vs. log10 price condition.\nLinearity: relationship log10 size log10 price approximately linear within condition level. little challenging see due overplotting.Independence: house sale separate observation, independence reasonable.Plot Q-Q plot residuals:Histogram residuals:\nFIGURE D.4: Histogram residuals.\nNormality: Residuals appear roughly bell-shaped log10 transformation.Plot residuals vs. fitted (log10_price_hat) values:\nFIGURE D.5: Residuals vs. fitted values.\nEqual variance: spread residuals fairly constant across fitted values.\nOverall, LINE conditions reasonably satisfied.(LC11.2) Repeat regression modeling prediction house condition 5 size 1900 square feet, using parallel slopes model.Solution:(LC11.3) Interpret results rows get_regression_table(price_interaction) already discussed.Solution:condition2–condition5: give intercept differences relative condition 1 houses. example, positive value condition5 means condition 5 houses start higher baseline log10 price size = 1 square foot.log10_size:condition2–log10_size:condition5: give slope differences relative condition 1. positive coefficient means price increases faster size condition compared condition 1.(LC11.4) Create, visualize, interpret confidence intervals using theory-based simulation-based approaches.Solution:Theory-based: Use get_regression_table(price_interaction) reports CIs \\(t\\)-distribution. example, CI log10_size excludes 0, size significant.Simulation-based: Bootstrap regression resampling (infer package fit() + get_confidence_interval()). Plot bootstrap distribution coefficients shade middle 95%.\napproaches agree: log10_size confidence interval include 0, confirming significant relationship log10 price.(LC11.5) date 1994 2003 fewest number births US? story tell ?Solution:dates fewest number births US 12/25 years 2001, 2000, 2003, 2002, 1999. Christmas Day hospitals don’t generally induce labor day.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(fivethirtyeight)\nlibrary(infer)\nprice_interaction <- lm(\n  log10_price ~ log10_size * condition,\n  data = house_prices\n)\n\nregression_points <- get_regression_points(price_interaction, ID = \"id\")\nggplot(house_prices, aes(x = log10(sqft_living), y = log10(price), color = condition)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Log10 Size (sqft)\", y = \"Log10 Price ($)\", color = \"Condition\")\nggplot(regression_points, aes(sample = residual)) +\n  geom_qq(alpha = 0.3) +\n  geom_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\")\nggplot(regression_points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.05, color = \"white\") +\n  labs(x = \"Residual\")\nggplot(regression_points, aes(x = log10_price_hat, y = residual)) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\")\nhouse_prices <- house_prices |> \n  mutate(\n    log10_price = log10(price),\n    log10_size = log10(sqft_living)\n  )\n# Fit regression model:\nprice_interaction <- lm(log10_price ~ log10_size + condition,\n  data = house_prices\n)\n\n# Get regression table:\nget_regression_table(price_interaction)# A tibble: 6 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept       2.882     0.036    79.994   0        2.811    2.952\n2 log10_size      0.837     0.006   134.169   0        0.825    0.85 \n3 condition: 2   -0.039     0.033    -1.16    0.246   -0.104    0.027\n4 condition: 3    0.032     0.031     1.037   0.3     -0.028    0.092\n5 condition: 4    0.044     0.031     1.422   0.155   -0.017    0.104\n6 condition: 5    0.096     0.031     3.094   0.002    0.035    0.156\n10^(2.88 + 0.096 + 0.837 * log10(1900))[1] 525190\nUS_births_1994_2003 |> \n  arrange(births)# A tibble: 3,652 × 6\n    year month date_of_month date       day_of_week births\n   <int> <int>         <int> <date>     <ord>        <int>\n 1  2001    12            25 2001-12-25 Tues          6443\n 2  2000    12            25 2000-12-25 Mon           6566\n 3  2003    12            25 2003-12-25 Thurs         6628\n 4  2002    12            25 2002-12-25 Wed           6629\n 5  1999    12            25 1999-12-25 Sat           6674\n 6  2000    12            24 2000-12-24 Sun           6801\n 7  1995    12            24 1995-12-24 Sun           6999\n 8  2002     4             7 2002-04-07 Sun           7008\n 9  2002     3            31 2002-03-31 Sun           7019\n10  1998    12            25 1998-12-25 Fri           7020\n# ℹ 3,642 more rows"},{"path":"appendixD.html","id":"appendix-a1-solutions","chapter":"D Learning Check Solutions","heading":"D.12 Appendix A1 Solutions","text":"(LCA1.1) proportion area normal curve less 3? Greater 12? 0 12?Less 3: 3 one standard deviation less mean 6, since \\(\\frac{3 - \\mu}{\\sigma} = \\frac{3-6}{3} = -1\\). Thus compute area left \\(z = -1\\) Figure .2: 0.15% + 2.35% + 13.5% = 16%.Greater 12: 12 two standard deviations greater mean 6, since \\(\\frac{12 - \\mu}{\\sigma} = \\frac{12-6}{3} = +2\\), compute area right \\(z = +2\\) Figure .2: 2.35% + 0.15% = 2.5%.0 12: 0 two standard deviations less mean 6, since \\(\\frac{0 - \\mu}{\\sigma} = \\frac{0-6}{3} = -2\\). Thus compute area \\(z = -2\\) \\(z = +2\\) Figure .2: 13.5% + 34% + 34% + 13.5% = 95%.(LCA1.2) 2.5th percentile area normal curve? 97.5th percentile? 100th percentile?2.5th percentile: Starting left Figure .2, since 0.15% + 2.35% = 2.5%, 2.5th percentile \\(z = -2\\). However, standard units. Thus need value normal distribution two standard deviations lower mean: \\(\\mu - 2 \\cdot\\sigma = 6 - 2 \\cdot 3 = 0\\).97.5th percentile: Starting left Figure .2, since 0.15% + 2.35% + 13.5% + 34% + 34% + 13.5% = 97.5%, 97.5th percentile \\(z = +2\\). However, standard units. Thus need value normal distribution two standard deviations higher mean: \\(\\mu + 2 \\cdot\\sigma = 6 + 2 \\cdot 3 = 12\\).100th percentile: \\(+\\infty\\). words, 100% values less positive infinity.","code":""},{"path":"the-theory-behind-the-bootstrap-method.html","id":"the-theory-behind-the-bootstrap-method","chapter":"E The Theory Behind the Bootstrap Method","heading":"E The Theory Behind the Bootstrap Method","text":"explain bootstrap method use construct interval estimate \n\\(\\mu\\), explanation can easily generalized confidence intervals \nparameters. let letter \\(F\\) represent \npopulation distribution. can think distribution set \npossible values can observed probability chance \nvalues may appear sample taken population. Recall \nhistogram can thought visual representation distribution.\nMoreover, imagine \\(\\mu\\) parameter wish estimate. Observe ,\ngeneral, need restrict treatment \\(\\mu\\) \nparameter characteristic population.take sample size\n\\(n\\) estimate \\(\\mu\\), construct empirical distribution based \nsample, assigning probability \\(1/n\\) value sample.\ncall empirical distribution \\(F_n\\). treat empirical distribution\nestimate population distribution.\nexample, population mean empirical distribution precisely\naverage values sample, sample mean, call \n\\(\\mu_n\\) emphasize treating population.bootstrap consists using empirical distribution nothing else\nconstruction estimates, confidence intervals, etc. \nintroducing additional assumptions population distribution, \nstructure sample, simply taking advantage sample \ncollected.now, \\(F_n\\) considered population distribution, described \nSection 8.2 obtain \nbootstrap sample sampling, replacement, sample size \\(n\\) \nempirical distribution. bootstrap sample now considered empirical\ndistribution, call \\(F^*_n\\).Let’s recap. want study population distribution \\(F\\), take \nsample size \\(n\\) distribution, call \\(F_n\\), use \nempirical distribution estimates \\(F\\). let \\(F_n\\) play\nrole population distribution, take bootstrap sample \\(F_n\\)\n(also size \\(n\\)), call \\(F^*_n\\), use empirical distribution \nestimates \\(F_n\\)., first stage know anything \\(F\\) sample\ntaken. second stage, know everything \\(F_n\\), can\nstudy \\(F^*_n\\) behaves respect \\(F_n\\). idea bootstrap \ntranslate whatever learn relationship \\(F^*_n\\) \\(F_n\\) \npossible relationship \\(F_n\\) \\(F\\).Furthermore, obtain every possible bootstrap sample \\(F^*_n\\) \\(F_n\\),\nreally large number, discussed Section\n8.2. Instead, thousand bootstrap samples \nused.Let’s translate ideas parameters estimators. interested \nestimating \\(\\mu\\) \\(F\\). constructed \\(\\mu_n\\) \\(F_n\\), , \nknow mean empirical distribution, just sample\nmean sample taken. Now, can obtain distribution \\(\\mu^*_n\\),\n, distribution sample means bootstrap samples,\nstudy much variation approximate standard error, use\ninformation construct confidence intervals, perform hypothesis tests,\netc.(Hall 1992) provided theoretical justification use\nbootstrap confidence intervals using Edgeworth expansions. description \nEdgeworth expansions beyond scope book, simple terms,\ncan think approximations population distribution \nparameter estimates, certain context provide better approximations \nCentral Limit Theorem.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
