<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Inference for Regression | Statistical Inference via Data Science</title>
<meta name="author" content="Chester Ismay, Albert Y. Kim, and Arturo Valdivia   Foreword by Kelly S. McConville">
<meta name="description" content="In this chapter, we revisit the regression model studied in Chapters 5 and 6. We do it by taking into account the inferential statistics methods introduced in Chapters 8 and 9. We will show that...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 10 Inference for Regression | Statistical Inference via Data Science">
<meta property="og:type" content="book">
<meta property="og:description" content="In this chapter, we revisit the regression model studied in Chapters 5 and 6. We do it by taking into account the inferential statistics methods introduced in Chapters 8 and 9. We will show that...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Inference for Regression | Statistical Inference via Data Science">
<meta name="twitter:site" content="@ModernDive">
<meta name="twitter:description" content="In this chapter, we revisit the regression model studied in Chapters 5 and 6. We do it by taking into account the inferential statistics methods introduced in Chapters 8 and 9. We will show that...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet">
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script><script src="libs/dygraphs-1.1.1/shapes.js"></script><script src="libs/moment-2.8.4/moment.js"></script><script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script><script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script><script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YVFBK9P73R"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YVFBK9P73R');
    </script><!-- Open Graph Meta Tags --><meta property="og:title" content="ModernDive V2">
<meta property="og:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools.">
<meta property="og:image" content="https://moderndive.com/v2/images/logos/v2_cover.jpg">
<meta property="og:url" content="https://moderndive.com/v2/inference-for-regression.html">
<meta name="twitter:card" content="summary_large_image">
<!-- Set up favicon --><link rel="icon" href="images/logos/favicons/favicon.ico" type="image/x-icon">
<!-- Redirect to ModernDive V2 --><script>
      if (window.location.pathname === '/v2') {
        window.location.replace(window.location.pathname + '/');
      }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="A ModernDive into R and the Tidyverse &lt;br&gt; (Second Edition)">Statistical Inference via Data Science</a>:
        <small class="text-muted">A ModernDive into R and the Tidyverse <br> (Second Edition)</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome to ModernDive (v2)</a></li>
<li><a class="" href="foreword.html">Foreword</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="about-the-authors.html">About the authors</a></li>
<li><a class="" href="getting-started.html"><span class="header-section-number">1</span> Getting Started with Data in R</a></li>
<li class="book-part">Data Science with tidyverse</li>
<li><a class="" href="viz.html"><span class="header-section-number">2</span> Data Visualization</a></li>
<li><a class="" href="wrangling.html"><span class="header-section-number">3</span> Data Wrangling</a></li>
<li><a class="" href="tidy.html"><span class="header-section-number">4</span> Data Importing and Tidy Data</a></li>
<li class="book-part">Statistical Modeling with moderndive</li>
<li><a class="" href="regression.html"><span class="header-section-number">5</span> Simple Linear Regression</a></li>
<li><a class="" href="multiple-regression.html"><span class="header-section-number">6</span> Multiple Regression</a></li>
<li class="book-part">Statistical Inference with infer</li>
<li><a class="" href="sampling.html"><span class="header-section-number">7</span> Sampling</a></li>
<li><a class="" href="confidence-intervals.html"><span class="header-section-number">8</span> Estimation, Confidence Intervals, and Bootstrapping</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="active" href="inference-for-regression.html"><span class="header-section-number">10</span> Inference for Regression</a></li>
<li class="book-part">Conclusion</li>
<li><a class="" href="thinking-with-data.html"><span class="header-section-number">11</span> Tell Your Story with Data</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="appendixA.html"><span class="header-section-number">A</span> Statistical Background</a></li>
<li><a class="" href="appendixB.html"><span class="header-section-number">B</span> Inference Examples</a></li>
<li><a class="" href="appendixC.html"><span class="header-section-number">C</span> Tips and Tricks</a></li>
<li><a class="" href="the-theory-behind-the-bootstrap-method.html"><span class="header-section-number">D</span> The Theory Behind the Bootstrap Method</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/moderndive/ModernDive_book/">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><img src="https://moderndive.com/wide_format.png" alt="ModernDive" style="max-width: 100%; width: 100%; height: auto; display: block; margin: 0 auto; padding-top: 10px;"><div id="inference-for-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Inference for Regression<a class="anchor" aria-label="anchor" href="#inference-for-regression"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter, we revisit the regression model studied in Chapters <a href="regression.html#regression">5</a> and <a href="multiple-regression.html#multiple-regression">6</a>.
We do it by taking into account the inferential statistics methods introduced in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> and <a href="hypothesis-testing.html#hypothesis-testing">9</a>.
We will show that when applying the linear regression methods introduced earlier on sample data, we can gain insight into the relationships between the response and explanatory variables of an entire population.</p>
<div id="inf-packages" class="section level2 unnumbered">
<h2>Needed packages<a class="anchor" aria-label="anchor" href="#inf-packages"><i class="fas fa-link"></i></a>
</h2>
<p>If needed, read Section <a href="getting-started.html#packages">1.3</a> for information on how to install and load R packages.</p>
<div class="sourceCode" id="cb462"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://moderndive.github.io/moderndive/">moderndive</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidymodels/infer">infer</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggobi.github.io/ggally/">GGally</a></span><span class="op">)</span></span></code></pre></div>
<p>Recall that loading the <code>tidyverse</code> package loads many packages that we have encountered earlier. For details refer to Section <a href="tidy.html#tidyverse-package">4.4</a>. The packages <code>moderndive</code> and <code>infer</code> contain functions and data frames that will be used in this chapter.</p>
</div>
<div id="the-simple-linear-regression-model" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> The simple linear regression model<a class="anchor" aria-label="anchor" href="#the-simple-linear-regression-model"><i class="fas fa-link"></i></a>
</h2>
<div id="un-member-states-revisited" class="section level3" number="10.1.1">
<h3>
<span class="header-section-number">10.1.1</span> UN member states revisited<a class="anchor" aria-label="anchor" href="#un-member-states-revisited"><i class="fas fa-link"></i></a>
</h3>
<p>We briefly review the example of UN member states covered in Section <a href="regression.html#model1">5.1</a>.
Data on the current UN member states, as of 2024, can be found in the <code>un_member_states_2024</code> data frame included in the <code>moderndive</code> package.
As we did in Section <a href="regression.html#model1">5.1</a>, we save these data as a new data frame called <code>UN_data_ch10</code>, <code><a href="https://dplyr.tidyverse.org/reference/select.html">select()</a></code> the required variables, and include rows without missing data using <code><a href="https://rdrr.io/r/stats/na.fail.html">na.omit()</a></code>:</p>
<div class="sourceCode" id="cb463"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">UN_data_ch10</span> <span class="op">&lt;-</span> <span class="va">un_member_states_2024</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">country</span>,</span>
<span>         life_exp <span class="op">=</span> <span class="va">life_expectancy_2022</span>, </span>
<span>         fert_rate <span class="op">=</span> <span class="va">fertility_rate_2022</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb464"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">UN_data_ch10</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
column
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
Q1
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
Q3
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
sd
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
life_exp
</td>
<td style="text-align:right;">
183
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
53.6
</td>
<td style="text-align:right;">
69.4
</td>
<td style="text-align:right;">
73.66
</td>
<td style="text-align:right;">
75.2
</td>
<td style="text-align:right;">
78.3
</td>
<td style="text-align:right;">
86.4
</td>
<td style="text-align:right;">
6.84
</td>
</tr>
<tr>
<td style="text-align:left;">
fert_rate
</td>
<td style="text-align:right;">
183
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
1.6
</td>
<td style="text-align:right;">
2.48
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
3.2
</td>
<td style="text-align:right;">
6.6
</td>
<td style="text-align:right;">
1.15
</td>
</tr>
</tbody>
</table></div>
<p>Above we show the summary of the two numerical variables.
Observe that there are 183 observations without missing values.
Using simple linear regression between the response variable fertility rate (<code>fert_rate</code>) or <span class="math inline">\(y\)</span>, and the regressor life expectancy (<code>life_exp</code>) or <span class="math inline">\(x\)</span>, the regression line is:</p>
<p><span class="math display">\[
\widehat{y}_i = b_0 + b_1 \cdot x_i.
\]</span></p>
<p>We have presented this equation in Section <a href="regression.html#model1">5.1</a>, but we now add the subscript <span class="math inline">\(i\)</span> to represent the <span class="math inline">\(i\)</span>th observation or country in the UN dataset, and we let <span class="math inline">\(i = 1\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(n\)</span> with <span class="math inline">\(n = 183\)</span> for this UN data.
The value <span class="math inline">\(x_i\)</span> represents the life expectancy value for the <span class="math inline">\(i\)</span>th member state, and <span class="math inline">\(\widehat{y}_i\)</span> is the fitted fertility rate for the <span class="math inline">\(i\)</span>th member state.
The fitted fertility rate is the result of the regression line and is typically different than the observed response <span class="math inline">\(y_i\)</span>.
The residual is given as the difference <span class="math inline">\(y_i - \widehat{y}_i\)</span>.</p>
<p>As discussed in Subsection <a href="regression.html#leastsquares">5.3.2</a>, the intercept (<span class="math inline">\(b_0\)</span>) and slope (<span class="math inline">\(b_1\)</span>) are the regression coefficients, such that the regression line is the “best-fitting” line based on the least-squares criterion.
In other words, the fitted values <span class="math inline">\(\widehat{y}\)</span> calculated using the least-squares coefficients (<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>) minimize the <em>sum of the squared residuals</em>:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
\]</span></p>
<p>As we did in Section <a href="regression.html#model1">5.1</a>, we fit the linear regression model.
By “fit” we mean to calculate the regression coefficients, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, that minimize the sum of squared residuals.
To do this in R, we use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function with the formula <code>fert_rate ~ life_exp</code> and save the solution in <code>simple_model</code>:</p>
<div class="sourceCode" id="cb465"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simple_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">fert_rate</span> <span class="op">~</span> <span class="va">life_exp</span>, data <span class="op">=</span> <span class="va">UN_data_ch10</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">simple_model</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b0
</td>
<td style="text-align:right;">
12.613
</td>
</tr>
<tr>
<td style="text-align:left;">
life_exp
</td>
<td style="text-align:left;">
b1
</td>
<td style="text-align:right;">
-0.137
</td>
</tr>
</tbody>
</table></div>
<p>The regression line is <span class="math inline">\(\widehat{y}_i = b_0 + b_1 \cdot x_i = 12.613 - 0.137 \cdot x_i\)</span>, where <span class="math inline">\(x_i\)</span> is the life expectancy for the <span class="math inline">\(i\)</span>th country and <span class="math inline">\(\widehat{y}_i\)</span> is the corresponding fitted fertility rate.
The <span class="math inline">\(b_0\)</span> coefficient is the intercept and has a meaning only if the range of values of the regressor, <span class="math inline">\(x_i\)</span>, includes zero.
Since life expectancy is always a positive value, we do not provide any interpretation to the intercept in this example.
The <span class="math inline">\(b_1\)</span> coefficient is the slope of the regression line; for any country, if the life expectancy were to increase by about one year, we would expect an associated reduction of the fertility rate by about 0.137 units.</p>
<p>We visualize the relationship of the data observed in Figure <a href="inference-for-regression.html#fig:regline-ch10">10.1</a> by plotting the scatterplot of fertility rate against life expectancy for all the UN member states with complete data.
We also include the regression line using the least-squares criterion:</p>
<div class="sourceCode" id="cb466"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">UN_data_ch10</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">life_exp</span>, y <span class="op">=</span> <span class="va">fert_rate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Life Expectancy (x)"</span>, </span>
<span>       y <span class="op">=</span> <span class="st">"Fertility Rate (y)"</span>,</span>
<span>       title <span class="op">=</span> <span class="st">"Relationship between fertility rate and life expectancy"</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span>, linewidth <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:regline-ch10"></span>
<img src="ModernDive_files/figure-html/regline-ch10-1.png" alt="Relationship with regression line." width="\textwidth"><p class="caption">
FIGURE 10.1: Relationship with regression line.
</p>
</div>
<p>Finally, we review how to determine the fitted values and residuals for observations in the dataset.
France is one of the UN member states, and suppose we want to determine the fitted fertility rate for France based on the linear regression.
We start by determining what is the location of France in the <code>UN_data_ch10</code> data frame, using <code><a href="https://tibble.tidyverse.org/reference/rownames.html">rowid_to_column()</a></code> and <code><a href="https://dplyr.tidyverse.org/reference/filter.html">filter()</a></code> with the variable country equal to “France.”
The <code><a href="https://dplyr.tidyverse.org/reference/pull.html">pull()</a></code> function converts the row number as a data frame to a single value:</p>
<div class="sourceCode" id="cb467"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">UN_data_ch10</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/rownames.html">rowid_to_column</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">country</span> <span class="op">==</span> <span class="st">"France"</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">rowid</span><span class="op">)</span></span></code></pre></div>
<pre><code>[1] 57</code></pre>
<p>France is the 57th member state in <code>UN_data_ch10</code>. Its observed fertility rate and life expectancy are:</p>
<div class="sourceCode" id="cb469"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">UN_data_ch10</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">country</span> <span class="op">==</span> <span class="st">"France"</span><span class="op">)</span></span></code></pre></div>
<pre><code># A tibble: 1 × 3
  country life_exp fert_rate
  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;
1 France     82.59       1.8</code></pre>
<p>France’s life expectancy is <span class="math inline">\(x_{57} = 82.59\)</span> years and the fertility rate is <span class="math inline">\(y_{57} =1.8\)</span>.
Using the regression line from earlier, we can determine France’s fitted fertility rate:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{y}_{57} &amp;= 12.613 - 0.137 \cdot x_{57}\\
&amp;= 12.613 - 0.137 \cdot 82.59\\
&amp;= 1.258.
\end{aligned}
\]</span></p>
<p>Based on our regression line we would expect France’s fertility rate to be 1.258.
The observed fertility rate for France was 1.8, so the residual for France is <span class="math inline">\(y_{57} - \widehat{y}_{57} = 1.8 - 1.258 = 0.542\)</span>.</p>
<p>Using R we are not required to manually calculate the fitted values and residual for each UN member state.
We do this directly using the regression model <code>simple_model</code> and the <code><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points()</a></code> function.
To do this only for France, we <code><a href="https://dplyr.tidyverse.org/reference/filter.html">filter()</a></code> the 57th observation in the data frame.</p>
<div class="sourceCode" id="cb471"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simple_model</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">ID</span> <span class="op">==</span> <span class="fl">57</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:right;">
ID
</th>
<th style="text-align:right;">
fert_rate
</th>
<th style="text-align:right;">
life_exp
</th>
<th style="text-align:right;">
fert_rate_hat
</th>
<th style="text-align:right;">
residual
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
57
</td>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
82.6
</td>
<td style="text-align:right;">
1.26
</td>
<td style="text-align:right;">
0.542
</td>
</tr></tbody>
</table></div>
<p>We can retrieve this information for each observation.
Here we show the first few rows:</p>
<div class="sourceCode" id="cb472"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simple_model</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code># A tibble: 183 × 5
      ID fert_rate life_exp fert_rate_hat residual
   &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;
 1     1       4.3    53.65         5.237   -0.937
 2     2       1.4    79.47         1.687   -0.287
 3     3       2.7    78.03         1.885    0.815
 4     4       5      62.11         4.074    0.926
 5     5       1.6    77.8          1.916   -0.316
 6     6       1.9    78.31         1.846    0.054
 7     7       1.6    76.13         2.146   -0.546
 8     8       1.6    83.09         1.189    0.411
 9     9       1.5    82.27         1.302    0.198
10    10       1.6    74.15         2.418   -0.818
# ℹ 173 more rows</code></pre>
<p>This concludes our review of material covered in Section <a href="regression.html#model1">5.1</a>. We now explain how to use this information for statistical inference.</p>
</div>
<div id="simple-linear-model" class="section level3" number="10.1.2">
<h3>
<span class="header-section-number">10.1.2</span> The model<a class="anchor" aria-label="anchor" href="#simple-linear-model"><i class="fas fa-link"></i></a>
</h3>
<p>As we did in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> on confidence intervals and <a href="hypothesis-testing.html#hypothesis-testing">9</a> on hypothesis testing, we present this problem in the context of a population and associated parameters of interest.
We then collect a random sample from this population and use it to estimate these parameters.</p>
<p>We assume that this population has a response variable (<span class="math inline">\(Y\)</span>), an explanatory variable (<span class="math inline">\(X\)</span>), and there is a <em>statistical linear relationship</em> between these variables, given by the linear model</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 \cdot X + \epsilon,\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the population intercept and <span class="math inline">\(\beta_1\)</span> is the population slope.
These are now the parameters of the model that alongside the explanatory variable (<span class="math inline">\(X\)</span>) produce the equation of a line.
The statistical part of this relationship is given by <span class="math inline">\(\epsilon\)</span>, a random variable called the <em>error term</em>.
The error term accounts for the portion of <span class="math inline">\(Y\)</span> that is not explained by the line.</p>
<p>We make additional assumptions about the distribution of the error term, <span class="math inline">\(\epsilon\)</span>.
The assumed expected value of the error term is zero, and the assumed standard deviation is equal to a positive constant called <span class="math inline">\(\sigma\)</span>, or in mathematical terms: <span class="math inline">\(E(\epsilon) = 0\)</span> and <span class="math inline">\(SD(\epsilon) = \sigma.\)</span></p>
<p>We review the meaning of these quantities.
If you were to take a large number of observations from this population, we would expect the error terms sometimes to be greater than zero and sometimes less than zero, but on average, be equal to zero.
Similarly, some error terms will be very close to zero and others very far from zero, but on average, we would expect them to be roughly <span class="math inline">\(\sigma\)</span> units away from zero.</p>
<p>Recall the square of the standard deviation is called the variance, so <span class="math inline">\(Var(\epsilon) = \sigma^2\)</span>.
The variance of the error term is equal to <span class="math inline">\(\sigma^2\)</span> regardless of the value of <span class="math inline">\(X\)</span>.
This property is called <em>homoskedasticity</em> or constancy of the variance.
It will be useful later on in our analysis.</p>
</div>
<div id="sample-regression-inference" class="section level3" number="10.1.3">
<h3>
<span class="header-section-number">10.1.3</span> Using a sample for inference<a class="anchor" aria-label="anchor" href="#sample-regression-inference"><i class="fas fa-link"></i></a>
</h3>
<p>As we did in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> and <a href="hypothesis-testing.html#hypothesis-testing">9</a>, we use a sample to estimate the parameters in the population.
We use data collected from the Old Faithful Geyser in Yellowstone National Park in Wyoming, USA.
This dataset contains the <code>duration</code> of the geyser eruption in seconds and the <code>waiting</code> time to the next eruption in minutes.
The duration of the current eruption can help determine fairly well the waiting time to the next eruption.
For this example, we use a sample of data collected by volunteers and saved on the website <a href="https://geysertimes.org/"><em>https://geysertimes.org/</em></a> between June 1st, 2024 and August 19th, 2024.</p>
<p>These data are stored in the <code>old_faithful_2024</code> data frame in the <code>moderndive</code> package.
While data collected by volunteers are not a random sample, as the volunteers could introduce some sort of bias, the eruptions selected by the volunteers had no specific patterns.
Further, beyond the individual skill of each volunteer measuring the times appropriately, no response bias or preference seems to be present.
Therefore, it seems safe to consider the data a random sample. The first ten rows are shown here:</p>
<div class="sourceCode" id="cb474"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_faithful_2024</span></span></code></pre></div>
<pre><code># A tibble: 114 × 6
   eruption_id date        time waiting webcam duration
         &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;
 1     1473854 2024-08-19   538     180 Yes         235
 2     1473352 2024-08-15  1541     184 Yes         259
 3     1473337 2024-08-15  1425     116 Yes         137
 4     1473334 2024-08-15  1237     188 Yes         222
 5     1473331 2024-08-15  1131     106 Yes         105
 6     1473328 2024-08-15   944     187 Yes         180
 7     1473207 2024-08-14  1231     182 Yes         244
 8     1473201 2024-08-14  1041     190 Yes         278
 9     1473137 2024-08-13  1810     138 Yes         249
10     1473108 2024-08-13  1624     186 Yes         262
# ℹ 104 more rows</code></pre>
<p>By looking at the first row we can tell, for example, that an eruption on August 19, 2024, at 5:38 AM lasted 235 seconds, and the waiting time for the next eruption was 180 minutes.
We next display the summary for these two variables:</p>
<div class="sourceCode" id="cb476"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_faithful_2024</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">duration</span>, <span class="va">waiting</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="moderndive.github.io/moderndive/reference/tidy_summary.html">tidy_summary</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
column
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
Q1
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
Q3
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
sd
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
duration
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
180
</td>
<td style="text-align:right;">
217
</td>
<td style="text-align:right;">
240
</td>
<td style="text-align:right;">
259
</td>
<td style="text-align:right;">
300
</td>
<td style="text-align:right;">
59.0
</td>
</tr>
<tr>
<td style="text-align:left;">
waiting
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
102
</td>
<td style="text-align:right;">
139
</td>
<td style="text-align:right;">
160
</td>
<td style="text-align:right;">
176
</td>
<td style="text-align:right;">
184
</td>
<td style="text-align:right;">
201
</td>
<td style="text-align:right;">
29.9
</td>
</tr>
</tbody>
</table></div>
<p>We have a sample of 114 eruptions, lasting between 99 seconds and 300 seconds, and the waiting time to the next eruption was between 102 minutes and 201 minutes.
Observe that each observation is a pair of values, the value of the explanatory variable (<span class="math inline">\(X\)</span>) and the value of the response (<span class="math inline">\(Y\)</span>). The sample takes the form:</p>
<p><span class="math display">\[\begin{array}{c}
(x_1,y_1)\\
(x_2, y_2)\\
\vdots\\
(x_n, y_n)\\
\end{array}\]</span></p>
<p>where, for example, <span class="math inline">\((x_2, y_2)\)</span> is the pair of explanatory and response values, respectively, for the second observation in the sample.
More generally, we denote the <span class="math inline">\(i\)</span>th pair by <span class="math inline">\((x_i, y_i)\)</span>, where <span class="math inline">\(x_i\)</span> is the observed value of the explanatory variable <span class="math inline">\(X\)</span> and <span class="math inline">\(y_i\)</span> is the observed value of the response variable <span class="math inline">\(Y\)</span>.
Since the sample has <span class="math inline">\(n\)</span> observations we let <span class="math inline">\(i=1\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(n\)</span>.</p>
<p>In our example <span class="math inline">\(n = 114\)</span>, and <span class="math inline">\((x_2, y_2) = (259, 184)\)</span>.
Figure <a href="inference-for-regression.html#fig:geyserplot1">10.2</a> shows the scatterplot for the entire sample with some transparency set to check for overplotting:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:geyserplot1"></span>
<img src="ModernDive_files/figure-html/geyserplot1-1.png" alt="Scatterplot of relationship of eruption duration and waiting time." width="\textwidth"><p class="caption">
FIGURE 10.2: Scatterplot of relationship of eruption duration and waiting time.
</p>
</div>
<p>The relationship seems positive and, to some extent, linear.</p>
</div>
<div id="least-squares" class="section level3" number="10.1.4">
<h3>
<span class="header-section-number">10.1.4</span> The method of least squares<a class="anchor" aria-label="anchor" href="#least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>If the association of these variables is linear or approximately linear, we can apply the linear model described in Subsection <a href="inference-for-regression.html#simple-linear-model">10.1.2</a> to each observation in the sample:</p>
<p><span class="math display">\[\begin{aligned}
y_1 &amp;= \beta_0 + \beta_1 \cdot x_1 + \epsilon_1\\
y_2 &amp;= \beta_0 + \beta_1 \cdot x_2 + \epsilon_2\\
\vdots &amp; \phantom{= \beta_0 + \beta_1 \cdot + \epsilon_2 +}\vdots \\
y_n &amp;= \beta_0 + \beta_1 \cdot x_n + \epsilon_n
\end{aligned}\]</span></p>
<p>We want to be able to use this model to describe the relationship between the explanatory variable and the response, but the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown to us.
We estimate these parameters using the random sample by applying the <em>least-squares</em> method introduced in Section <a href="regression.html#model1">5.1</a>.
We compute the estimators for the intercept (<span class="math inline">\(\beta_0\)</span>) and slope (<span class="math inline">\(\beta_1\)</span>) that minimize the <em>sum of squared residuals</em>:</p>
<p><span class="math display">\[\sum_{i=1}^n \left[y_i - (\beta_0 + \beta_1 \cdot x_i)\right]^2.\]</span></p>
<p>This is an optimization problem and to solve it analytically we require calculus and the topic goes beyond the scope of this book.
We provide a sketch of the solution here for those familiar with the method: using the expression above we find the partial derivative with respect to <span class="math inline">\(\beta_0\)</span> and equate that expression to zero, the partial derivative with respect to <span class="math inline">\(\beta_1\)</span> and equate that expression to zero, and use those two equations to solve for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
The solutions are the regression coefficients introduced first in Section <a href="regression.html#model1">5.1</a>: <span class="math inline">\(b_0\)</span> is the estimator of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(b_1\)</span> is the estimator of <span class="math inline">\(\beta_1\)</span>.
They are called the <em>least squares estimators</em> and their mathematical expressions are:</p>
<p><span class="math display">\[b_1 =  \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} \text{ and } b_0 = \bar y - b_1 \cdot \bar x.\]</span></p>
<p>Furthermore, an <em>estimator</em> for the standard deviation of <span class="math inline">\(\epsilon_i\)</span> is given by</p>
<p><span class="math display">\[s = \sqrt{\frac{\sum_{i=1}^n \left[y_i - (b_0 + b_1 \cdot x_i)\right]^2}{n-2}} = \sqrt{\frac{\sum_{i=1}^n \left(y_i - \widehat{y}_i\right)^2}{n-2}}.\]</span></p>
<p>These or equivalent calculations are done in R when using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function.
For <code>old_faithful_2024</code> we get the results shown in Table
<a href="inference-for-regression.html#tab:regtable-ch10-1">10.1</a>:</p>
<div class="sourceCode" id="cb477"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">model_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span>, data <span class="op">=</span> <span class="va">old_faithful_2024</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the coefficients and standard deviation for the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model_1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">model_1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-ch10-1">TABLE 10.1: </span>Old Faithful geyser linear regression coefficients
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b0
</td>
<td style="text-align:right;">
79.459
</td>
</tr>
<tr>
<td style="text-align:left;">
duration
</td>
<td style="text-align:left;">
b1
</td>
<td style="text-align:right;">
0.371
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
s
</td>
<td style="text-align:right;">
20.370
</td>
</tr>
</tbody>
</table></div>
<p>Based on these data and assuming the linear model is appropriate, we can say that for every additional second that an eruption lasts, the waiting time to the next eruption increases, on average, by 0.37 minutes.
Any eruption lasts longer than zero seconds, so the intercept has no meaningful interpretation in this example.
Finally, we roughly expect the waiting time for the next eruption to be 20.37 minutes away from the regression line value, on average.</p>
</div>
<div id="properties-least-squares" class="section level3" number="10.1.5">
<h3>
<span class="header-section-number">10.1.5</span> Properties of the least squares estimators<a class="anchor" aria-label="anchor" href="#properties-least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>The least squares method produces the <em>best-fitting</em> line by selecting the least squares estimators, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, that make the sum of residual squares the smallest possible.
But the choice of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> depends on the sample observed.
For every random sample taken from the data, different values for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> will be determined.
In that sense, the least squares estimators, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, are random variables and as such, they have very useful properties:</p>
<ul>
<li>
<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, or using mathematical notation: <span class="math inline">\(E(b_0) = \beta_0\)</span> and <span class="math inline">\(E(b_1) = \beta_1\)</span>.
This means that, for some random samples, <span class="math inline">\(b_1\)</span> will be greater than <span class="math inline">\(\beta_1\)</span> and for others less than <span class="math inline">\(\beta_1\)</span>.
On average, <span class="math inline">\(b_1\)</span> will be equal to <span class="math inline">\(\beta_1\)</span>.</li>
<li>
<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are linear combinations of the observed responses <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(y_n\)</span>.
This means that, for example for <span class="math inline">\(b_1\)</span>, there are known constants <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(c_n\)</span> such that <span class="math inline">\(b_1 = \sum_{i=1}^n c_iy_i\)</span>.</li>
<li>
<span class="math inline">\(s^2\)</span> is an unbiased estimator of the variance <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>These properties will be useful in the next subsection, once we perform theory-based inference for regression.</p>
</div>
<div id="relating-basic-regression-to-other-methods" class="section level3" number="10.1.6">
<h3>
<span class="header-section-number">10.1.6</span> Relating basic regression to other methods<a class="anchor" aria-label="anchor" href="#relating-basic-regression-to-other-methods"><i class="fas fa-link"></i></a>
</h3>
<p>To wrap-up this section, we’ll be investigating how regression relates to two different statistical techniques. One of them was covered already in this book, the difference in sample means, and the other is new to the text but is related, ANOVA. We’ll see how both can be represented in the regression framework. <!-- The hope is that this subsection helps you to tie together many of the concepts you've seen in the Statistical/Data Modeling and Statistical Inference parts of this book.--></p>
<div id="two-sample-difference-in-means" class="section level4 unnumbered">
<h4>Two-sample difference in means<a class="anchor" aria-label="anchor" href="#two-sample-difference-in-means"><i class="fas fa-link"></i></a>
</h4>
<p>The two-sample difference in means is a common statistical technique used to compare the means of two groups as seen in Section <a href="hypothesis-testing.html#ht-case-study">9.6</a>. It is often used to determine if there is a significant difference in the mean response between two groups, such as a treatment group and a control group. The two-sample difference in means can be represented in the regression framework by using a dummy variable to represent the two groups.</p>
<p>Let’s again consider the <code>movies_sample</code> data frame in the <code>moderndive</code> package. We’ll compare once more the average rating for the genres of “Action” versus “Romance.” We can use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a linear model with a dummy variable for the genre and then use <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a>:</p>
<div class="sourceCode" id="cb478"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod_diff_means</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">rating</span> <span class="op">~</span> <span class="va">genre</span>, data <span class="op">=</span> <span class="va">movies_sample</span><span class="op">)</span></span>
<span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">mod_diff_means</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:diff-means-reg">TABLE 10.2: </span>Regression table for two-sample difference in means example
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
5.28
</td>
<td style="text-align:right;">
0.265
</td>
<td style="text-align:right;">
19.92
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
4.746
</td>
<td style="text-align:right;">
5.80
</td>
</tr>
<tr>
<td style="text-align:left;">
genre: Romance
</td>
<td style="text-align:right;">
1.05
</td>
<td style="text-align:right;">
0.364
</td>
<td style="text-align:right;">
2.88
</td>
<td style="text-align:right;">
0.005
</td>
<td style="text-align:right;">
0.321
</td>
<td style="text-align:right;">
1.77
</td>
</tr>
</tbody>
</table></div>
<p>Note from Table <a href="inference-for-regression.html#tab:diff-means-reg">10.2</a> that <code>p_value</code> for the <code>genre: Romance</code> row is the <span class="math inline">\(p\)</span>-value for the hypothesis test of</p>
<p><span class="math display">\[
H_0: \text{action and romance have the same mean rating}
\]</span>
<span class="math display">\[
H_A: \text{action and romance have different mean ratings}
\]</span></p>
<p>This <span class="math inline">\(p\)</span>-value result matches closely with what was found in Section <a href="hypothesis-testing.html#ht-case-study">9.6</a>, but here we are using a theory-based approach with a linear model. The <code>estimate</code> for the <code>genre: Romance</code> row is the observed difference in means between the “Action” and “Romance” genres that we also saw in Section <a href="hypothesis-testing.html#ht-case-study">9.6</a>, except the sign is switched since the “Action” genre is the reference level.</p>
</div>
<div id="anova" class="section level4 unnumbered">
<h4>ANOVA<a class="anchor" aria-label="anchor" href="#anova"><i class="fas fa-link"></i></a>
</h4>
<p>ANOVA, or analysis of variance, is a statistical technique used to compare the means of three or more groups by seeing if there is a statistically significant difference between the means of multiple groups. ANOVA can be represented in the regression framework by using dummy variables to represent the groups. Let’s say we wanted to compare the <code>popularity</code> (numeric) values in the <code>spotify_by_genre</code> data frame from the <code>moderndive</code> package across the genres of <code>country</code>, <code>hip-hop</code>, and <code>rock</code>. We use the <code><a href="https://dplyr.tidyverse.org/reference/slice.html">slice_sample()</a></code> function after narrowing in on our selected columns and filtered rows of interest to see what a few rows of this data frame look like in Table <a href="inference-for-regression.html#tab:spotify-for-anova-slice-five">10.3</a>.</p>
<div class="sourceCode" id="cb479"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">spotify_for_anova</span> <span class="op">&lt;-</span> <span class="va">spotify_by_genre</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">artists</span>, <span class="va">track_name</span>, <span class="va">popularity</span>, <span class="va">track_genre</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">track_genre</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"country"</span>, <span class="st">"hip-hop"</span>, <span class="st">"rock"</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<div class="sourceCode" id="cb480"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">spotify_for_anova</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice_sample</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>

<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:spotify-for-anova-slice-five">TABLE 10.3: </span>Five randomly selected rows from <code>spotify_for_anova</code>
</caption>
<thead><tr>
<th style="text-align:left;">
artists
</th>
<th style="text-align:left;">
track_name
</th>
<th style="text-align:right;">
popularity
</th>
<th style="text-align:left;">
track_genre
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Counting Crows
</td>
<td style="text-align:left;">
Mr. Jones
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
rock
</td>
</tr>
<tr>
<td style="text-align:left;">
Luke Bryan
</td>
<td style="text-align:left;">
Country Girl (Shake It For Me)
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
country
</td>
</tr>
<tr>
<td style="text-align:left;">
Salebarbes
</td>
<td style="text-align:left;">
Marcher l’plancher - Live
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:left;">
country
</td>
</tr>
<tr>
<td style="text-align:left;">
Darius Rucker
</td>
<td style="text-align:left;">
Wagon Wheel
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
country
</td>
</tr>
<tr>
<td style="text-align:left;">
Billy Joel
</td>
<td style="text-align:left;">
Vienna
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:left;">
rock
</td>
</tr>
</tbody>
</table></div>
<p>Before we fit a linear model, let’s take a look at the boxplot of <code>track_genre</code> versus <code>popularity</code> in Figure <a href="inference-for-regression.html#fig:pop-by-genre-plot">10.3</a> to see if there are any differences in the distributions of the three genres.</p>
<div class="sourceCode" id="cb481"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">spotify_for_anova</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">track_genre</span>, y <span class="op">=</span> <span class="va">popularity</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Genre"</span>, y <span class="op">=</span> <span class="st">"Popularity"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pop-by-genre-plot"></span>
<img src="ModernDive_files/figure-html/pop-by-genre-plot-1.png" alt="Boxplot of popularity by genre." width="\textwidth"><p class="caption">
FIGURE 10.3: Boxplot of popularity by genre.
</p>
</div>
<p>We can also compute the mean <code>popularity</code> grouping by <code>track_genre</code>:</p>
<div class="sourceCode" id="cb482"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mean_popularities_by_genre</span> <span class="op">&lt;-</span> <span class="va">spotify_for_anova</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">track_genre</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>mean_popularity <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">popularity</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">mean_popularities_by_genre</span></span></code></pre></div>
<pre><code># A tibble: 3 × 2
  track_genre mean_popularity
  &lt;chr&gt;                 &lt;dbl&gt;
1 country              17.028
2 hip-hop              37.759
3 rock                 19.001</code></pre>
<p>We can use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a linear model with dummy variables for the genres. We’ll then use the <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a> function to get the regression table in Table <a href="inference-for-regression.html#tab:anova-reg-table">10.4</a>.</p>
<div class="sourceCode" id="cb484"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod_anova</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">popularity</span> <span class="op">~</span> <span class="va">track_genre</span>, data <span class="op">=</span> <span class="va">spotify_for_anova</span><span class="op">)</span></span>
<span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">mod_anova</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:anova-reg-table">TABLE 10.4: </span>Regression table for ANOVA example
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
17.03
</td>
<td style="text-align:right;">
0.976
</td>
<td style="text-align:right;">
17.45
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
15.114
</td>
<td style="text-align:right;">
18.94
</td>
</tr>
<tr>
<td style="text-align:left;">
track_genre: hip-hop
</td>
<td style="text-align:right;">
20.73
</td>
<td style="text-align:right;">
1.380
</td>
<td style="text-align:right;">
15.02
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
18.025
</td>
<td style="text-align:right;">
23.44
</td>
</tr>
<tr>
<td style="text-align:left;">
track_genre: rock
</td>
<td style="text-align:right;">
1.97
</td>
<td style="text-align:right;">
1.380
</td>
<td style="text-align:right;">
1.43
</td>
<td style="text-align:right;">
0.153
</td>
<td style="text-align:right;">
-0.733
</td>
<td style="text-align:right;">
4.68
</td>
</tr>
</tbody>
</table></div>
<p>The <code>estimate</code> for the <code>track_genre: hip-hop</code> and <code>track_genre: rock</code> rows are the differences in means between the “hip-hop” and “country” genres and the “rock” and “country” genres, respectively. The “country” genre is the reference level. These values match up (with some rounding differences) to what is shown in <code>mean_popularities_by_genre</code>.</p>
<p>The <code>p_value</code> column corresponds to <code>hip-hop</code> having a statistically higher mean <code>popularity</code> compared to <code>country</code> with a value of close to 0 (reported as 0). It also gives us that <code>rock</code> does not have a statistically significant <span class="math inline">\(p\)</span>-value at 0.153, which would make us inclined to say that <code>rock</code> does not have a significantly higher popularity compared to <code>country</code>.</p>
<p>The traditional ANOVA doesn’t give this level of granularity. It can be performed using the <code><a href="https://rdrr.io/r/stats/aov.html">aov()</a></code> function and the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function via a pipe (<code>|&gt;</code>):</p>
<div class="sourceCode" id="cb485"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">popularity</span> <span class="op">~</span> <span class="va">track_genre</span>, data <span class="op">=</span> <span class="va">spotify_for_anova</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>Analysis of Variance Table

Response: popularity
              Df  Sum Sq Mean Sq F value              Pr(&gt;F)    
track_genre    2  261843  130922     137 &lt;0.0000000000000002 ***
Residuals   2997 2855039     953                                
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>The small <span class="math inline">\(p\)</span>-value here of <code>2.2e-16</code> is very close to 0, which would lead us to reject the null hypothesis that the mean popularities are equal across the three genres. This is consistent with the results we found using the linear model. The traditional ANOVA results do not tell us which means are different from each other though, but the linear model does. ANOVA tells us only that a difference exists in the means of the groups.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.1)</strong> What does the error term <span class="math inline">\(\epsilon\)</span> in the linear model <span class="math inline">\(Y = \beta_0 + \beta_1 \cdot X + \epsilon\)</span> represent?</p>
<ul>
<li>A. The exact value of the response variable.</li>
<li>B. The predicted value of the response variable based on the model.</li>
<li>C. The part of the response variable not explained by the line.</li>
<li>D. The slope of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
</ul>
<p><strong>(LC10.2)</strong> Which of the following is a property of the least squares estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>?</p>
<ul>
<li>A. They are biased estimators of the population parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
<li>B. They are linear combinations of the observed responses <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>.</li>
<li>C. They are always equal to the population parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
<li>D. They depend on the specific values of the explanatory variable <span class="math inline">\(X\)</span> only.</li>
</ul>
<p><strong>(LC10.3)</strong> How can the difference in means between two groups be represented in a linear regression model?</p>
<ul>
<li>A. By adding an interaction term between the groups and the response variable.</li>
<li>B. By fitting separate regression lines for each group and comparing their slopes.</li>
<li>C. By including a dummy variable to represent the groups.</li>
<li>D. By subtracting the mean of one group from the mean of the other and using this difference as the predictor.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
</div>
<div id="theory-simple-regression" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Theory-based inference for simple linear regression<a class="anchor" aria-label="anchor" href="#theory-simple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>This section introduces the conceptual framework needed for theory-based inference for regression (see Subsections <a href="inference-for-regression.html#framework-simple-lm">10.2.1</a> and <a href="inference-for-regression.html#se-simple-lm">10.2.2</a>) and discusses the two most prominent methods for inference: confidence intervals (Subsection <a href="inference-for-regression.html#conf-intervals-b0-b1">10.2.3</a>) and hypothesis tests (Subsection <a href="inference-for-regression.html#hypo-test-simple-lm">10.2.4</a>).
Some of this material is slightly more technical than other sections in this chapter, but most of the material is illustrated by working with a real example and interpretations and explanations complement the theory. Subsection <a href="inference-for-regression.html#regression-table">10.2.5</a> presents the R code needed to calculate relevant quantities for inference. Feel free to read this section first.</p>
<div id="framework-simple-lm" class="section level3" number="10.2.1">
<h3>
<span class="header-section-number">10.2.1</span> Conceptual framework<a class="anchor" aria-label="anchor" href="#framework-simple-lm"><i class="fas fa-link"></i></a>
</h3>
<p>We start by reviewing the assumptions of the linear model. We continue using the <code>old_faithful_2024</code> to illustrate some of this framework. Recall that we have a random sample of <span class="math inline">\(n = 114\)</span> observations.
Since we assume a linear relationship between the <code>duration</code> of an eruption and the <code>waiting</code> time to the next eruption, we can express the linear relationship for the <span class="math inline">\(i\)</span>th observation as <span class="math inline">\(y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i\)</span> for <span class="math inline">\(i=1,\dots,n\)</span>. Observe that <span class="math inline">\(x_i\)</span> is the <code>duration</code> of the <span class="math inline">\(i\)</span>th eruption in the sample, <span class="math inline">\(y_i\)</span> is the <code>waiting</code> time to the next eruption, and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the population parameters that are considered constant.
The error term <span class="math inline">\(\epsilon_i\)</span> is a random variable that represents how different the observed response <span class="math inline">\(y_i\)</span> is from the expected response <span class="math inline">\(\beta_0 + \beta_1 \cdot x_i\)</span>.</p>
<p>We can illustrate the role of the error term using two observations from our <code>old_faithful_2024</code> dataset.
We assume for now that the linear model is appropriate and truly represents the relationship between <code>duration</code> and <code>waiting</code> times.
We select the 49th and 51st observations in our sample by using the function <code><a href="https://dplyr.tidyverse.org/reference/slice.html">slice()</a></code> with the corresponding rows:</p>
<div class="sourceCode" id="cb487"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_faithful_2024</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">49</span>, <span class="fl">51</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code># A tibble: 2 × 6
  eruption_id date        time waiting webcam duration
        &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;
1     1469584 2024-07-18  1117     139 Yes         236
2     1469437 2024-07-17  1157     176 Yes         236</code></pre>
<p>Observe that the <code>duration</code> time is the same for both observations, but the response <code>waiting</code> time is different.
Assuming that the linear model is appropriate, both responses can be expressed as:</p>
<p><span class="math display">\[\begin{aligned}
y_{49} &amp;= \beta_0 + \beta_1 \cdot 236 + \epsilon_{49}\\
y_{51} &amp;= \beta_0 + \beta_1 \cdot 236 + \epsilon_{51}
\end{aligned}\]</span></p>
<p>but <span class="math inline">\(y_{49} = 139\)</span> and <span class="math inline">\(y_{51} = 176\)</span>.
The difference in responses is due to the error term as it accounts for variation in the response not accounted for by the linear model.</p>
<p>In the linear model the error term <span class="math inline">\(\epsilon_i\)</span> has expected value <span class="math inline">\(E(\epsilon_i) = 0\)</span> and standard deviation <span class="math inline">\(SD(\epsilon_i) = \sigma\)</span>.
Since a random sample is taken, we assume that any two error terms <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> for any two different eruptions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are independent.</p>
<p>In order to perform the theory-based inference we require one additional assumption.
We let the error term be normally distributed with an expected value (mean) equal to zero and a standard deviation equal to <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[\epsilon_i \sim Normal(0, \sigma).\]</span></p>
<p>The population parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are constants.
Similarly, the <code>duration</code> of the <span class="math inline">\(i\)</span>th eruption, <span class="math inline">\(x_i\)</span>, is known and also a constant.
Therefore, the expression <span class="math inline">\(\beta_0 + \beta_1 \cdot x_i\)</span> is a constant. By contrast, <span class="math inline">\(\epsilon_i\)</span> is a normally distributed random variable.</p>
<p>The response <span class="math inline">\(y_i\)</span> (the <code>waiting</code> time for the <span class="math inline">\(i\)</span>th eruption to the next) is the sum of the constant <span class="math inline">\(\beta_0 + \beta_1 \cdot x_i\)</span> and the normally distributed random variable <span class="math inline">\(\epsilon_i\)</span>.
Based on properties of random variables and the normal distribution, we can state that <span class="math inline">\(y_i\)</span> is also a normally distribution random variable with mean equal to <span class="math inline">\(\beta_0 + \beta_1 \cdot x_i\)</span> and standard deviation equal to <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[y_i \sim Normal(\beta_0 + \beta_1 x_i\,,\, \sigma)\]</span></p>
<p>for <span class="math inline">\(i=1,\dots,n\)</span>.
Since <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> are independent, <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> are also independent for any <span class="math inline">\(i \ne j\)</span>.</p>
<p>In addition, as stated in Subsection <a href="inference-for-regression.html#properties-least-squares">10.1.5</a>, the least-squares estimator <span class="math inline">\(b_1\)</span> is a linear combination of the random variables <span class="math inline">\(y_1, \dots, y_n\)</span>.
So <span class="math inline">\(b_1\)</span> is also a random variable!
What does this mean?
The coefficient for the slope results from <em>a particular sample</em> of <span class="math inline">\(n\)</span> pairs of <code>duration</code> and <code>waiting</code> times.
If we collected a different sample of <span class="math inline">\(n\)</span> pairs, the coefficient for the slope would likely be different due to <em>sampling variation</em>.</p>
<p>Say we hypothetically collect many random samples of pairs of <code>duration</code> and <code>waiting</code> times, and using the least-squares method compute the slope <span class="math inline">\(b_1\)</span> for each of these samples.
These slopes would form the sampling distribution of <span class="math inline">\(b_1\)</span>, which we discussed in Subsection <a href="sampling.html#sampling-variation">7.3.4</a> in the context of sample proportions.
What we would learn is that, because <span class="math inline">\(y_1, \dots, y_n\)</span> are normally distributed and <span class="math inline">\(b_1\)</span> is a linear combination of these random variables, <span class="math inline">\(b_1\)</span> is also normally distributed.
After some calculations that go beyond the scope of this book but take into account properties of the expected value and standard deviation of the responses <span class="math inline">\(y_1, \dots, y_n\)</span>, it can be shown that:</p>
<p><span class="math display">\[b_1 \sim Normal \left(\beta_1\,,\, \frac{\sigma}{\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}}\right)\]</span></p>
<p>That is, <span class="math inline">\(b_1\)</span> is normally distributed with expected value <span class="math inline">\(\beta_1\)</span> and standard deviation equal to the expression above (inside the parentheses and after the comma).
Similarly, <span class="math inline">\(b_0\)</span> is a linear combination of <span class="math inline">\(y_1, \dots, y_n\)</span> and using properties of the expected value and standard deviation of the responses, we get:</p>
<p><span class="math display">\[b_0 \sim Normal \left(\beta_0\,,\, \sigma\sqrt{\frac1n + \frac{\bar x^2}{\sum_{i=1}^n(x_i - \bar x)^2}}\right)\]</span></p>
<p>We can also standardize the least-square estimators such that</p>
<p><span class="math display">\[z_0 = \frac{b_0 - \beta_0}{\left(\sigma\sqrt{\frac1n + \frac{\bar x^2}{\sum_{i=1}^n(x_i - \bar x)^2}}\right)}\qquad\text{ and }\qquad z_1 = \frac{b_1 - \beta_1}{\left(\frac{\sigma}{\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}}\right)}\]</span></p>
<p>are the corresponding standard normal distributions.</p>
</div>
<div id="se-simple-lm" class="section level3" number="10.2.2">
<h3>
<span class="header-section-number">10.2.2</span> Standard errors for least-squares estimators<a class="anchor" aria-label="anchor" href="#se-simple-lm"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that in Chapter <a href="sampling.html#sampling">7</a> and in Subsection <a href="sampling.html#CLT-mean">7.4.6</a> we discussed that, due to the Central Limit Theorem, the distribution of the sample mean <span class="math inline">\(\overline X\)</span> was approximately normal with mean equal to the parameter <span class="math inline">\(\mu\)</span> and standard deviation equal to <span class="math inline">\(\sigma/\sqrt n\)</span>.
We then used the estimated standard error of <span class="math inline">\(\overline X\)</span> to construct confidence intervals and hypothesis tests.</p>
<p>An analogous treatment is now used to construct confidence intervals and hypothesis tests for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.
Observe in the equations above that the standard deviations for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are constructed using the sample size <span class="math inline">\(n\)</span>, the values of the explanatory variables, their means, and the standard deviation of <span class="math inline">\(y_i\)</span> (<span class="math inline">\(\sigma\)</span>).
While most of these values are known to us, <span class="math inline">\(\sigma\)</span> is typically not.</p>
<p>Instead, we estimate <span class="math inline">\(\sigma\)</span> using the estimator of the standard deviation, <span class="math inline">\(s\)</span>, introduced in Subsection <a href="inference-for-regression.html#least-squares">10.1.4</a>.
The estimated standard deviation of <span class="math inline">\(b_1\)</span> is called the <em>standard error</em> of <span class="math inline">\(b_1\)</span>, and it is given by:</p>
<p><span class="math display">\[SE(b_1) = \frac{s}{\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}}.\]</span></p>
<p>Recall that the <em>standard error</em> is the standard deviation of any point estimate computed from a sample.
The <em>standard error</em> of <span class="math inline">\(b_1\)</span> quantifies how much variation the estimator of the slope <span class="math inline">\(b_1\)</span> may have for different random samples.
The larger the standard error, the more variation we would expect in the estimated slope <span class="math inline">\(b_1\)</span>.
Similarly, the <em>standard error</em> of <span class="math inline">\(b_0\)</span> is:</p>
<p><span class="math display">\[SE(b_0) = s\sqrt{\frac1n + \frac{\bar x^2}{\sum_{i=1}^n(x_i - \bar x)^2}}\]</span></p>
<p>As was discussed in Subsection <a href="confidence-intervals.html#t-distribution-CI">8.1.4</a>, when using the estimator <span class="math inline">\(s\)</span> instead of the parameter <span class="math inline">\(\sigma\)</span>, we are introducing additional uncertainty in our calculations.
For example, we can standardize <span class="math inline">\(b_1\)</span> using</p>
<p><span class="math display">\[t = \frac{b_1 - \beta_1}{SE(b_1)}.\]</span></p>
<p>Because we are using <span class="math inline">\(s\)</span> to calculate <span class="math inline">\(SE(b_1)\)</span>, the value of the standard error changes from sample to sample, and this additional uncertainty makes the distribution of the test statistic <span class="math inline">\(t\)</span> no longer normal.
Instead, it follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.
The loss of two degrees of freedom relates to the fact that we are trying to estimate two parameters in the linear model: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We are ready to use this information to perform inference for the least-square estimators, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.</p>
</div>
<div id="conf-intervals-b0-b1" class="section level3" number="10.2.3">
<h3>
<span class="header-section-number">10.2.3</span> Confidence intervals for the least-squares estimators<a class="anchor" aria-label="anchor" href="#conf-intervals-b0-b1"><i class="fas fa-link"></i></a>
</h3>
<p>A 95% confidence interval for <span class="math inline">\(\beta_1\)</span> can be thought of as a range of plausible values for the population slope <span class="math inline">\(\beta_1\)</span> of the linear relationship between <code>duration</code> and <code>waiting</code> times.
In general, if the sampling distribution of an estimator is normal or approximately normal, the confidence interval for the relevant parameter is</p>
<p><span class="math display">\[
\text{point estimate} \pm \text{margin of error} = \text{point estimate} \pm (\text{critical value} \cdot \text{standard error}).
\]</span></p>
<p>The formula for a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> is given by <span class="math inline">\(b_1 \pm q \cdot SE(b_1)\)</span> where the critical value <span class="math inline">\(q\)</span> is determined by the level of confidence required, the sample size used, and the corresponding degrees of freedom needed for the <span class="math inline">\(t\)</span>-distribution.
We now illustrate how to find the 95% confidence interval for the slope in the Old Faithful example manually, but we show later how to do this directly in R using the function <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a>.
First, observe that <span class="math inline">\(n = 114\)</span>, so the degrees of freedom are <span class="math inline">\(n-2 = 112\)</span>. The critical value for a 95% confidence interval on a <span class="math inline">\(t\)</span>-distribution with 112 degrees of freedom is <span class="math inline">\(q = 1.981\)</span>. Second, the estimates <span class="math inline">\(b_0\)</span>, <span class="math inline">\(b_1\)</span>, and <span class="math inline">\(s\)</span> were found earlier and are shown here again in Table <a href="inference-for-regression.html#tab:regtable-ch10-2">10.5</a>:</p>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-ch10-2">TABLE 10.5: </span>Old Faithful linear regression coefficients
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b0
</td>
<td style="text-align:right;">
79.459
</td>
</tr>
<tr>
<td style="text-align:left;">
duration
</td>
<td style="text-align:left;">
b1
</td>
<td style="text-align:right;">
0.371
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
s
</td>
<td style="text-align:right;">
20.370
</td>
</tr>
</tbody>
</table></div>
<p>Third, the standard error for <span class="math inline">\(b_1\)</span> using the formula presented earlier is:</p>
<p><span class="math display">\[SE(b_1) = \frac{s}{\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}} = \frac{20.37}{627.583} = 0.032.\]</span></p>
<p>Finally, the 95% confidence interval for <span class="math inline">\(\beta_1\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
b_1 &amp;\pm q \cdot SE(b_1)\\
&amp;= 0.371 \pm 1.981\cdot 0.032\\
&amp;= (0.308 , 0.434)
\end{aligned}\]</span></p>
<p>We are 95% confident that the population slope <span class="math inline">\(\beta_1\)</span> is a number between 0.308 and 0.434.</p>
<p>The construction of a 95% confidence interval for <span class="math inline">\(\beta_0\)</span> follows exactly the same steps using <span class="math inline">\(b_0\)</span>, <span class="math inline">\(SE(b_0)\)</span>, and the same critical value <span class="math inline">\(q\)</span> as the degrees of freedom for the <span class="math inline">\(t\)</span>-distribution are exactly the same, <span class="math inline">\(n-2\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
b_0 &amp;\pm q \cdot SE(b_0)\\
&amp;= 79.459 \pm 1.981\cdot 7.311\\
&amp;= (-14.112, 14.854)
\end{aligned}\]</span></p>
<p>The results of the confidence intervals are valid only if the linear model assumptions are satisfied.
We discuss these assumptions in Section <a href="inference-for-regression.html#model-fit">10.2.6</a>.</p>
</div>
<div id="hypo-test-simple-lm" class="section level3" number="10.2.4">
<h3>
<span class="header-section-number">10.2.4</span> Hypothesis test for population slope<a class="anchor" aria-label="anchor" href="#hypo-test-simple-lm"><i class="fas fa-link"></i></a>
</h3>
<p>To perform a hypothesis test for <span class="math inline">\(\beta_1\)</span>, the general formulation of a two-sided test is</p>
<p><span class="math display">\[\begin{aligned}
H_0: \beta_1 = B\\
H_A: \beta_1 \ne B
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the hypothesized value for <span class="math inline">\(\beta_1\)</span>. Recall the terminology, notation, and definitions related to hypothesis tests we introduced in Section <a href="hypothesis-testing.html#understanding-ht">9.3</a>.
A <em>hypothesis test</em> consists of a test between two competing hypotheses: (1) a <em>null hypothesis</em> <span class="math inline">\(H_0\)</span> versus (2) an <em>alternative hypothesis</em> <span class="math inline">\(H_A\)</span>.</p>
<div id="t-test-stat-simple-lm" class="section level4 unnumbered">
<h4>Test statistic<a class="anchor" aria-label="anchor" href="#t-test-stat-simple-lm"><i class="fas fa-link"></i></a>
</h4>
<p>A <em>test statistic</em> is a point estimator used for hypothesis testing. Here, the <em>t-test statistic</em> is given by</p>
<p><span class="math display">\[t = \frac{b_1 - B}{SE(b_1)}.\]</span></p>
<p>This test statistic follows, under the null hypothesis, a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.
A particularly useful test is whether there is a linear association between the explanatory variable and the response, which is equivalent to testing:</p>
<p><span class="math display">\[\begin{aligned}
H_0: \beta_1 = 0\\
H_1: \beta_1 \ne 0
\end{aligned}\]</span></p>
<p>For example, we may use this test to determine whether there is a linear relationship between the duration of the Old Faithful geyser eruptions (<code>duration</code>) and the waiting time to the next eruption (<code>waiting</code>).
The <em>null hypothesis</em> <span class="math inline">\(H_0\)</span> assumes that the population slope <span class="math inline">\(\beta_1\)</span> is 0.
If this is true, then there is <em>no linear relationship</em> between the <code>duration</code> and <code>waiting</code> times.
When performing a hypothesis test, we assume that the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true and try to find evidence against it based on the data observed.</p>
<p>The <em>alternative hypothesis</em> <span class="math inline">\(H_A\)</span>, on the other hand, states that the population slope <span class="math inline">\(\beta_1\)</span> is not 0, meaning that longer eruption <code>duration</code> may result in greater or smaller <code>waiting</code> times to the next eruption.
This suggests either a positive or negative linear relationship between the explanatory variable and the response.
Since evidence against the null hypothesis may happen in either direction in this context, we call this a <em>two-sided</em> test.
The <em>t-test</em> statistic for this problem is given by:</p>
<p><span class="math display">\[t = \frac{b_1 - 0}{SE(b_1)} = \frac{0.371 - 0}{0.032} = 11.594\]</span></p>
</div>
<div id="the-p-value" class="section level4 unnumbered">
<h4>The p-value<a class="anchor" aria-label="anchor" href="#the-p-value"><i class="fas fa-link"></i></a>
</h4>
<p>Recall the terminology, notation, and definitions related to hypothesis tests we introduced in Section <a href="hypothesis-testing.html#understanding-ht">9.3</a>.
The definition of the <span class="math inline">\(p\)</span>-value is the probability of obtaining a test statistic just as extreme as or more extreme than the one observed, <em>assuming the null hypothesis</em> <span class="math inline">\(H_0\)</span> is true.
We can intuitively think of the <span class="math inline">\(p\)</span>-value as quantifying how “extreme” the estimated slope is (<span class="math inline">\(b_1\)</span> = 0.371), assuming there is no relationship between <code>duration</code> and <code>waiting</code> times.</p>
<p>For a two-sided test, if the test statistic is <span class="math inline">\(t = 2\)</span> for example, the <span class="math inline">\(p\)</span>-value is calculated as the area under the <span class="math inline">\(t\)</span>-curve to the left of <span class="math inline">\(-2\)</span> and to the right of <span class="math inline">\(2\)</span> is shown in Figure <a href="inference-for-regression.html#fig:pvalue1">10.4</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pvalue1"></span>
<img src="ModernDive_files/figure-html/pvalue1-1.png" alt="Illustration of a two-sided p-value for a t-test." width="\textwidth"><p class="caption">
FIGURE 10.4: Illustration of a two-sided p-value for a t-test.
</p>
</div>
<p>In our Old Faithful geyser eruptions example, the test statistic for the test <span class="math inline">\(H_0: \beta_1 = 0\)</span> was <span class="math inline">\(t = 11.594\)</span>.
The <span class="math inline">\(p\)</span>-value was so small that R simply shows that it is equal to zero.</p>
</div>
<div id="interpretation" class="section level4 unnumbered">
<h4>Interpretation<a class="anchor" aria-label="anchor" href="#interpretation"><i class="fas fa-link"></i></a>
</h4>
<p>Following the hypothesis testing procedure we outlined in Section <a href="hypothesis-testing.html#ht-interpretation">9.5</a>, since the <span class="math inline">\(p\)</span>-value was practically 0, for any choice of significance level <span class="math inline">\(\alpha\)</span>, we would reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_A\)</span>.
In other words, assuming that there is no linear association between <code>duration</code> and <code>waiting</code> times, the probability of observing a slope as extreme as the one we have attained using our random sample, was practically zero.
In conclusion, we reject the null hypothesis that there is no linear relationship between <code>duration</code> and <code>waiting</code> times.
We have enough statistical evidence to conclude that there is a linear relationship between these variables.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.4)</strong> In the context of a linear regression model, what does the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> represent?</p>
<ul>
<li>A. There is no linear association between the response and the explanatory variable.</li>
<li>B. The difference between the observed and predicted values is zero.</li>
<li>C. The linear association between response and explanatory variable crosses the origin.</li>
<li>D. The probability of committing a Type II Error is zero.</li>
</ul>
<!-- question above was repeated. I changed it --><p><strong>(LC10.5)</strong> Which of the following is an assumption of the linear regression model?</p>
<ul>
<li>A. The error terms <span class="math inline">\(\epsilon_i\)</span> are normally distributed with constant variance.</li>
<li>B. The error terms <span class="math inline">\(\epsilon_i\)</span> have a non-zero mean.</li>
<li>C. The error terms <span class="math inline">\(\epsilon_i\)</span> are dependent on each other.</li>
<li>D. The explanatory variable must be normally distributed.</li>
</ul>
<p><strong>(LC10.6)</strong> What does it mean when we say that the slope estimator <span class="math inline">\(b_1\)</span> is a random variable?</p>
<ul>
<li>A. <span class="math inline">\(b_1\)</span> will be the same for every sample taken from the population.</li>
<li>B. <span class="math inline">\(b_1\)</span> is a fixed value that does not change with different samples.</li>
<li>C. <span class="math inline">\(b_1\)</span> can vary from sample to sample due to sampling variation.</li>
<li>D. <span class="math inline">\(b_1\)</span> is always equal to the population slope <span class="math inline">\(\beta_1\)</span>.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="regression-table" class="section level3" number="10.2.5">
<h3>
<span class="header-section-number">10.2.5</span> The regression table in R<a class="anchor" aria-label="anchor" href="#regression-table"><i class="fas fa-link"></i></a>
</h3>
<p>The least-square estimates, standard errors, test statistics, <span class="math inline">\(p\)</span>-values, and confidence interval bounds discussed in Section <a href="inference-for-regression.html#theory-simple-regression">10.2</a> and Subsections <a href="inference-for-regression.html#framework-simple-lm">10.2.1</a>, <a href="inference-for-regression.html#se-simple-lm">10.2.2</a>, <a href="inference-for-regression.html#conf-intervals-b0-b1">10.2.3</a>, and <a href="inference-for-regression.html#hypo-test-simple-lm">10.2.4</a> can be calculated, all at once, using the R wrapper function <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a> from the <code>moderndive</code> package.
For <code>model_1</code>, the output is presented in Table <a href="inference-for-regression.html#tab:simple-model-part-deux">10.6</a>.</p>
<div class="sourceCode" id="cb489"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">model_1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:simple-model-part-deux">TABLE 10.6: </span>The regression table for this model
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
79.459
</td>
<td style="text-align:right;">
7.311
</td>
<td style="text-align:right;">
10.9
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
64.973
</td>
<td style="text-align:right;">
93.944
</td>
</tr>
<tr>
<td style="text-align:left;">
duration
</td>
<td style="text-align:right;">
0.371
</td>
<td style="text-align:right;">
0.032
</td>
<td style="text-align:right;">
11.4
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.307
</td>
<td style="text-align:right;">
0.435
</td>
</tr>
</tbody>
</table></div>
<p>Note that the first row in Table <a href="inference-for-regression.html#tab:simple-model-part-deux">10.6</a> addresses inferences for the intercept <span class="math inline">\(\beta_0\)</span>, and the second row deals with inference for the slope <span class="math inline">\(\beta_1\)</span>.
The headers of the table present the information found for inference:</p>
<ul>
<li>The <code>estimate</code> column contains the least-squares estimates, <span class="math inline">\(b_0\)</span> (first row) and <span class="math inline">\(b_1\)</span> (second row).</li>
<li>The <code>std_error</code> contains <span class="math inline">\(SE(b_0)\)</span> and <span class="math inline">\(SE(b_1)\)</span> (the standard errors for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>), respectively.
We defined these standard errors in Subsection <a href="inference-for-regression.html#se-simple-lm">10.2.2</a>.</li>
<li>The <code>statistic</code> column contains the <span class="math inline">\(t\)</span>-test statistic for <span class="math inline">\(b_0\)</span> (first row) and <span class="math inline">\(b_1\)</span> (second row).
If we focus on <span class="math inline">\(b_1\)</span>, the <span class="math inline">\(t\)</span>-test statistic was constructed using the equation</li>
</ul>
<p><span class="math display">\[
t = \frac{b_1 - 0}{SE(b_1)} = 11.594
\]</span></p>
<p>which corresponds to the hypotheses <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_A: \beta_1 \ne 0\)</span>.</p>
<ul>
<li>The <code>p_value</code> is the probability of obtaining a test statistic just as extreme as or more extreme than the one observed, assuming the null hypothesis is true.
For this hypothesis test, the <span class="math inline">\(t\)</span>-test statistic was equal to 11.594 and, therefore, the <span class="math inline">\(p\)</span>-value was near zero, suggesting rejection of the null hypothesis in favor of the alternative.</li>
<li>The values <code>lower_ci</code> and <code>upper_ci</code> are the lower and upper bounds of a 95% confidence interval for <span class="math inline">\(\beta_1\)</span>.</li>
</ul>
<p>Please refer to previous subsections for the conceptual framework and a more detailed description of these quantities.</p>
</div>
<div id="model-fit" class="section level3" number="10.2.6">
<h3>
<span class="header-section-number">10.2.6</span> Model fit and model assumptions<a class="anchor" aria-label="anchor" href="#model-fit"><i class="fas fa-link"></i></a>
</h3>
<p>We have introduced the linear model alongside assumptions about many of its elements and assumed all along that this is an appropriate representation of the relationship between the response and the explanatory variable.
In real-life applications, it is uncertain whether the relationship is appropriately described by the linear model or whether all the assumptions we have introduced are satisfied.</p>
<p>Of course, we do not expect the linear model described in this chapter, or any other model, to be a perfect representation of a phenomenon presented in nature.
Models are simplifications of reality in that they do not intend to represent exactly the relationship in question but rather provide useful approximations that help improve our understanding of this relationship.
Even more, we want models that are as simple as possible and still capture relevant features of the natural phenomenon we are studying.
This approach is known as the <em>principle of parsimony</em> or <em>Occam’s razor</em>.</p>
<p>But even with a simple model like a linear one, we still want to know if it accurately represents the relationship in the data.
This is called <em>model fit</em>.
In addition, we want to determine whether or not the model assumptions have been met.</p>
<p>There are four elements in the linear model we want to check.
An acrostic is a composition in which certain letters from each piece form a word or words.
To help you remember the four elements, we can use the following acrostic:</p>
<ol style="list-style-type: decimal">
<li>
<strong>L</strong>inearity of relationship between variables
<ul>
<li>Is the relationship between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> truly linear for each <span class="math inline">\(i = 1, \dots, n\)</span>?
In other words, is the linear model <span class="math inline">\(y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i\)</span> a good fit?</li>
</ul>
</li>
<li>
<strong>I</strong>ndependence of each of the response values <span class="math inline">\(y_i\)</span>
<ul>
<li>Are <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> independent for any <span class="math inline">\(i \ne j\)</span>?</li>
</ul>
</li>
<li>
<strong>N</strong>ormality of the error terms
<ul>
<li>Is the distribution of the error terms at least approximately normal?</li>
</ul>
</li>
<li>
<strong>E</strong>quality or constancy of the variance for <span class="math inline">\(y_i\)</span> (and for the error term <span class="math inline">\(\epsilon_i\)</span>)
<ul>
<li>Is the variance, or equivalently standard deviation, of the response <span class="math inline">\(y_i\)</span> always the same, regardless of the fitted value (<span class="math inline">\(\widehat{y}_i\)</span>) or the regressor value (<span class="math inline">\(x_i\)</span>)?</li>
</ul>
</li>
</ol>
<p>In this case, our acrostic follows the word <strong>LINE</strong>.
This can serve as a nice reminder of what to check when using linear regression.
To check for <strong>L</strong>inearity, <strong>N</strong>ormality, and <strong>E</strong>qual or constant variance, we use the residuals of the linear regression via <em>residual diagnostics</em> as we explain in the next subsection.
To check for <strong>I</strong>ndependence we can use the residuals if the data was collected using a time sequence or other type of sequences.
Otherwise, independence may be achieved by taking a random sample, which eliminates a sequential type of dependency.</p>
<p>We start by reviewing how residuals are calculated, introduce residual diagnostics via visualizations, use the example of the Old Faithful geyser eruptions to determine whether each of the four <strong>LINE</strong> elements are met, and discuss the implications.</p>
<div id="residuals" class="section level4 unnumbered">
<h4>Residuals<a class="anchor" aria-label="anchor" href="#residuals"><i class="fas fa-link"></i></a>
</h4>
<p>Recall that given a random sample of <span class="math inline">\(n\)</span> pairs <span class="math inline">\((x_1, y_1), \dots, (x_n,y_n)\)</span> the linear regression was given by:</p>
<p><span class="math display">\[\widehat{y}_i = b_0 + b_1 \cdot x_i\]</span></p>
<p>for all the observations <span class="math inline">\(i = 1, dots,n\)</span>.
Recall that the residual as defined in Subsection <a href="regression.html#model1points">5.1.3</a>, is the <em>observed response</em> minus the <em>fitted value</em>.
If we denote the residuals with the letter <span class="math inline">\(e\)</span> we get:</p>
<p><span class="math display">\[e_i = y_i - \widehat{y}_i\]</span></p>
<p>for <span class="math inline">\(i = 1, \dots, n\)</span>.
Combining these two formulas we get</p>
<p><span class="math display">\[y_i = \underline{\widehat{y}_i} + e_i = \underline{b_0 + b_1 \cdot x_i} + e_i\]</span></p>
<p>the resulting formula looks very similar to our linear model:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i\]</span></p>
<p>In this context, residuals can be thought of as rough estimates of the error terms.
Since many of the assumptions of the linear model are related to the error terms, we can check these assumptions by studying the residuals.</p>
<p>In Figure <a href="inference-for-regression.html#fig:residual-example">10.5</a>, we illustrate one particular residual for the Old Faithful geyser eruption where <code>duration</code> time is the explanatory variable and <code>waiting</code> time is the response.
We use an arrow to connect the observed waiting time (a circle) with the fitted waiting time (a square).
The vertical distance between these two points (or equivalently, the magnitude of the arrow) is the value of the residual for this observation.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:residual-example"></span>
<img src="ModernDive_files/figure-html/residual-example-1.png" alt="Example of observed value, fitted value, and residual." width="\textwidth"><p class="caption">
FIGURE 10.5: Example of observed value, fitted value, and residual.
</p>
</div>
<p>We can calculate all the <span class="math inline">\(n = 114\)</span> residuals by applying the <code><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points()</a></code> function to the regression model <code>model_1</code>.
Observe how the resulting values of <code>residual</code> are roughly equal to <code>waiting - waiting_hat</code> (there may be a slight difference due to rounding error).</p>
<div class="sourceCode" id="cb490"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">model_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span>, data <span class="op">=</span> <span class="va">old_faithful_2024</span><span class="op">)</span></span>
<span><span class="co"># Get regression points:</span></span>
<span><span class="va">fitted_and_residuals</span> <span class="op">&lt;-</span> <span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points</a></span><span class="op">(</span><span class="va">model_1</span><span class="op">)</span></span>
<span><span class="va">fitted_and_residuals</span></span></code></pre></div>
<pre><code># A tibble: 114 × 5
      ID waiting duration waiting_hat residual
   &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;
 1     1     180      235     166.666   13.334
 2     2     184      259     175.572    8.428
 3     3     116      137     130.299  -14.299
 4     4     188      222     161.842   26.158
 5     5     106      105     118.424  -12.424
 6     6     187      180     146.256   40.744
 7     7     182      244     170.006   11.994
 8     8     190      278     182.623    7.377
 9     9     138      249     171.861  -33.861
10    10     186      262     176.686    9.314
# ℹ 104 more rows</code></pre>
</div>
<div id="residual-diagnostics" class="section level4 unnumbered">
<h4>Residual diagnostics<a class="anchor" aria-label="anchor" href="#residual-diagnostics"><i class="fas fa-link"></i></a>
</h4>
<p><em>Residual diagnostics</em> are used to verify conditions <strong>L</strong>, <strong>N</strong>, and <strong>E</strong>.
While there are more sophisticated statistical approaches that can be used, we focus on data visualization. One of the most useful plots is a <em>residual plot</em>, which is a scatterplot of the residuals against the fitted values.
We use the <code>fitted_and_residuals</code> object to draw the scatterplot using <code><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point()</a></code> with the fitted values (<code>waiting_hat</code>) on the x-axis and the residuals (<code>residual</code>) on the y-axis.
In addition, we add titles to our axes with <code><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs()</a></code> and draw a horizontal line at 0 for reference using <code><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline()</a></code> and <code>yintercept = 0</code>, as shown in the following code:</p>
<div class="sourceCode" id="cb492"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">fitted_and_residuals</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">waiting_hat</span>, y <span class="op">=</span> <span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"duration"</span>, y <span class="op">=</span> <span class="st">"residual"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span></code></pre></div>
<p>In Figure <a href="inference-for-regression.html#fig:scatt-and-residual">10.6</a> we show this residual plot (right plot) alongside the scatterplot for <code>duration</code> vs <code>waiting</code> (left plot).
Note how the residuals on the left plot are determined by the vertical distance between the observed response and the linear regression.
On the right plot (residuals), we have removed the effect of the linear regression and the effect of the residuals is seen as the vertical distance from each point to the zero line (y-axis).
Using this residuals plot, it is easier to spot patterns or trends that may be in conflict with the assumptions of the model, as we describe next.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scatt-and-residual"></span>
<img src="ModernDive_files/figure-html/scatt-and-residual-1.png" alt="The scatterplot and residual plot for the Old Faithful data." width="\textwidth"><p class="caption">
FIGURE 10.6: The scatterplot and residual plot for the Old Faithful data.
</p>
</div>
<p>In what follows we show how the residual plot can be used to determine whether the linear model assumptions are met.</p>
<div id="linearity-of-relationship" class="section level5 unnumbered">
<h5>Linearity of relationship<a class="anchor" aria-label="anchor" href="#linearity-of-relationship"><i class="fas fa-link"></i></a>
</h5>
<p>We want to check whether the association between the response <span class="math inline">\(y_i\)</span> and the explanatory variable <span class="math inline">\(x_i\)</span> is <strong>L</strong>inear.
We expect, due to the error term in the model, that the scatterplot of residuals against fitted values shows some random variation, but the variation should not be systematic in any direction and the trend should not show non-linear patterns.</p>
<p>A scatterplot of residuals against fitted values showing no patterns but simply a cloud of points that seems randomly assigned in every direction with the residuals’ variation (y-axis) about the same for any fitted values (x-axis) and with points located as much above as below the zero line is called a <em>null</em> plot.
Plots of residuals against fitted values or regressors that are <em>null</em> plots do not show any evidence against the assumptions of the model.
In other words, if we want our linear model to be adequate, we hope to see <em>null</em> plots when plotting residuals against fitted values.</p>
<p>This is largely the case for the Old Faithful geyser example with the residuals against the fitted values (<code>waiting_hat</code>) shown in the right-plot of Figure <a href="inference-for-regression.html#fig:scatt-and-residual">10.6</a>.
The residual plot is not a <em>null</em> plot as it appears there are some clusters of points as opposed to a complete random assignment, but there are not clear systematic trends in any direction or the appearance of a non-linear relationship.
So, based on this plot, we believe that the data does not violate the assumption of linearity.</p>
<p>By contrast, assume now that the scatterplot of <code>waiting</code> against <code>duration</code> and its associated residual plot are shown in Figure <a href="inference-for-regression.html#fig:non-linear">10.7</a>.
We are not using the real data here, but simulated data.
A quick look at the scatterplot and regression line (left plot) could lead us to believe that the regression line is an appropriate summary of the relationship.
But if we look carefully, you may notice that the residuals for low values of <code>duration</code> are mostly below the regression line, residuals for values in the middle range of <code>duration</code> are mostly above the regression line, and residuals for large values of <code>duration</code> are again below the regression line.</p>
<p>This is the reason we prefer to use plots of residuals against fitted values (right plot) as we have removed the effect of the regression and can focus entirely on the residuals.
The points clearly do not form a line, but rather a U-shaped polynomial curve.
If this was the real data observed, using the linear regression with these data would produce results that are not valid or adequate.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:non-linear"></span>
<img src="ModernDive_files/figure-html/non-linear-1.png" alt="Example of a non-linear relationship." width="\textwidth"><p class="caption">
FIGURE 10.7: Example of a non-linear relationship.
</p>
</div>
</div>
<div id="independence-of-the-error-terms-and-the-response" class="section level5 unnumbered">
<h5>Independence of the error terms and the response<a class="anchor" aria-label="anchor" href="#independence-of-the-error-terms-and-the-response"><i class="fas fa-link"></i></a>
</h5>
<p>Another assumption we want to check is the <strong>I</strong>ndependence of the response values.
If they are not independent, some patterns of dependency may appear in the observed data.</p>
<p>The residuals could be used for this purpose too as they are a rough approximation of the error terms.
If data was collected in a time sequence or other type of sequence, the residuals may also help us determine lack of independence by plotting the residuals against time.
As it happens, the Old Faithful geyser eruption example does have a time component we can use: the <code>old_faithful_2024</code> dataset contains the <code>date</code> variable.
We show the plot of <code>residuals</code> against <code>date</code> (time) in Figure <a href="inference-for-regression.html#fig:time-plot">10.8</a>:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:time-plot"></span>
<img src="ModernDive_files/figure-html/time-plot-1.png" alt="Scatterplot of date (time) vs residuals for the Old Faithful example." width="\textwidth"><p class="caption">
FIGURE 10.8: Scatterplot of date (time) vs residuals for the Old Faithful example.
</p>
</div>
<p>The plot of residuals against time (<code>date</code>) seems to be a null plot.
Based on this plot we could say that the residuals do not exhibit any evidence of dependency.</p>
<p>Now, the observations in this dataset are only a subset of all the Old Faithful geyser eruptions that happen during this time frame and most or all of them are eruptions that do not happened sequentially, one after the next.
Each observation in this dataset represents a unique eruption of Old Faithful, with <code>waiting</code> times and <code>duration</code> recorded separately for each event.
Since these eruptions occur independently of one another, the residuals derived from the regression of <code>waiting</code> versus <code>duration</code> are also expected to be independent.
As discussed in Subsection <a href="inference-for-regression.html#sample-regression-inference">10.1.3</a>, we can consider this a random sample.</p>
<p>In this case, the assumption of independence seems acceptable.
Note that the <code>old_faithful_2024</code> data do not involve repeated measurements or grouped observations that could lead to dependency issues.
Therefore, we can proceed with trusting the regression analysis as we believe that the error terms are not systematically related to one another.
While determining lack of independence may not be easy in certain settings, in particular if no time sequence or other sequence measurements are involved, taking a random sample is the golden standard.</p>
</div>
<div id="normality-of-the-error-terms" class="section level5 unnumbered">
<h5>Normality of the error terms<a class="anchor" aria-label="anchor" href="#normality-of-the-error-terms"><i class="fas fa-link"></i></a>
</h5>
<p>The third assumption we want to check is whether the error terms follow <strong>N</strong>ormal distributions with expected value equal to zero.
Using the residuals as a rough estimate of the error term values, we have seen in the right plot of Figure <a href="inference-for-regression.html#fig:scatt-and-residual">10.6</a> that sometimes the residuals are positive and other times negative.
We want to see if, <em>on average</em>, the errors equal zero and the shape of their distribution approximate a bell shaped curve.</p>
<p>We can use a histogram to visualize the distribution of the residuals:</p>
<div class="sourceCode" id="cb493"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">fitted_and_residuals</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>binwidth <span class="op">=</span> <span class="fl">10</span>, color <span class="op">=</span> <span class="st">"white"</span><span class="op">)</span></span></code></pre></div>
<p>We can also use a <em>quantile-to-quantile</em> plot or <em>QQ-plot</em>.
The QQ-plot creates a scatterplot of the quantiles (or percentiles) of the residuals against the quantiles of a normal distribution.
If the residuals follow approximately a normal distribution, the scatterplot would be a straight line of 45 degrees.
To draw a QQ-plot for the Old Faithful geyser eruptions example, we use the <code>fitted_and_residuals</code> data frame that contains the residuals of the regression, <code><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot()</a></code> with <code>aes(sample = residual)</code> and the <code><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq()</a></code> function for drawing the QQ-plot.
We also include the function <code><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq_line()</a></code> to add a 45-degree line for comparison:</p>
<div class="sourceCode" id="cb494"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fitted_and_residuals</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>sample <span class="op">=</span> <span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq_line</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>In Figure <a href="inference-for-regression.html#fig:model1residualshist">10.9</a> we include both the histogram of the residuals including a normal curve for comparison (left plot) and a QQ-plot (right plot):</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:model1residualshist"></span>
<img src="ModernDive_files/figure-html/model1residualshist-1.png" alt="Histogram of residuals." width="\textwidth"><p class="caption">
FIGURE 10.9: Histogram of residuals.
</p>
</div>
<p>The histogram of the residuals shown in Figure <a href="inference-for-regression.html#fig:model1residualshist">10.9</a> (left plot) does not appear exactly normal as there are some deviations, such as the highest bin value appearing just to the right of the center.
But the histogram does not seem too far from normality either.
The QQ-plot (right plot) supports this conclusion.
The scatterplot is not exactly on the 45-degree line but it does not deviate much from it either.</p>
<p>We compare these results with residuals found by simulation that do not appear to follow normality as shown in Figure <a href="inference-for-regression.html#fig:not-normal-residuals">10.10</a>.
In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:not-normal-residuals"></span>
<img src="ModernDive_files/figure-html/not-normal-residuals-1.png" alt="Non-normal residuals." width="\textwidth"><p class="caption">
FIGURE 10.10: Non-normal residuals.
</p>
</div>
</div>
<div id="equality-or-constancy-of-variance-for-errors" class="section level5 unnumbered">
<h5>Equality or constancy of variance for errors<a class="anchor" aria-label="anchor" href="#equality-or-constancy-of-variance-for-errors"><i class="fas fa-link"></i></a>
</h5>
<p>The final assumption we check is the <strong>E</strong>quality or constancy of the variance for the error term across all fitted values or regressor values.
Constancy of variance is also known as <em>homoskedasticity</em>. Using the residuals again as rough estimates of the error terms, we want to check that the dispersion of the residuals is the same for any fitted value <span class="math inline">\(\widehat{y}_i\)</span> or regressor <span class="math inline">\(x_i\)</span>.
In Figure <a href="inference-for-regression.html#fig:scatt-and-residual">10.6</a>, we showed the scatterplot of residuals against fitted values (right plot).
We can also produce the scatterplot of residuals against the regressor <code>duration</code> in Figure <a href="inference-for-regression.html#fig:residual-plot">10.11</a>:</p>
<div class="sourceCode" id="cb495"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">fitted_and_residuals</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">duration</span>, y <span class="op">=</span> <span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"duration"</span>, y <span class="op">=</span> <span class="st">"residual"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:residual-plot"></span>
<img src="ModernDive_files/figure-html/residual-plot-1.png" alt="Plot of residuals against the regressor." width="\textwidth"><p class="caption">
FIGURE 10.11: Plot of residuals against the regressor.
</p>
</div>
<p>With the exception of the change of scale on the x-axis, it is equivalent (for visualization purposes) to producing a plot of residuals (<span class="math inline">\(e_i\)</span>) against either the fitted values (<span class="math inline">\(\widehat{y}_i\)</span>) or the regressor values (<span class="math inline">\(x_i\)</span>).
This happens because the fitted values are a linear transformation of the regressor values, <span class="math inline">\(\widehat{y}_i = b_0 + b_1\cdot x_i\)</span>.</p>
<p>Observe the vertical dispersion or spread of the residuals for different values of <code>duration</code>:</p>
<ul>
<li>For <code>duration</code> values between 100 and 150 seconds, the residual values are somewhere between -25 and 40, a spread of about 65 units.</li>
<li>For <code>duration</code> values between 150 and 200 seconds, there are only a handful of observations and it is not clear what the spread is.</li>
<li>For <code>duration</code> values between 200 and 250 seconds, the residual values are somewhere between -37 and 32, a spread of about 69 units.</li>
<li>For <code>duration</code> values between 250 and 300 seconds, the residual values are somewhere between -42 and 27, a spread of about 69 units.</li>
</ul>
<p>The spread is not exactly constant across all values of <code>duration</code>.
It seems to be slightly higher for greater values of <code>duration</code>, but there seems to be a larger number of observations for higher values of <code>duration</code> as well.
Observe also that there are two or three cluster of points and the dispersion of residuals is not completely uniform.
While the residual plot is not exactly a null plot, there is not clear evidence against the assumption of homoskedasticity.</p>
<p>We are not surprised to see plots such as this one when dealing with real data.
It is possible that the residual plot is not exactly a <em>null</em> plot, because there may be some information we are missing that could improve our model.
For example, we could include another regressor in our model.
Do not forget that we are using a linear model to approximate the relationship between <code>duration</code> and <code>waiting</code> times, and we do not expect the model to perfectly describe this relationship.
When you look at these plots, you are trying to find clear evidence of the data not meeting the assumptions used.
This example does not appear to violate the constant variance assumption.</p>
<p>In Figure <a href="inference-for-regression.html#fig:equal-variance-residuals">10.12</a> we present an example using simulated data with non-constant variance.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:equal-variance-residuals"></span>
<img src="ModernDive_files/figure-html/equal-variance-residuals-1.png" alt="Example of clearly non-equal variance." width="\textwidth"><p class="caption">
FIGURE 10.12: Example of clearly non-equal variance.
</p>
</div>
<p>Observe how the spread of the residuals increases as the regressor value increases.
Lack of constant variance is also known as <em>heteroskedasticity</em>.
When heteroskedasticity is present, some of the results such as the standard error of the least-square estimators, confidence intervals, or the conclusion for a related hypothesis test would not be valid.</p>
</div>
<div id="what-is-the-conclusion" class="section level5 unnumbered">
<h5>What is the conclusion?<a class="anchor" aria-label="anchor" href="#what-is-the-conclusion"><i class="fas fa-link"></i></a>
</h5>
<p>We did not find conclusive evidence against any of the assumptions of the model:</p>
<ol style="list-style-type: decimal">
<li>
<strong>L</strong>inearity of relationship between variables</li>
<li>
<strong>I</strong>ndependence of the error terms</li>
<li>
<strong>N</strong>ormality of the error terms</li>
<li>
<strong>E</strong>quality or constancy of the variance</li>
</ol>
<p>This does not mean that our model was perfectly adequate.
For example, the residual plot was not a <em>null</em> plot and had some clusters of points that cannot be explained by the model.
But overall, there were no trends that could be considered clear violations of the assumptions and the conclusions we get from this model may be valid.</p>
<p><strong>What do we do when the assumptions are not met?</strong></p>
<p>When there are clear violations of the assumptions in the model, all the results found may be suspect.
In addition, there may be some remedial measures that can be taken to improve the model.
None of these measures will be addressed here in depth as this material extends beyond the scope of this book, but we briefly discuss potential solutions for future reference.</p>
<p>When the <strong>L</strong>inearity of the relationship between variables is not met, a simple transformation of the regressor, the response, or both variables may solve the problem.
An example of such a transformation is given in <a href="https://moderndive.com/v2/appendixa">Appendix A online</a>.
If not, alternative methods such as <em>spline regression</em>, <em>generalized linear models</em>, or <em>non-linear models</em> may be used to address these situations.
When additional regressors are available, including other regressors as in <em>multiple linear regression</em> may produce better results.</p>
<p>If the <strong>I</strong>ndependence assumption is not met, but the dependency is established by a variable within the data at hand, <em>linear mixed-effects models</em> can also be used. These models may also be referred to as <em>hierarchical</em> or <em>multilevel models</em>.</p>
<p>Small departures of the <strong>N</strong>ormality of the error terms assumption are not too concerning and most of the results, including those related to confidence intervals and hypothesis tests, may still be valid.
On the other hand, when the number of violations to the normality assumption is large, many of the results may no longer be valid.
Using the advanced methods suggested earlier here may correct these problems too.</p>
<p>When the <strong>E</strong>quality or constancy of the variance is not met, adjusting the variance by adding weights to individual observations may be possible if relevant information is available that makes those weights known.
This method is called <em>weighted linear regression</em> or <em>weighted least squares</em>, and it is a direct extension of the model we have studied.
If information of the weights is not available, some methods can be used to provide an estimator for the internal structure of the variance in the model.
One of the most popular of these methods is called the <em>sandwich estimator</em>.</p>
<p>Checking that the assumptions of the model are satisfied is a key component of regression analysis.
Constructing and interpreting confidence intervals as well as conducting hypothesis tests and providing conclusions from the results of hypothesis tests are directly affected by whether or not assumptions are satisfied.
At the same time, it is often the case with regression analysis that a level of subjectivity when visualizing and interpreting plots is present, and sometimes we are faced with difficult statistical decisions.</p>
<p>So what can be done? We suggest transparency and clarity in communicating results.
It is important to highlight important elements that may suggest departures from relevant assumptions, and then provide pertinent conclusions.
In this way, the stakeholders of any analysis are aware of the model’s shortcomings and can decide whether or not to agree with the conclusions presented to them.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.7)</strong> Use the the <code>un_member_states_2024</code> data frame included in the <code>moderndive</code> package with response variable fertility rate (<code>fert_rate</code>) and the regressor life expectancy (<code>life_exp</code>).</p>
<ul>
<li>Use the <code><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points()</a></code> function to get the observed values, fitted values, and residuals for all UN member countries.</li>
<li>Perform a residual analysis and look for any systematic patterns in the residuals.
Ideally, there should be little to no pattern but comment on what you find here.</li>
</ul>
<p><strong>(LC10.8)</strong> In the context of linear regression, a <code>p_value</code> of near zero for the slope coefficient suggests which of the following?</p>
<ul>
<li>A. The intercept is statistically significant at a 95% confidence level.</li>
<li>B. There is strong evidence against the null hypothesis that the slope coefficient is zero, suggesting there exists a linear relationship between the explanatory and response variables.</li>
<li>C. The variance of the response variable is significantly greater than the variance of the explanatory variable.</li>
<li>D. The residuals are normally distributed with mean zero and constant variance.</li>
</ul>
<p><strong>(LC10.9)</strong> Explain whether or not the residual plot helps assess each one of the following assumptions.</p>
<ul>
<li>Linearity of the relationship between variables</li>
<li>Independence of the error terms</li>
<li>Normality of the error terms</li>
<li>Equality or constancy of variance</li>
</ul>
<p><strong>(LC10.10)</strong> If the residual plot against fitted values shows a “U-shaped” pattern, what does this suggest?</p>
<ul>
<li>A. The variance of the residuals is constant.</li>
<li>B. The linearity assumption is violated.</li>
<li>C. The independence assumption is violated.</li>
<li>D. The normality assumption is satisfied.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
</div>
</div>
<div id="infer-regression" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Simulation-based inference for simple linear regression<a class="anchor" aria-label="anchor" href="#infer-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we’ll use the simulation-based methods you previously learned in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> and <a href="hypothesis-testing.html#hypothesis-testing">9</a> to recreate the values in the regression table.
In particular, we’ll use the <code>infer</code> package workflow to</p>
<ul>
<li>Construct a 95% confidence interval for the population slope <span class="math inline">\(\beta_1\)</span> using bootstrap resampling with replacement. We did this previously in Sections <a href="confidence-intervals.html#bootstrap-process">8.2.2</a> with the almonds data and <a href="confidence-intervals.html#case-study-two-prop-ci">8.4</a> with the <code>mythbusters_yawn</code> data.</li>
<li>Conduct a hypothesis test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_A: \beta_1 \neq 0\)</span> using a permutation test. We did this previously in Sections <a href="hypothesis-testing.html#ht-infer">9.4</a> with the <code>spotify_sample</code> data and <a href="hypothesis-testing.html#ht-case-study">9.6</a> with the <code>movies_sample</code> IMDb data.</li>
</ul>
<div id="infer-ci-slr" class="section level3" number="10.3.1">
<h3>
<span class="header-section-number">10.3.1</span> Confidence intervals for the population slope using <code>infer</code><a class="anchor" aria-label="anchor" href="#infer-ci-slr"><i class="fas fa-link"></i></a>
</h3>
<p>We’ll construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the <code>infer</code> workflow outlined in Subsection <a href="confidence-intervals.html#infer-workflow">8.2.3</a>. Specifically, we’ll first construct the bootstrap distribution for the fitted slope <span class="math inline">\(b_1\)</span> using our single sample of 114 eruptions:</p>
<ol style="list-style-type: decimal">
<li>
<code><a href="https://infer.tidymodels.org/reference/specify.html">specify()</a></code> the variables of interest in <code>old_faithful_2024</code> with the formula: <code>waiting ~ duration</code>.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/generate.html">generate()</a></code> replicates by using <code>bootstrap</code> resampling with replacement from the original sample of 114 courses. We generate <code>reps = 1000</code> replicates using <code>type = "bootstrap"</code>.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/calculate.html">calculate()</a></code> the summary statistic of interest: the fitted <code>slope</code> <span class="math inline">\(b_1\)</span>.</li>
</ol>
<p>Using this bootstrap distribution, we’ll construct the 95% confidence interval using the percentile method and (if appropriate) the standard error method as well. It is important to note in this case that the bootstrapping with replacement is done <em>row-by-row</em>. Thus, the original pairs of <code>waiting</code> and <code>duration</code> values are always kept together, but different pairs of <code>waiting</code> and <code>duration</code> values may be resampled multiple times. The resulting confidence interval will denote a range of plausible values for the unknown population slope <span class="math inline">\(\beta_1\)</span> quantifying the relationship between waiting times and duration for Old Faithful eruptions.</p>
<p>Let’s first construct the bootstrap distribution for the fitted slope <span class="math inline">\(b_1\)</span>:</p>
<div class="sourceCode" id="cb496"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bootstrap_distn_slope</span> <span class="op">&lt;-</span> <span class="va">old_faithful_2024</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/generate.html">generate</a></span><span class="op">(</span>reps <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">"bootstrap"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/calculate.html">calculate</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"slope"</span><span class="op">)</span></span>
<span><span class="va">bootstrap_distn_slope</span></span></code></pre></div>
<pre><code>Response: waiting (numeric)
Explanatory: duration (numeric)
# A tibble: 1,000 × 2
   replicate     stat
       &lt;int&gt;    &lt;dbl&gt;
 1         1 0.334197
 2         2 0.331819
 3         3 0.385334
 4         4 0.380571
 5         5 0.369226
 6         6 0.370921
 7         7 0.337145
 8         8 0.417517
 9         9 0.343136
10        10 0.359239
# ℹ 990 more rows</code></pre>
<p>Observe how we have 1000 values of the bootstrapped slope <span class="math inline">\(b_1\)</span> in the <code>stat</code> column. Let’s visualize the 1000 bootstrapped values in Figure <a href="inference-for-regression.html#fig:bootstrap-distribution-slope">10.13</a>.</p>
<div class="sourceCode" id="cb498"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://infer.tidymodels.org/reference/visualize.html">visualize</a></span><span class="op">(</span><span class="va">bootstrap_distn_slope</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bootstrap-distribution-slope"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-slope-1.png" alt="Bootstrap distribution of slope." width="\textwidth"><p class="caption">
FIGURE 10.13: Bootstrap distribution of slope.
</p>
</div>
<p>Observe how the bootstrap distribution is roughly bell-shaped. Recall from Subsection <a href="confidence-intervals.html#revisit-almond-bootstrap">8.2.1</a> that the shape of the bootstrap distribution of <span class="math inline">\(b_1\)</span> closely approximates the shape of the sampling distribution of <span class="math inline">\(b_1\)</span>.</p>
<div id="percentile-method" class="section level4 unnumbered">
<h4>Percentile-method<a class="anchor" aria-label="anchor" href="#percentile-method"><i class="fas fa-link"></i></a>
</h4>
<p>First, let’s compute the 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the percentile method. We’ll do so by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped.</p>
<div class="sourceCode" id="cb499"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">percentile_ci</span> <span class="op">&lt;-</span> <span class="va">bootstrap_distn_slope</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/get_confidence_interval.html">get_confidence_interval</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"percentile"</span>, level <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span>
<span><span class="va">percentile_ci</span></span></code></pre></div>
<pre><code># A tibble: 1 × 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1 0.309088 0.425198</code></pre>
<p>The resulting percentile-based 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of (0.309, 0.425).</p>
</div>
<div id="standard-error-method" class="section level4 unnumbered">
<h4>Standard error method<a class="anchor" aria-label="anchor" href="#standard-error-method"><i class="fas fa-link"></i></a>
</h4>
<p>Since the bootstrap distribution in Figure <a href="inference-for-regression.html#fig:bootstrap-distribution-slope">10.13</a> appears to be roughly bell-shaped, we can also construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the standard error method.</p>
<p>In order to do this, we need to first compute the fitted slope <span class="math inline">\(b_1\)</span>, which will act as the center of our standard error-based confidence interval. While we saw in the regression table in Table <a href="inference-for-regression.html#tab:simple-model-part-deux">10.6</a> that this was <span class="math inline">\(b_1\)</span> = 0.371, we can also use the <code>infer</code> pipeline with the <code><a href="https://infer.tidymodels.org/reference/generate.html">generate()</a></code> step removed to calculate it:</p>
<div class="sourceCode" id="cb501"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">observed_slope</span> <span class="op">&lt;-</span> <span class="va">old_faithful_2024</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span><span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/calculate.html">calculate</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"slope"</span><span class="op">)</span></span>
<span><span class="va">observed_slope</span></span></code></pre></div>
<pre><code>Response: waiting (numeric)
Explanatory: duration (numeric)
# A tibble: 1 × 1
      stat
     &lt;dbl&gt;
1 0.371095</code></pre>
<p>We then use the <code><a href="https://infer.tidymodels.org/reference/get_confidence_interval.html">get_ci()</a></code> function with <code>level = 0.95</code> to compute the 95% confidence interval for <span class="math inline">\(\beta_1\)</span>. Note that setting the <code>point_estimate</code> argument to the <code>observed_slope</code> of 0.371 sets the center of the confidence interval.</p>
<div class="sourceCode" id="cb503"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">se_ci</span> <span class="op">&lt;-</span> <span class="va">bootstrap_distn_slope</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/get_confidence_interval.html">get_ci</a></span><span class="op">(</span>level <span class="op">=</span> <span class="fl">0.95</span>, type <span class="op">=</span> <span class="st">"se"</span>, point_estimate <span class="op">=</span> <span class="va">observed_slope</span><span class="op">)</span></span>
<span><span class="va">se_ci</span></span></code></pre></div>
<pre><code># A tibble: 1 × 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1 0.311278 0.430912</code></pre>
<p>The resulting standard error-based 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of <span class="math inline">\((0.311, 0.431)\)</span> is slightly different than the percentile-based confidence interval. Note that neither of these confidence intervals contains 0 and is entirely located above 0. This is suggesting that there is in fact a meaningful positive relationship between waiting times and duration for Old Faithful eruptions.</p>
</div>
</div>
<div id="infer-hypo-slr" class="section level3" number="10.3.2">
<h3>
<span class="header-section-number">10.3.2</span> Hypothesis test for population slope using <code>infer</code><a class="anchor" aria-label="anchor" href="#infer-hypo-slr"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s now conduct a hypothesis test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> vs. <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>. We will use the <code>infer</code> package, which follows the hypothesis testing paradigm in the “There is only one test” diagram in Figure <a href="hypothesis-testing.html#fig:htdowney">9.11</a>.</p>
<p>Let’s first think about what it means for <span class="math inline">\(\beta_1\)</span> to be zero as assumed in the null hypothesis <span class="math inline">\(H_0\)</span>. Recall we said if <span class="math inline">\(\beta_1 = 0\)</span>, then this is saying there is no relationship between the waiting time and duration. Thus, assuming this particular null hypothesis <span class="math inline">\(H_0\)</span> means that in our “hypothesized universe” there is no relationship between <code>waiting</code> and <code>duration</code>. We can therefore shuffle/permute the <code>waiting</code> variable to no consequence.</p>
<p>We construct the null distribution of the fitted slope <span class="math inline">\(b_1\)</span> by performing the steps that follow. Recall from Section <a href="hypothesis-testing.html#understanding-ht">9.3</a> on terminology, notation, and definitions related to hypothesis testing where we defined the <em>null distribution</em>: the sampling distribution of our test statistic <span class="math inline">\(b_1\)</span> assuming the null hypothesis <span class="math inline">\(H_0\)</span> is true.</p>
<ol style="list-style-type: decimal">
<li>
<code><a href="https://infer.tidymodels.org/reference/specify.html">specify()</a></code> the variables of interest in <code>old_faithful_2024</code> with the formula: <code>waiting ~ duration</code>.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/hypothesize.html">hypothesize()</a></code> the null hypothesis of <code>independence</code>. Recall from Section <a href="hypothesis-testing.html#ht-infer">9.4</a> that this is an additional step that needs to be added for hypothesis testing.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/generate.html">generate()</a></code> replicates by permuting/shuffling values from the original sample of 114 eruptions. We generate <code>reps = 1000</code> replicates using <code>type = "permute"</code> here.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/calculate.html">calculate()</a></code> the test statistic of interest: the fitted <code>slope</code> <span class="math inline">\(b_1\)</span>.</li>
</ol>
<p>In this case, we <code>permute</code> the values of <code>waiting</code> across the values of <code>duration</code> 1000 times. We can do this shuffling/permuting since we assumed a “hypothesized universe” of no relationship between these two variables. Then we <code>calculate</code> the <code>"slope"</code> coefficient for each of these 1000 <code>generate</code>d samples.</p>
<div class="sourceCode" id="cb505"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">null_distn_slope</span> <span class="op">&lt;-</span> <span class="va">old_faithful_2024</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span><span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/hypothesize.html">hypothesize</a></span><span class="op">(</span>null <span class="op">=</span> <span class="st">"independence"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/generate.html">generate</a></span><span class="op">(</span>reps <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">"permute"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/calculate.html">calculate</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"slope"</span><span class="op">)</span></span></code></pre></div>
<p>Observe the resulting null distribution for the fitted slope <span class="math inline">\(b_1\)</span> in Figure <a href="inference-for-regression.html#fig:null-distribution-slope">10.14</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:null-distribution-slope"></span>
<img src="ModernDive_files/figure-html/null-distribution-slope-1.png" alt="Null distribution of slopes." width="\textwidth"><p class="caption">
FIGURE 10.14: Null distribution of slopes.
</p>
</div>
<p>Notice how it is centered at <span class="math inline">\(b_1\)</span> = 0. This is because in our hypothesized universe, there is no relationship between <code>waiting</code> and <code>duration</code> and so <span class="math inline">\(\beta_1 = 0\)</span>. Thus, the most typical fitted slope <span class="math inline">\(b_1\)</span> we observe across our simulations is 0. Observe, furthermore, how there is variation around this central value of 0.</p>
<div class="sourceCode" id="cb506"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Observed slope</span></span>
<span><span class="va">b1</span> <span class="op">&lt;-</span> <span class="va">old_faithful_2024</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span><span class="va">waiting</span> <span class="op">~</span> <span class="va">duration</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/calculate.html">calculate</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"slope"</span><span class="op">)</span></span>
<span><span class="va">b1</span></span></code></pre></div>
<pre><code>Response: waiting (numeric)
Explanatory: duration (numeric)
# A tibble: 1 × 1
      stat
     &lt;dbl&gt;
1 0.371095</code></pre>
<p>Let’s visualize the <span class="math inline">\(p\)</span>-value in the null distribution by comparing it to the observed test statistic of <span class="math inline">\(b_1\)</span> = 0.371 in Figure <a href="inference-for-regression.html#fig:p-value-slope">10.15</a>. We’ll do this by adding a <code><a href="https://infer.tidymodels.org/reference/shade_p_value.html">shade_p_value()</a></code> layer to the previous <code><a href="https://infer.tidymodels.org/reference/visualize.html">visualize()</a></code> code.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:p-value-slope"></span>
<img src="ModernDive_files/figure-html/p-value-slope-1.png" alt="Null distribution and $p$-value." width="\textwidth"><p class="caption">
FIGURE 10.15: Null distribution and <span class="math inline">\(p\)</span>-value.
</p>
</div>
<p>Since the observed fitted slope 0.371 falls far to the right of this null distribution and thus the shaded region doesn’t overlap it, we’ll have a <span class="math inline">\(p\)</span>-value of 0. For completeness, however, let’s compute the numerical value of the <span class="math inline">\(p\)</span>-value anyways using the <code><a href="https://infer.tidymodels.org/reference/get_p_value.html">get_p_value()</a></code> function. Recall that it takes the same inputs as the <code><a href="https://infer.tidymodels.org/reference/shade_p_value.html">shade_p_value()</a></code> function:</p>
<div class="sourceCode" id="cb508"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">null_distn_slope</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/get_p_value.html">get_p_value</a></span><span class="op">(</span>obs_stat <span class="op">=</span> <span class="va">b1</span>, direction <span class="op">=</span> <span class="st">"both"</span><span class="op">)</span></span></code></pre></div>
<pre><code># A tibble: 1 × 1
  p_value
    &lt;dbl&gt;
1       0</code></pre>
<p>This matches the <span class="math inline">\(p\)</span>-value of 0 in the regression table. We therefore reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> in favor of the alternative hypothesis <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>. We thus have evidence that suggests there is a significant relationship between waiting time and duration values for eruptions of Old Faithful.</p>
<p>When the conditions for inference for regression are met and the null distribution has a bell shape, we are likely to see similar results between the simulation-based results we just demonstrated and the theory-based results shown in the regression table.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.11)</strong> Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of <code>stat = "correlation"</code> in the <code><a href="https://infer.tidymodels.org/reference/calculate.html">calculate()</a></code> function of the <code>infer</code> package.</p>
<p><strong>(LC10.12)</strong> Why is it appropriate to use the bootstrap percentile method to construct a 95% confidence interval for the population slope <span class="math inline">\(\beta_1\)</span> in the Old Faithful data?</p>
<ul>
<li>A. Because it assumes the slope follows a perfect normal distribution.</li>
<li>B. Because it relies on resampling the residuals instead of the original data points.</li>
<li>C. Because it requires the original data to be uniformly distributed.</li>
<li>D. Because it does not require the bootstrap distribution to be normally shaped.</li>
</ul>
<p><strong>(LC10.13)</strong> What is the role of the permutation test in the hypothesis testing for the population slope <span class="math inline">\(\beta_1\)</span>?</p>
<ul>
<li>A. It generates new samples to confirm the confidence interval boundaries.</li>
<li>B. It assesses whether the observed slope could have occurred by chance under the null hypothesis of no relationship.</li>
<li>C. It adjusts the sample size to reduce sampling variability.</li>
<li>D. It ensures the residuals of the regression model are normally distributed.</li>
</ul>
<p><strong>(LC10.14)</strong> After generating a null distribution for the slope using <code>infer</code>, you find the <span class="math inline">\(p\)</span>-value to be near 0. What does this indicate about the relationship between <code>waiting</code> and <code>duration</code> in the Old Faithful data?</p>
<ul>
<li>A. There is no evidence of a relationship between <code>waiting</code> and <code>duration</code>.</li>
<li>B. The observed slope is likely due to random variation under the null hypothesis.</li>
<li>C. The observed slope is significantly different from zero, suggesting a meaningful relationship between <code>waiting</code> and <code>duration</code>.</li>
<li>D. The null hypothesis cannot be rejected because the <span class="math inline">\(p\)</span>-value is too small.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="the-multiple-linear-regression-model" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> The multiple linear regression model<a class="anchor" aria-label="anchor" href="#the-multiple-linear-regression-model"><i class="fas fa-link"></i></a>
</h2>
<div id="multiple-linear-model" class="section level3" number="10.4.1">
<h3>
<span class="header-section-number">10.4.1</span> The model<a class="anchor" aria-label="anchor" href="#multiple-linear-model"><i class="fas fa-link"></i></a>
</h3>
<p>The extension from a simple to a multiple regression model is discussed next.
We assume that a population has a response variable (<span class="math inline">\(Y\)</span>) and two or more explanatory variables (<span class="math inline">\(X_1, X_2, \dots, X_p\)</span>) with <span class="math inline">\(p \ge 2\)</span>.
The <em>statistical linear relationship</em> between these variables is given by</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 \cdot X_1 + \dots + \beta_p X_p + \epsilon\]</span> where <span class="math inline">\(\beta_0\)</span> is the population intercept and <span class="math inline">\(\beta_j\)</span> is the population partial slope related to regressor <span class="math inline">\(X_j\)</span>.
The error term <span class="math inline">\(\epsilon\)</span> accounts for the portion of <span class="math inline">\(Y\)</span> that is not explained by the line.
As in the simple case, we assume that the expected value is <span class="math inline">\(E(\epsilon) = 0\)</span>, the standard deviation is <span class="math inline">\(SD(\epsilon) = \sigma\)</span>, and the variance is <span class="math inline">\(Var(\epsilon) = \sigma^2\)</span>.
The variance and standard deviation are constant regardless of the value of <span class="math inline">\(X_1, X_2, \dots, X_p\)</span>.
If you were to take a large number of observations from this population, we expect the error terms sometimes to be greater than zero and other times less than zero, but on average equal to zero, give or take <span class="math inline">\(\sigma\)</span> units away from zero.</p>
</div>
<div id="example-coffee-quality-rating-scores" class="section level3" number="10.4.2">
<h3>
<span class="header-section-number">10.4.2</span> Example: coffee quality rating scores<a class="anchor" aria-label="anchor" href="#example-coffee-quality-rating-scores"><i class="fas fa-link"></i></a>
</h3>
<p>As in the case of simple linear regression we use a random sample to estimate the parameters in the population.
To illustrate these methods, we use the <code>coffee_quality</code> data frame from the <code>moderndive</code> package.
This dataset from the Coffee Quality Institute contains information about coffee rating scores based on ten different attributes: <code>aroma</code>, <code>flavor</code>, <code>aftertaste</code>, <code>acidity</code>, <code>body</code>, <code>balance</code>, <code>uniformity</code>, <code>clean_cup</code>, <code>sweetness</code>, and <code>overall.</code>
In addition, the data frame contains other information such as the <code>moisture_percentage</code> and the coffee’s <code>country</code> and <code>continent_of_origin</code>.
We can assume that this is a random sample.</p>
<p>We plan to regress <code>total_cup_points</code> (response variable) on the numerical explanatory variables <code>aroma</code>, <code>flavor</code>, and <code>moisture_percentage</code> and the categorical explanatory variable with four categories; <code>Africa</code>, <code>Asia</code>, <code>North America</code>, and <code>South America</code>.
Before proceeding, we construct a new data frame called <code>coffee_data</code> by keeping the variables of interest.
In addition, the variable <code>continent_of_origin</code> has been read into R with type <code>character</code>, and we want to make it type <code>factor</code>.
We do this by using <code>dplyr</code> verbs and including the command <code><a href="https://rdrr.io/r/base/factor.html">as.factor()</a></code> inside <code><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate()</a></code> to make <code>continent_of_origin</code> a factor with the ordering of <code>"Africa"</code>, <code>"Asia"</code>, <code>"North America"</code>, and <code>"South America"</code>.</p>
<div class="sourceCode" id="cb510"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span> <span class="op">&lt;-</span> <span class="va">coffee_quality</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">aroma</span>, </span>
<span>         <span class="va">flavor</span>, </span>
<span>         <span class="va">moisture_percentage</span>, </span>
<span>         <span class="va">continent_of_origin</span>, </span>
<span>         <span class="va">total_cup_points</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>continent_of_origin <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">continent_of_origin</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The first ten rows of <code>coffee_data</code> are shown here:</p>
<div class="sourceCode" id="cb511"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span></span></code></pre></div>
<pre><code># A tibble: 207 × 5
   aroma flavor moisture_percentage continent_of_origin total_cup_points
   &lt;dbl&gt;  &lt;dbl&gt;               &lt;dbl&gt; &lt;fct&gt;                          &lt;dbl&gt;
 1  8.58   8.5                 11.8 South America                  89.33
 2  8.5    8.5                 10.5 Asia                           87.58
 3  8.33   8.42                10.4 Asia                           87.42
 4  8.08   8.17                11.8 North America                  87.17
 5  8.33   8.33                11.6 South America                  87.08
 6  8.33   8.33                10.7 North America                  87   
 7  8.33   8.17                 9.1 Asia                           86.92
 8  8.25   8.25                10   Asia                           86.75
 9  8.08   8.08                10.8 Asia                           86.67
10  8.08   8.17                11   Africa                         86.5 
# ℹ 197 more rows</code></pre>
<p>By looking at the fourth row we can tell, for example, that the <code>total_cup_points</code> are 87.17 with <code>aroma</code> score equal to 8.08 points, <code>flavor</code> score equal to 8.17 points, <code>moisture_percentage</code> equal to 11.8%, and North America is the <code>country_of_origin</code>.
We also display the summary for these variables in Table <a href="inference-for-regression.html#tab:coffee-tidy-summary">10.7</a>:</p>
<div class="sourceCode" id="cb513"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="moderndive.github.io/moderndive/reference/tidy_summary.html">tidy_summary</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:coffee-tidy-summary">TABLE 10.7: </span>Summary of coffee data
</caption>
<thead><tr>
<th style="text-align:left;">
column
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
Q1
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
Q3
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
sd
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
207
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
6.50
</td>
<td style="text-align:right;">
7.58
</td>
<td style="text-align:right;">
7.72
</td>
<td style="text-align:right;">
7.67
</td>
<td style="text-align:right;">
7.92
</td>
<td style="text-align:right;">
8.58
</td>
<td style="text-align:right;">
0.288
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
207
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
6.75
</td>
<td style="text-align:right;">
7.58
</td>
<td style="text-align:right;">
7.75
</td>
<td style="text-align:right;">
7.75
</td>
<td style="text-align:right;">
7.92
</td>
<td style="text-align:right;">
8.50
</td>
<td style="text-align:right;">
0.280
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
207
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
10.10
</td>
<td style="text-align:right;">
10.73
</td>
<td style="text-align:right;">
10.80
</td>
<td style="text-align:right;">
11.50
</td>
<td style="text-align:right;">
13.50
</td>
<td style="text-align:right;">
1.247
</td>
</tr>
<tr>
<td style="text-align:left;">
total_cup_points
</td>
<td style="text-align:right;">
207
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
78.00
</td>
<td style="text-align:right;">
82.58
</td>
<td style="text-align:right;">
83.71
</td>
<td style="text-align:right;">
83.75
</td>
<td style="text-align:right;">
84.83
</td>
<td style="text-align:right;">
89.33
</td>
<td style="text-align:right;">
1.730
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:left;">
Africa
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:left;">
Asia
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:left;">
North America
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:left;">
South America
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table></div>
<p>Observe that we have a sample of 207 observations, the <code>total_cup_points</code> ranges from 6.5 to 8.58, the average <code>aroma</code> score was 7.745, and the median <code>flavor</code> score was 10.8. Note that each observation is composed of <span class="math inline">\(p+1\)</span> values: the values for the explanatory variables <span class="math inline">\((X_1, \dots, X_p)\)</span> and the value for the response (<span class="math inline">\(Y\)</span>).
The sample takes the form:</p>
<p><span class="math display">\[\begin{array}{c}
(x_{11}, x_{12},\dots, x_{1p}, y_1)\\
(x_{21}, x_{22},\dots, x_{2p}, y_2)\\
\vdots\\
(x_{n1}, x_{n2},\dots, x_{np}, y_n)\\
\end{array}\]</span></p>
<p>where <span class="math inline">\((x_{i1}, x_{i2},\dots, x_{ip}, y_i)\)</span> are the values of the <span class="math inline">\(i\)</span>th observation in the sample for <span class="math inline">\(i=1\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(n\)</span>.
Using this notation, for example, <span class="math inline">\(x_{i2}\)</span> is the value of the <span class="math inline">\(i\)</span>th observation in the data for the second explanatory variable <span class="math inline">\(X_2\)</span>. In the coffee example <span class="math inline">\(n = 207\)</span> and the value of the second explanatory variable (<code>flavor</code>) for the 4th observation is <span class="math inline">\(x_{42} = 11.8\)</span>.</p>
<p>We can now create data visualizations.
When performing multiple regression it is useful to construct a scatterplot matrix, a matrix that contains the scatterplots for all the variable pair combinations.
In R, we can use the function <code><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs()</a></code> from package <code>GGally</code> to generate a scatterplot matrix and some useful additional information. In Figure <a href="inference-for-regression.html#fig:coffee-scatter-matrix">10.16</a>, we present the scatterplot matrix for all the variables of interest.</p>
<div class="sourceCode" id="cb514"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:coffee-scatter-matrix"></span>
<img src="ModernDive_files/figure-html/coffee-scatter-matrix-1.png" alt="Scatterplot matrix for coffee variables of interest." width="\textwidth"><p class="caption">
FIGURE 10.16: Scatterplot matrix for coffee variables of interest.
</p>
</div>
<p>We first comment on the plots with the response (<code>total_cup_points</code>) on the vertical axis.
They are located in the last row of plots in the figure.
When plotting <code>total_cup_points</code> against <code>aroma</code> (bottom row, leftmost plot) or <code>total_cup_points</code> against <code>flavor</code> (bottom row, second plot from the left), we observe a strong and positive linear relationship.
The plot of <code>total_cup_points</code> against <code>moisture_percentage</code> (bottom row, third plot from the left) does not provide much information and it appears that these variables are not associated in any way, but we observe an outlying observation for <code>moisture_percentage</code> around zero.
The plot of <code>total_cup_points</code> versus <code>continent_of_origin</code> (bottom row, fourth plot from the left) shows four histograms, one for each factor level; the fourth group seems to have a larger dispersion, even though the number of observations seems smaller.
The associated boxplots connecting these two variables (fourth row, rightmost plot) suggest that the first level of the factor (<code>"Africa"</code>) has a higher mean cup points than the other three.
It is often useful to also find any linear associations between numerical regressors as is the case for the scatterplot for <code>aroma</code> against <code>flavor</code> which suggests a strong positive linear association.
By contrast, almost no relationship can be found when observing <code>moisture_percentage</code> against either <code>aroma</code> or <code>flavor</code>.</p>
<p>The function <code><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs()</a></code> not only produces these plots, but includes the correlation coefficient for any pair of numerical variables.
In this example, the correlation coefficients support the findings from using the scatterplot matrix.
The correlations between <code>total_cup_points</code> and <code>aroma</code> (0.87) and <code>total_cup_points</code> and <code>flavor</code> (0.94) are positive and close to one, suggesting a strong positive linear association.
Recall that the correlation coefficient is relevant if the association is approximately linear.
The correlation between <code>moisture_percentage</code> and any other variable is close to zero, suggesting that <code>moisture_percentage</code> is likely not linearly associated with any other variable (either response or regressor).
Note also that the correlation between <code>aroma</code> and <code>flavor</code> (0.82) supports our conclusion of a strong positive association.</p>
</div>
<div id="least-squares-multiple" class="section level3" number="10.4.3">
<h3>
<span class="header-section-number">10.4.3</span> Least squares for multiple regression<a class="anchor" aria-label="anchor" href="#least-squares-multiple"><i class="fas fa-link"></i></a>
</h3>
<p>Observe that we have three numerical regressors and one factor (<code>continent_of_origin</code>) with four levels: <code>"Africa"</code>, <code>"Asia"</code>, <code>"North America"</code>, and <code>"South America"</code>.
To introduce the factor levels in the linear model, we represent the factor levels using dummy variables as described in Subsection <a href="multiple-regression.html#model4interactiontable">6.1.2</a>.
These are the dummy variables that we need:</p>
<p><span class="math display">\[\begin{aligned}
D_1 &amp;= \left\{
\begin{array}{ll}
1 &amp; \text{if the continent of origin is Africa} \phantom{afdasfd} \\
0 &amp; \text{otherwise}\end{array}
\right.\\
D_2 &amp;= \left\{
\begin{array}{ll}
1 &amp; \text{if the continent of origin is Asia}\phantom{asdfasdfa} \\
0 &amp; \text{otherwise}\end{array}
\right.\\
D_3 &amp;= \left\{
\begin{array}{ll}
1 &amp; \text{if the continent of origin is North America}\phantom{} \\
0 &amp; \text{otherwise}\end{array}
\right.\\
D_4 &amp;= \left\{
\begin{array}{ll}
1 &amp; \text{if the continent of origin is South America} \phantom{}\\
0 &amp; \text{otherwise}\end{array}
\right.\\
\end{aligned}\]</span></p>
<p>Recall also that we drop the first level as this level will be accounted by the intercept in the model.
As we did in the simple linear case, we assume that linearity between the response and the regressors holds and apply the linear model described in Subsection <a href="inference-for-regression.html#multiple-linear-model">10.4.1</a> to each observation in the sample.
If we express the model in terms of the <span class="math inline">\(i\)</span>th observation, we get</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_{02}D_{i2} + \beta_{03}D_{i3} + \beta_{04}D_{i4} + \epsilon_i
\]</span></p>
<p>where, for the <span class="math inline">\(i\)</span>th observation in the sample, <span class="math inline">\(x_{i1}\)</span> represents the <code>aroma</code> score, <span class="math inline">\(x_{i2}\)</span> the <code>flavor</code> score, <span class="math inline">\(x_{i3}\)</span> the <code>moisture_percentage</code>, <span class="math inline">\(D_{i2}\)</span> the dummy variable for <code>Asia</code>, <span class="math inline">\(D_{i3}\)</span> the dummy variable for <code>North America</code>, and <span class="math inline">\(D_{i4}\)</span> the dummy variable for <code>South America</code>.
Recall that <span class="math inline">\(i\)</span> is the subscript that represents any one observation in the sample.
Alternatively, we could present the model for all the observations:</p>
<p><span class="math display">\[\begin{aligned}
y_1
&amp;=
\beta_0 + \beta_1 x_{11} + \beta_2 x_{12} + \beta_3 x_{13}+ \beta_{02}D_{12} + \beta_{03}D_{13} + \beta_{04}D_{14} + \epsilon_1\\
y_2
&amp;=
\beta_0 + \beta_1 x_{21} + \beta_2 x_{22} + \beta_3 x_{23}+ \beta_{02}D_{22} + \beta_{03}D_{23} + \beta_{04}D_{24} + \epsilon_2\\
&amp; \phantom{  a}\vdots \\
y_n
&amp;=
\beta_0 + \beta_1 x_{n1} + \beta_2 x_{n2} + \beta_3 x_{n3}+ \beta_{02}D_{n2} + \beta_{03}D_{n3} + \beta_{04}D_{n4} + \epsilon_n
\end{aligned}\]</span></p>
<p>The extension of the least-squares method applied to multiple regression follows.
We want to retrieve the coefficient estimators that minimize the <em>sum of squared residuals</em>:</p>
<p><span class="math display">\[\sum_{i=1}^n \left[y_i - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}+ \beta_{02}D_{i2} + \beta_{03}D_{i3} + \beta_{04}D_{i4} )\right]^2.\]</span></p>
<p>This optimization problem is similar to the simple linear case, and it is solved using calculus.
We have more equations to deal with now; seven equations in our coffee example, each connected with the coefficient estimators that need to be estimated.
The solutions to this problem are reached by using matrices and matrix calculus, and are the regression coefficients introduced in Chapter <a href="multiple-regression.html#multiple-regression">6</a> and Subsection <a href="multiple-regression.html#model4interactiontable">6.1.2</a>.
They are called the <em>least-squares estimators</em>: <span class="math inline">\(b_0\)</span> is the least-square estimator of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(b_1\)</span> is the least-square estimator of <span class="math inline">\(\beta_1\)</span>, etc.</p>
<p>The fitted values, residuals, estimator of the variance (<span class="math inline">\(s^2\)</span>), and standard deviation (<span class="math inline">\(s\)</span>), are direct extensions to the simple linear case. In the general case, with <span class="math inline">\(p\)</span> regressors, the fitted values are</p>
<p><span class="math display">\[\widehat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip},\]</span></p>
<p>the residuals are <span class="math inline">\(e_i = y_i - \widehat{y}_i\)</span>, and the model variance estimator is</p>
<p><span class="math display">\[
s^2 = \frac{\sum_{i=1}^n \left(y_i - \widehat{y}_i\right)^2}{n-p}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of coefficients. When applying these formulas to the coffee scores example, the fitted values are</p>
<p><span class="math display">\[\widehat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3}+ b_{02}D_{i2} + b_{03}D_{i3} + b_{04}D_{i4}\,\,,\]</span></p>
<p>the variance estimator is</p>
<p><span class="math display">\[\begin{aligned}
s^2 &amp;= \frac{\sum_{i=1}^n \left[y_i - (b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3}+ b_{02}D_{i2} + b_{03}D_{i3} + b_{04}D_{i4} )\right]^2}{n-7}\\
&amp;= \frac{\sum_{i=1}^n \left(y_i - \widehat{y}_i\right)^2}{n-7},
\end{aligned}\]</span></p>
<p>and the standard deviation estimator is</p>
<p><span class="math display">\[
s = \sqrt{\frac{\sum_{i=1}^n \left(y_i - \widehat{y}_i\right)^2}{n-7}}.
\]</span></p>
<p>When we think about the least-square estimators as random variables that depend on the random sample taken, the properties of these estimators are a direct extension of the properties presented for the simple linear case:</p>
<ul>
<li>The least-square estimators are unbiased estimators of the parameters of the model.
For example, if we choose <span class="math inline">\(\beta_1\)</span> (the estimator for the partial slope for <code>aroma</code>), then the expected value is equal to the parameter, <span class="math inline">\(E(b_1) = \beta_1\)</span>.
This means that for some random samples the estimated value <span class="math inline">\(b_1\)</span> will be greater than <span class="math inline">\(\beta_1\)</span>, and for others less than <span class="math inline">\(\beta_1\)</span>; but, on average, <span class="math inline">\(b_1\)</span> will be equal to <span class="math inline">\(\beta_1\)</span>.</li>
<li>The least-square estimators are linear combinations of the observed responses <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(y_n\)</span>.
For <span class="math inline">\(b_3\)</span>, for example, there are known constants <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(c_n\)</span> such that <span class="math inline">\(b_1 = \sum_{i=1}^n c_iy_i\)</span>.</li>
</ul>
<p>When using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function, all the necessary calculations are done in R. In Table <a href="inference-for-regression.html#tab:regtable-coffee">10.8</a>, for the coffee example, we get:</p>
<div class="sourceCode" id="cb515"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">mod_mult</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span> <span class="op">+</span> <span class="va">continent_of_origin</span>, </span>
<span>  data <span class="op">=</span> <span class="va">coffee_data</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the coefficients of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_mult</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the standard deviation of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">mod_mult</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-coffee">TABLE 10.8: </span>Coffee example linear regression coefficients
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b0
</td>
<td style="text-align:right;">
37.321
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:left;">
b1
</td>
<td style="text-align:right;">
1.732
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:left;">
b2
</td>
<td style="text-align:right;">
4.316
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:left;">
b3
</td>
<td style="text-align:right;">
-0.008
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_originAsia
</td>
<td style="text-align:left;">
b02
</td>
<td style="text-align:right;">
-0.393
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_originNorth America
</td>
<td style="text-align:left;">
b03
</td>
<td style="text-align:right;">
-0.273
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_originSouth America
</td>
<td style="text-align:left;">
b04
</td>
<td style="text-align:right;">
-0.478
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
s
</td>
<td style="text-align:right;">
0.507
</td>
</tr>
</tbody>
</table></div>
<p>If the linear model is appropriate, we can interpret these coefficients as we did in Subsections <a href="multiple-regression.html#model4interactiontable">6.1.2</a> and <a href="multiple-regression.html#model3table">6.2.2</a>.
Recall that the numerical regressors’ coefficients (<span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span>, and <span class="math inline">\(b_3\)</span>) are partial slopes, each representing the extra effect (or additional effect) of increasing the corresponding regressor by one unit while keeping all the other regressors fixed to some value.
For example, using <code>mod_mult</code>, if for a given observation we increase the <code>flavor</code> score by one unit, keeping all the other regressors fixed to some level, the <code>total_cup_points</code> would increase by 4.32 units, on average.
This interpretation is only valid for the linear regression model <code>mod_mult</code>.
If we decide to change the regressors used, add some or remove others, the model changes and the partial slope for <code>flavor</code> will be different in magnitude and in meaning; do not forget that the partial slope is the additional contribution of <code>flavor</code> when added to a model that includes all the other regressors for that particular model.</p>
<p>In addition, observe that <code>mod_mult</code> is a model without interactions, similar to the model described in Subsection <a href="multiple-regression.html#model4table">6.1.3</a>. The coefficients for factor levels of <code>continent_of_origin</code> (<span class="math inline">\(b_{02}\)</span>, <span class="math inline">\(b_{03}\)</span>, and <span class="math inline">\(b_{04}\)</span>) affect only the intercept of the model, based on the category of the observation in question.
Recall that the factor levels, in order, are <code>Africa</code>, <code>Asia</code>, <code>North America</code>, and <code>South America</code>.
For example, if the fifth observation’s continent of origin is South America (<span class="math inline">\(D_{04} = 1\)</span> and <span class="math inline">\(D_{02} = D_{03} = 0\)</span>), the regression formula is given by</p>
<p><span class="math display">\[\begin{aligned}
\widehat{y}_5 &amp;= b_0 + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}+ b_{02}D_{52} + b_{03}D_{53} + b_{04}D_{54}\\
&amp;= b_0 + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}+ b_{02}\cdot 0 + b_{03}\cdot 0 + b_{04}\cdot 1\\
&amp;= (b_0 + b_{04}) + b_1 x_{51} + b_2 x_{52} + b_3 x_{53}\\
\end{aligned}\]</span></p>
<p>and the regression intercept for this observation is estimated to be</p>
<p><span class="math display">\[b_0 + b_{04} = 37.32 + (-0.48) = 36.84.\]</span></p>
<p>We typically do not expect the scores of all regressors to be zero, but you can always check the range of values that your regressors take.
For <code>mod_mult</code> the range of regressors can be extracted by using <code><a href="moderndive.github.io/moderndive/reference/tidy_summary.html">tidy_summary()</a></code>.
In the following code, using the <code>coffee_data</code> dataset, we select the numerical regressors, use <code><a href="moderndive.github.io/moderndive/reference/tidy_summary.html">tidy_summary()</a></code>, and select <code>column</code> name, <code>min</code>, and <code>max</code> to get the range for all regressors</p>
<div class="sourceCode" id="cb516"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">aroma</span>, <span class="va">flavor</span>, <span class="va">moisture_percentage</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="moderndive.github.io/moderndive/reference/tidy_summary.html">tidy_summary</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">column</span>, <span class="va">min</span>, <span class="va">max</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
column
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
max
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
6.50
</td>
<td style="text-align:right;">
8.58
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
6.75
</td>
<td style="text-align:right;">
8.50
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
13.50
</td>
</tr>
</tbody>
</table></div>
<p>As we see, only <code>moisture_percentage</code> includes zero in its range, and we would need all numerical regressors to include zero in order for the intercept to have a special meaning in the context of the problem.</p>
<p>Observe that we have decided to use only a subset of regressors and construct a model without interactions.
We discuss in Subsection <a href="inference-for-regression.html#model-fit-mult">10.5.5</a> how we could determine what is the best subset of regressors to use when many are available.
We are now ready to discuss inference for multiple linear regression.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.15)</strong> In a multiple linear regression model, what does the coefficient <span class="math inline">\(\beta_j\)</span> represent?</p>
<ul>
<li>A. The intercept of the model.</li>
<li>B. The standard error of the estimate.</li>
<li>C. The total variance explained by the model.</li>
<li>D. The partial slope related to the regressor <span class="math inline">\(X_j\)</span>, accounting for all other regressors.</li>
</ul>
<p><strong>(LC10.16)</strong> Why is it necessary to convert <code>continent_of_origin</code> to a factor when preparing the <code>coffee_data</code> data frame for regression analysis?</p>
<ul>
<li>A. To allow the regression model to interpret <code>continent_of_origin</code> as a numerical variable.</li>
<li>B. To create dummy variables that represent different categories of <code>continent_of_origin</code>.</li>
<li>C. To reduce the number of observations in the dataset.</li>
<li>D. To ensure the variable is included in the correlation matrix.</li>
</ul>
<p><strong>(LC10.17)</strong> What is the purpose of creating a scatterplot matrix in the context of multiple linear regression?</p>
<ul>
<li>A. To identify outliers that need to be removed from the dataset.</li>
<li>B. To test for normality of the residuals.</li>
<li>C. To examine linear relationships between all variable pairs and identify multicollinearity among regressors.</li>
<li>D. To determine the appropriate number of dummy variables.</li>
</ul>
<p><strong>(LC10.18)</strong> In the multiple regression model for the <code>coffee_data</code>, what is the role of dummy variables for <code>continent_of_origin</code>?</p>
<ul>
<li>A. They are used to predict the values of the numerical regressors.</li>
<li>B. They modify the intercept based on the specific category of <code>continent_of_origin</code>.</li>
<li>C. They serve to test the independence of residuals.</li>
<li>D. They indicate which observations should be excluded from the model.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="theory-multiple-regression" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Theory-based inference for multiple linear regression<a class="anchor" aria-label="anchor" href="#theory-multiple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we introduce some of the conceptual framework needed to understand inference in multiple linear regression.
We illustrate this framework using the coffee example and the R function <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a> introduced in Subsection <a href="inference-for-regression.html#regression-table">10.2.5</a>.</p>
<p>Inference for multiple linear regression is a natural extension of inference for simple linear regression. Recall that the linear model, for the <span class="math inline">\(i\)</span>th observation is given by</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 \cdot x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i\,.\]</span>
We assume again that the error term is normally distributed with an expected value (mean) equal to zero and a standard deviation equal to <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[\epsilon_i \sim Normal(0, \sigma).\]</span>
Since the error term is the only random element in the linear model, the response <span class="math inline">\(y_i\)</span> results from the sum of a constant</p>
<p><span class="math display">\[\beta_0 + \beta_1 \cdot x_{i1} + \dots + \beta_p x_{ip}\]</span></p>
<p>and a random variable: the error term <span class="math inline">\(\epsilon_i\)</span>.
Using properties of the normal distribution, the expected value, and the variance (and standard deviation), it can be shown that <span class="math inline">\(y_i\)</span> is also normally distributed with mean equal to <span class="math inline">\(\beta_0 + \beta_1 \cdot x_{i1} + \dots + \beta_p x_{ip}\)</span> and standard deviation equal to <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[y_i \sim Normal(\beta_0 + \beta_1 \cdot x_{i1} + \dots + \beta_p x_{ip}\,,\, \sigma)\]</span></p>
<p>for <span class="math inline">\(i=1,\dots,n\)</span>.
We also assume that <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> are independent, so <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> are also independent for any <span class="math inline">\(i \ne j\)</span>.
Moreover, the least-squares estimators (<span class="math inline">\(b_0, b_1, \dots, b_p\)</span>) are linear combinations of the random variables <span class="math inline">\(y_1, \dots, y_n\)</span> which are normally distributed, as shown above.
Again, following properties of the normal distribution, the expected value, and the variance it can be shown that</p>
<ul>
<li>the (least-square) estimators follow a normal distribution,</li>
<li>the estimators are unbiased, meaning that the expected value for each estimator is the parameter they are estimating; for example, <span class="math inline">\(E(b_1) = \beta_1\)</span>, or in general <span class="math inline">\(E(b_j) = \beta_j\)</span> for <span class="math inline">\(j = 0,1,\dots, p\)</span>.</li>
<li>the variance and standard deviation of each estimator (<span class="math inline">\(b_j\)</span>), is a function of <span class="math inline">\(\sigma\)</span>, and the observed data for the explanatory variable (the values of the regressors in the sample).
For simplicity, the standard deviation of the estimator <span class="math inline">\(b_j\)</span> is denoted by <span class="math inline">\(SD(b_j)\)</span> but remember that it is a function of the standard deviation of the response (<span class="math inline">\(\sigma\)</span>).</li>
<li>Using the information above, the distribution of the (least-squares) estimator <span class="math inline">\(b_j\)</span> is given by</li>
</ul>
<p><span class="math display">\[b_j \sim Normal(\beta_j, SD(b_j))\]</span></p>
<p>for <span class="math inline">\(j = 1, \dots, p\)</span>. Note also that <span class="math inline">\(\sigma\)</span> is typically unknown, and it is estimated using the estimated standard deviation <span class="math inline">\(s\)</span> instead of <span class="math inline">\(\sigma\)</span>. The estimated standard deviation for <span class="math inline">\(b_j\)</span> is called the standard error of <span class="math inline">\(b_j\)</span> and written as <span class="math inline">\(SE(b_j)\)</span>. Again, remember that the standard error is a function of <span class="math inline">\(s\)</span>.</p>
<p>The standard errors are shown when applying the <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a> function on a regression model. The output for the model <code>mod_mult</code> in in Table <a href="inference-for-regression.html#tab:mult-model-1">10.9</a>:</p>

<div class="sourceCode" id="cb517"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">mod_mult</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mult-model-1">TABLE 10.9: </span>The regression table for <code>mod_mult</code>
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
37.321
</td>
<td style="text-align:right;">
1.116
</td>
<td style="text-align:right;">
33.45
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
35.121
</td>
<td style="text-align:right;">
39.522
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
1.732
</td>
<td style="text-align:right;">
0.222
</td>
<td style="text-align:right;">
7.80
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.294
</td>
<td style="text-align:right;">
2.170
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
4.316
</td>
<td style="text-align:right;">
0.226
</td>
<td style="text-align:right;">
19.09
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
3.870
</td>
<td style="text-align:right;">
4.762
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
-0.008
</td>
<td style="text-align:right;">
0.030
</td>
<td style="text-align:right;">
-0.27
</td>
<td style="text-align:right;">
0.787
</td>
<td style="text-align:right;">
-0.067
</td>
<td style="text-align:right;">
0.051
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: Asia
</td>
<td style="text-align:right;">
-0.393
</td>
<td style="text-align:right;">
0.121
</td>
<td style="text-align:right;">
-3.24
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.632
</td>
<td style="text-align:right;">
-0.154
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: North America
</td>
<td style="text-align:right;">
-0.273
</td>
<td style="text-align:right;">
0.127
</td>
<td style="text-align:right;">
-2.15
</td>
<td style="text-align:right;">
0.033
</td>
<td style="text-align:right;">
-0.524
</td>
<td style="text-align:right;">
-0.023
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: South America
</td>
<td style="text-align:right;">
-0.478
</td>
<td style="text-align:right;">
0.142
</td>
<td style="text-align:right;">
-3.38
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.757
</td>
<td style="text-align:right;">
-0.199
</td>
</tr>
</tbody>
</table></div>
<p>We can see that the standard error for the numerical regressors are <span class="math inline">\(SE(b_1) = 0.22\)</span>, <span class="math inline">\(SE(b_2) = 0.23\)</span>, and <span class="math inline">\(SE(b_3) = 0.03\)</span>.</p>
<div id="model-dependency" class="section level3" number="10.5.1">
<h3>
<span class="header-section-number">10.5.1</span> Model dependency of estimators<a class="anchor" aria-label="anchor" href="#model-dependency"><i class="fas fa-link"></i></a>
</h3>
<p>Many inference methods and results for multiple linear regression are a direct extension of the methods and results discussed in simple linear regression.
The most important difference is the fact that the least-square estimators represent partial slopes and their values are dependent on the other regressors in the model.
If we change the set of regressors used in a model, the least-squares estimates and its standard errors will likely change as well.
And these changes will lead to different confidence interval limits, different test statistics for hypothesis tests, and potentially different conclusions about those regressors.</p>
<p>Using the <code>coffee_data</code> example, suppose that we consider the model:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \beta_3 x_{i3} + \epsilon_i\,\]</span>
where <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span>, are the parameters for the partial slopes of <code>aroma</code>, <code>flavor</code>, and <code>moisture_precentage</code>, respectively.
Recall that we assume that the parameters <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span> are constants, unknown to us, but constants.
We use multiple linear regression and find the least-square estimates <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span>, and <span class="math inline">\(b_3\)</span>, respectively. The results using the <code>coffee_data</code> are calculated and stored in the object <code>mod_mult_1</code> and shown in Table <a href="inference-for-regression.html#tab:regtable-coffee-mult-1">10.10</a>:</p>
<div class="sourceCode" id="cb518"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">mod_mult_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span>, </span>
<span>  data <span class="op">=</span> <span class="va">coffee_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the coefficients of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_mult_1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">mod_mult_1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-coffee-mult-1">TABLE 10.10: </span>Coffee example linear regression coefficients with three regressors
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b0
</td>
<td style="text-align:right;">
36.778
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:left;">
b1
</td>
<td style="text-align:right;">
1.800
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:left;">
b2
</td>
<td style="text-align:right;">
4.285
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:left;">
b3
</td>
<td style="text-align:right;">
-0.015
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
s
</td>
<td style="text-align:right;">
0.521
</td>
</tr>
</tbody>
</table></div>
<p>Now, assume that we decide instead to construct a model without the regressor <code>flavor</code>, so our model is</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_3 \cdot x_{i3} + \epsilon_i\,.\]</span>
Using multiple regression we compute the least-square estimates <span class="math inline">\(b'_1\)</span> and <span class="math inline">\(b'_3\)</span>, respectively, with the <span class="math inline">\('\)</span> denoting potentially different coefficient values in this different model.
The results using the <code>coffee_data</code> are calculated and stored in the object <code>mod_mult_2</code> and shown in Table <a href="inference-for-regression.html#tab:regtable-coffee-mult-2">10.11</a>:</p>
<div class="sourceCode" id="cb519"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">mod_mult_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">moisture_percentage</span>, data <span class="op">=</span> <span class="va">coffee_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the coefficients of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_mult_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span><span class="op">(</span><span class="va">mod_mult_2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-coffee-mult-2">TABLE 10.11: </span>Coffee example linear regression coefficients with two regressors
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Coefficients
</th>
<th style="text-align:right;">
Values
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
b’0
</td>
<td style="text-align:right;">
44.010
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:left;">
b’1
</td>
<td style="text-align:right;">
5.227
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:left;">
b’3
</td>
<td style="text-align:right;">
-0.062
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
s’
</td>
<td style="text-align:right;">
0.857
</td>
</tr>
</tbody>
</table></div>
<p>We focus on the partial slope for <code>aroma</code> using both models.
Observe that in model <code>mod_mult_1</code>, the partial slope for <code>aroma</code> is <span class="math inline">\(b_1 = 1.8\)</span>.
In model <code>mod_mult_2</code>, the partial slope for <code>aroma</code> is <span class="math inline">\(b'_1 = 5.23\)</span>.
The results are truly different because the models used in each case are different.
Similarly, every other coefficient found is different, the standard deviation estimate is different, and it is possible that the inferential results for confidence intervals or hypothesis tests are different too!</p>
<p>Any results, conclusions, and interpretations of a regressor are only valid for the model used.
For example, interpretations or conclusions made about <code>aroma</code> and its effects or influence in <code>total_cup_points</code> are entirely dependent on whether we have used <code>mod_mult_1</code>, <code>mod_mult_2</code>, or another model.
Never assume that a conclusion from using one model can translate to a different model.</p>
<p>In addition, it is important to determine which model is the most adequate.
Clearly, not both <code>mod_mult_1</code> and <code>mod_mult_2</code> can be correct, and we would like to use the one that is the most appropriate.
What if neither <code>mod_mult_1</code> nor <code>mod_mult_2</code> are adequate, and we should use another model instead?
There are two areas in inferential statistics that address these questions.
The first area works with comparisons between two models, one using a subset of regressors from the other, as in the coffee example where <code>mod_mult_1</code> used the regressors <code>aroma</code>, <code>flavor</code> and <code>moisture_percentage</code> while <code>mod_mult_2</code> used only two of those regressors: <code>aroma</code> and <code>moisture_percentage</code>.
We discuss methods addressing this comparison in Subsection <a href="inference-for-regression.html#hypo-test-mult-lm">10.5.3</a>.
The second area is called <em>model selection</em> or <em>variable selection</em> and uses alternative methods to determine which model, out of the possible available is the most adequate.</p>
</div>
<div id="confidence-intervals-1" class="section level3" number="10.5.2">
<h3>
<span class="header-section-number">10.5.2</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals-1"><i class="fas fa-link"></i></a>
</h3>
<p>A 95% confidence interval for any coefficient in multiple linear regression is constructed in exactly the same way as we did for simple linear regression, but we should always interpret them as dependent on the model for which they were attained.</p>
<p>For example, the formula for a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> is given by <span class="math inline">\(b_1 \pm q \cdot SE_{b_1}(s)\)</span> where the critical value <span class="math inline">\(q\)</span> is determined by the level of confidence required, the sample size used (<span class="math inline">\(n\)</span>), and the corresponding degrees of freedom needed for the <span class="math inline">\(t\)</span>-distribution (<span class="math inline">\(n - p\)</span>).
In the coffee example, the model <code>mod_mult</code> contains</p>
<ul>
<li>three numerical regressors (<code>aroma</code>, <code>flavor</code>, and <code>moisture_content</code>),</li>
<li>one factor (<code>continent_of_origin</code>),</li>
<li>the confidence level is 95%,</li>
<li>the sample size is <span class="math inline">\(n = 207\)</span>,</li>
<li>the number of regression coefficients is <span class="math inline">\(p = 7\)</span>:</li>
<li>one intercept (<span class="math inline">\(b_0\)</span>),</li>
<li>three partial slopes for <code>aroma</code>, <code>flavor</code> and <code>moisture_content</code> (<span class="math inline">\(b_1, b_2\)</span>, and <span class="math inline">\(b_3\)</span>), and</li>
<li>three coefficients for the factor levels in the model (<span class="math inline">\(b_{02}\)</span>, <span class="math inline">\(b_{03}\)</span>, and <span class="math inline">\(b_{04}\)</span>).</li>
</ul>
<p>So the degrees of freedom are <span class="math inline">\(n - p = 207 - 7 =  200\)</span>.</p>
<p>The 95% confidence interval for the partial slope of <span class="math inline">\(b_1\)</span> <code>aroma</code> is determined by</p>
<p><span class="math display">\[\begin{aligned}
b_1 &amp;\pm q \cdot SE(b_1)\\
= 1.73 &amp;\pm 1.97\cdot 0.22\\
= (1.29 &amp;, 2.17)
\end{aligned}\]</span></p>
<p>The interpretation of this interval is the customary: “We are 95% confident that the population partial slope for <code>aroma</code> (<span class="math inline">\(\beta_1\)</span>) in the model <code>mod_mult</code> is a number between 1.29 and 2.17.”</p>
<p>We find these values using <a href="https://moderndive.github.io/moderndive/reference/get_regression_table.html"><code>get_regression_table()</code></a> in model <code>mod_mult</code>.
This time, however, we add the argument <code>conf.level = 0.98</code> to get 98% confidence intervals as shown in Table <a href="inference-for-regression.html#tab:mult-model-2">10.12</a>.</p>
<div class="sourceCode" id="cb520"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">mod_mult</span>, conf.level <span class="op">=</span> <span class="fl">0.98</span><span class="op">)</span></span></code></pre></div>

<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mult-model-2">TABLE 10.12: </span>The regression table for <code>mod_mult</code> with 98% level
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
37.321
</td>
<td style="text-align:right;">
1.116
</td>
<td style="text-align:right;">
33.45
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
34.705
</td>
<td style="text-align:right;">
39.938
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
1.732
</td>
<td style="text-align:right;">
0.222
</td>
<td style="text-align:right;">
7.80
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.211
</td>
<td style="text-align:right;">
2.252
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
4.316
</td>
<td style="text-align:right;">
0.226
</td>
<td style="text-align:right;">
19.09
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
3.786
</td>
<td style="text-align:right;">
4.846
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
-0.008
</td>
<td style="text-align:right;">
0.030
</td>
<td style="text-align:right;">
-0.27
</td>
<td style="text-align:right;">
0.787
</td>
<td style="text-align:right;">
-0.078
</td>
<td style="text-align:right;">
0.062
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: Asia
</td>
<td style="text-align:right;">
-0.393
</td>
<td style="text-align:right;">
0.121
</td>
<td style="text-align:right;">
-3.24
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.678
</td>
<td style="text-align:right;">
-0.108
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: North America
</td>
<td style="text-align:right;">
-0.273
</td>
<td style="text-align:right;">
0.127
</td>
<td style="text-align:right;">
-2.15
</td>
<td style="text-align:right;">
0.033
</td>
<td style="text-align:right;">
-0.571
</td>
<td style="text-align:right;">
0.025
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: South America
</td>
<td style="text-align:right;">
-0.478
</td>
<td style="text-align:right;">
0.142
</td>
<td style="text-align:right;">
-3.38
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.810
</td>
<td style="text-align:right;">
-0.146
</td>
</tr>
</tbody>
</table></div>
<p>The interpretation for the coefficient for <code>flavor</code>, for example, is: “We are 98% confident that the value of <span class="math inline">\(\beta_2\)</span> (the population partial slope for <code>flavor</code>) is between 3.79 and 4.85.”</p>
</div>
<div id="hypo-test-mult-lm" class="section level3" number="10.5.3">
<h3>
<span class="header-section-number">10.5.3</span> Hypothesis test for a single coefficient<a class="anchor" aria-label="anchor" href="#hypo-test-mult-lm"><i class="fas fa-link"></i></a>
</h3>
<p>The hypothesis test for one coefficient, say <span class="math inline">\(\beta_1\)</span> in the model, is similar to the one for simple linear regression. The general formulation for a two-sided test is</p>
<p><span class="math display">\[\begin{aligned}
H_0: \beta_1 = B\quad \text{with }\beta_0, \beta_2, \dots, \beta_p \text{ given and arbitrary.}\\
H_A: \beta_1 \ne B\quad \text{with }\beta_0, \beta_2, \dots, \beta_p \text{ given and arbitrary.}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the hypothesized value for <span class="math inline">\(\beta_1\)</span>.
We make emphasis in stating that <span class="math inline">\(\beta_0, \beta_2, \dots, \beta_p\)</span> are given but arbitrary to acknowledge that the test only matters in the context of the appropriate model.
Also notice, that we can perform a test not only for <span class="math inline">\(\beta_1\)</span> but for any other parameter.</p>
<p>As we did for simple linear regression, the most commonly used test is the one where we check if <span class="math inline">\(\beta_j = 0\)</span> for any <span class="math inline">\(j=0,1,\dots,p\)</span>.
For <span class="math inline">\(\beta_1\)</span> the two-sided test would be:</p>
<p><span class="math display">\[\begin{aligned}
H_0: \beta_1 = 0\quad \text{with }\beta_0, \beta_2, \dots, \beta_p \text{ given and arbitrary}\\
H_A: \beta_1 \ne 0\quad \text{with }\beta_0, \beta_2, \dots, \beta_p \text{ given and arbitrary}
\end{aligned}\]</span></p>
<p>In simple linear regression, testing for <span class="math inline">\(\beta_1 = 0\)</span> was testing to determine if there was a linear relationship between the response and the only regressor.
Now, testing for <span class="math inline">\(\beta_1 = 0\)</span> is testing whether the corresponding regressor should be part of a linear model that already contains all the other regressors.</p>
<p>This test can be performed with any of the partial slope parameters.
For example, we use the coffee example and model <code>mod_mult_1</code> (the model with only three numerical regressors) and perform a test for <span class="math inline">\(\beta_2\)</span> (the population partial slope for regressor <code>flavor</code>). The hypotheses are:</p>
<p><span class="math display">\[\begin{aligned}
H_0: \beta_2 = 0\quad \text{with }\beta_0, \beta_1, \beta_3, \beta_{02}, \beta_{03}, \beta_{04} \text{ given and arbitrary.}\\
H_A: \beta_2 \ne 0\quad \text{with }\beta_0, \beta_1, \beta_3, \beta_{02}, \beta_{03}, \beta_{04} \text{ given and arbitrary.}
\end{aligned}\]</span></p>
<p>The relevant code is shown as follows with the output:</p>

<div class="sourceCode" id="cb521"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_table.html">get_regression_table</a></span><span class="op">(</span><span class="va">mod_mult_1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mult-model-3">TABLE 10.13: </span>The regression table for total_cup_points by aroma + flavor + moisture_percentage
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
36.778
</td>
<td style="text-align:right;">
1.095
</td>
<td style="text-align:right;">
33.591
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
34.619
</td>
<td style="text-align:right;">
38.937
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
1.800
</td>
<td style="text-align:right;">
0.223
</td>
<td style="text-align:right;">
8.089
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.361
</td>
<td style="text-align:right;">
2.239
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
4.285
</td>
<td style="text-align:right;">
0.229
</td>
<td style="text-align:right;">
18.698
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
3.833
</td>
<td style="text-align:right;">
4.737
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
-0.015
</td>
<td style="text-align:right;">
0.029
</td>
<td style="text-align:right;">
-0.499
</td>
<td style="text-align:right;">
0.618
</td>
<td style="text-align:right;">
-0.072
</td>
<td style="text-align:right;">
0.043
</td>
</tr>
</tbody>
</table></div>
<p>The t-test statistic is</p>
<p><span class="math display">\[t = \frac{b_2 - 0}{SE(b_2)} = \frac{4.28 - 0}{0.23} = 18.7\]</span></p>
<p>and using this test statistic, the associated <span class="math inline">\(p\)</span>-value is near zero and R only shows the output as zero.
We have enough evidence to reject the null hypothesis that <span class="math inline">\(\beta_2 = 0\)</span>.
Recall that when we reject the null hypothesis we say that the result was <em>statistically significant</em>, and we have enough evidence to conclude the alternative hypothesis (<span class="math inline">\(\beta_2 \ne 0\)</span>).
This implies that changes in <code>flavor</code> score provide information about the <code>total_cup_points</code> when <code>flavor</code> is added to a model that already contains <code>aroma</code> and <code>moisture_percentage</code>.</p>
<p>Table <a href="inference-for-regression.html#tab:mult-model-3">10.13</a> provides information for all the coefficients. Observe, in particular, that the test for <span class="math inline">\(\beta_1\)</span> (<code>aroma</code>) is also statistically significant but the test for <span class="math inline">\(\beta_3\)</span> (<code>moisture_percentage</code>) is not (<span class="math inline">\(p\)</span>-value = 0.62). For the latter, the conclusion is that there is not statistical evidence to reject the null hypothesis that this partial slope was zero. In other words, we have not found evidence that adding <code>moisture_percentage</code> to a model that already includes <code>aroma</code> and <code>flavor</code> helps explaining changes in the response <code>total_cup_points</code>. We can remove the <code>moisture_percentage</code> regressor from the model.</p>
</div>
<div id="hypo-test-mult-lm-1" class="section level3" number="10.5.4">
<h3>
<span class="header-section-number">10.5.4</span> Hypothesis test for model comparison<a class="anchor" aria-label="anchor" href="#hypo-test-mult-lm-1"><i class="fas fa-link"></i></a>
</h3>
<p>There is another hypothesis test that can be performed for multiple linear regression, a test that compares two models, one with a given set of regressors called the <em>full model</em> and the other with only a subset of those regressors called the <em>reduced model</em>.</p>
<p>Using the <code>coffee_data</code> example, suppose that the full model is:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_3 \cdot X_3 + \epsilon\]</span> where <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span> are the parameters for the partial slopes of <code>aroma</code>, <code>flavor</code>, and <code>moisture_precentage</code>, respectively. The multiple linear regression outcomes using the <code>coffee_data</code> dataset on this model are stored in the object <code>mod_mult_1</code>. The reduced model does not contain the regressor <code>flavor</code> and is given by</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 \cdot X_1  + \beta_3 \cdot X_3 + \epsilon.\]</span></p>
<p>The multiple linear regression output using this model is stored in the object <code>mod_mult_2</code>. The hypothesis test for comparing the full and reduced models can be written as:</p>
<p><span class="math display">\[\begin{aligned}
H_0:\quad &amp;Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2  + \epsilon\\
H_A:\quad &amp;Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_3 \cdot X_3 + \epsilon
\end{aligned}\]</span></p>
<p>or in words:</p>
<p><span class="math display">\[\begin{aligned}
H_0&amp;:\quad \text{the reduced model is adequate}\\
H_A&amp;:\quad \text{the full model is needed}
\end{aligned}\]</span></p>
<p>This test is called an ANOVA test or an <span class="math inline">\(F\)</span>-test, because the distribution of the test statistic follows an <span class="math inline">\(F\)</span> distribution.
The way it works is that the test compares the sum of squared residuals of both the full and reduced models and determines whether the difference between these models was large enough to suggest that the full model is needed.</p>
<p>To get the result of this test in R, we use the R function <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> and enter the reduced model followed by the full model with Table <a href="inference-for-regression.html#tab:mult-model-4">10.14</a> providing information for this test.</p>
<div class="sourceCode" id="cb522"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">mod_mult_2</span>, <span class="va">mod_mult_1</span><span class="op">)</span> </span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mult-model-4">TABLE 10.14: </span>ANOVA test for model comparison
</caption>
<thead><tr>
<th style="text-align:right;">
Res.Df
</th>
<th style="text-align:right;">
RSS
</th>
<th style="text-align:right;">
Df
</th>
<th style="text-align:right;">
Sum of Sq
</th>
<th style="text-align:right;">
F
</th>
<th style="text-align:right;">
Pr(&gt;F)
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
204
</td>
<td style="text-align:right;">
149.9
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;">
203
</td>
<td style="text-align:right;">
55.1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
94.8
</td>
<td style="text-align:right;">
350
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table></div>
<p>The test statistic is given in the second row for the column <code>F</code>.
The test statistic is <span class="math inline">\(F =349.6\)</span> and the associated <span class="math inline">\(p\)</span>-value is near zero.
In conclusion, we reject the null hypothesis and conclude that the full model was needed.</p>
<p>The ANOVA test can be used to test more than one regressor at a time.
This is useful when you have factors (categorical variables) in your model, as all the factor levels should be tested simultaneously.
We use our example again, this time making the full model the model with factor <code>continent_of_origin</code> in addition to all three numerical regressors, and the reduced model is the model without <code>continent_of_origin</code>.
These models have been computed already in <code>mod_mult</code> and <code>mod_mult_1</code>, respectively.
The ANOVA test is performed as follows with the output given in Table <a href="inference-for-regression.html#tab:mult-model-5">10.15</a>:</p>
<div class="sourceCode" id="cb523"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">mod_mult_1</span>, <span class="va">mod_mult</span><span class="op">)</span> </span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mult-model-5">TABLE 10.15: </span>ANOVA test for second model comparison
</caption>
<thead><tr>
<th style="text-align:right;">
Res.Df
</th>
<th style="text-align:right;">
RSS
</th>
<th style="text-align:right;">
Df
</th>
<th style="text-align:right;">
Sum of Sq
</th>
<th style="text-align:right;">
F
</th>
<th style="text-align:right;">
Pr(&gt;F)
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
203
</td>
<td style="text-align:right;">
55.1
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
51.3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3.72
</td>
<td style="text-align:right;">
4.83
</td>
<td style="text-align:right;">
0.003
</td>
</tr>
</tbody>
</table></div>
<p>Observe in this output that the degrees of freedom are 3, because we are testing three coefficients at the same time, <span class="math inline">\(\beta_{02}\)</span>, <span class="math inline">\(\beta_{03}\)</span>, and <span class="math inline">\(\beta_{04}\)</span>.
When we are testing for the inclusion of a factor in the model, we always need to test all the factor levels at once.
Based on the output, the test statistic is <span class="math inline">\(F =4.83\)</span> and the associated <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.003\)</span>.
We reject the null hypothesis and conclude that the full model was needed or, alternatively, that it is appropriate to add the factor <code>continent_of_origin</code> to a model that already has <code>aroma</code>, <code>flavor</code>, and <code>moisture_percentage</code>.</p>
</div>
<div id="model-fit-mult" class="section level3" number="10.5.5">
<h3>
<span class="header-section-number">10.5.5</span> Model fit and diagnostics<a class="anchor" aria-label="anchor" href="#model-fit-mult"><i class="fas fa-link"></i></a>
</h3>
<p>As we did for simple linear regression we use the residuals to determine the fit of the model and whether some of the assumptions are not met.
In particular, we continue using the plot of residuals against the fitted values and if no model violations are present the plot should be close to a null plot.</p>
<p>While most of the treatment and interpretations are similar to those presented for simple linear regression, when the plot of residuals against fitted values is not a null plot, we know that there is some sort of violation to one or more of the assumptions of the model.
It is no longer clear what is the reason for this, but at least one assumption is not met.
On the other hand, if the residuals-against-fitted-values plot appears to be close to a null plot, none of the assumptions have been broken and we can proceed with the use of this model.</p>
<p>Let’s use the coffee example.
Recall that we have shown that regressors <code>aroma</code>, <code>flavor</code>, and <code>continent_of_origin</code> were statistically significant, but <code>moisture_percentage</code> was not.
So, we create a model only with the relevant regressors called <code>mod_mult_final</code>, determine the residuals, and create a plot of residuals against fitted values and a QQ-plot using the <code><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange()</a></code> function from the <code>gridExtra</code> package in Figure <a href="inference-for-regression.html#fig:grid-arrange-plot-check">10.17</a>:</p>
<div class="sourceCode" id="cb524"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit regression model:</span></span>
<span><span class="va">mod_mult_final</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">continent_of_origin</span>, </span>
<span>                     <span class="va">coffee_data</span><span class="op">)</span></span>
<span><span class="co"># Get fitted values and residuals:</span></span>
<span><span class="va">fit_and_res_mult</span> <span class="op">&lt;-</span> <span class="fu"><a href="moderndive.github.io/moderndive/reference/get_regression_points.html">get_regression_points</a></span><span class="op">(</span><span class="va">mod_mult_final</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb525"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g1</span> <span class="op">&lt;-</span> <span class="va">fit_and_res_mult</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">total_cup_points_hat</span>, y <span class="op">=</span> <span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"fitted values (total cup points)"</span>, y <span class="op">=</span> <span class="st">"residual"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="va">g2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">fit_and_res_mult</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>sample <span class="op">=</span> <span class="va">residual</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq_line</a></span><span class="op">(</span>col<span class="op">=</span><span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">g1</span>, <span class="va">g2</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:grid-arrange-plot-check"></span>
<img src="ModernDive_files/figure-html/grid-arrange-plot-check-1.png" alt="Residuals vs. fitted values plot and QQ-plot for the multiple regression model." width="\textwidth"><p class="caption">
FIGURE 10.17: Residuals vs. fitted values plot and QQ-plot for the multiple regression model.
</p>
</div>
<p>The plot of residuals against fitted values (left) appears to be close to a null plot. This result is desirable because it supports the <strong>L</strong>inearity condition as no patterns are observed in this plot.
The <strong>E</strong>qual or constant variance also holds as the vertical dispersion seems to be fairly uniform for any fitted values.
Since we have assumed the data collected was random and there are no time sequences or other sequences to consider, the assumption of <strong>I</strong>ndependence seems to be acceptable too.
Finally, the QQ-plot suggests (with the exception of one or two observations) that the residuals follow approximately the normal distribution. We can conclude that this model seems to be good enough to hold the assumptions of the model.</p>
<p>This example concludes our treatment of theory-based inference. We now proceed to study the simulation-based inference.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.19)</strong> Why is it essential to know that the estimators (<span class="math inline">\(b_0, b_1, \dots, b_p\)</span>) in multiple linear regression are unbiased?</p>
<ul>
<li>A. It ensures that the variance of the estimators is always zero.</li>
<li>B. It means that, on average, the estimators will equal the true population parameters they estimate.</li>
<li>C. It implies that the estimators have a standard error of zero.</li>
<li>D. It suggests that the regression model will always have a perfect fit.</li>
</ul>
<p><strong>(LC10.20)</strong> Why do the least-squares estimates of coefficients change when different sets of regressors are used in multiple linear regression?</p>
<ul>
<li>A. Because the coefficients are recalculated each time, irrespective of the regressors.</li>
<li>B. Because the residuals are always zero when regressors are changed.</li>
<li>C. Because the value of each coefficient depends on the specific combination of regressors included in the model.</li>
<li>D. Because all models with different regressors will produce identical estimates.</li>
</ul>
<p><strong>(LC10.21)</strong> How is a 95% confidence interval for a coefficient in multiple linear regression constructed?</p>
<ul>
<li>A. By using the point estimate, the critical value from the t-distribution, and the standard error of the coefficient.</li>
<li>B. By taking the standard deviation of the coefficients only.</li>
<li>C. By resampling the data without replacement.</li>
<li>D. By calculating the mean of all the coefficients.</li>
</ul>
<p><strong>(LC10.22)</strong> What does the ANOVA test for comparing two models in multiple linear regression evaluate?</p>
<ul>
<li>A. Whether all regressors in both models have the same coefficients.</li>
<li>B. Whether the reduced model is adequate or if the full model is needed.</li>
<li>C. Whether the residuals of the two models follow a normal distribution.</li>
<li>D. Whether the regression coefficients of one model are unbiased estimators.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="simulation-based-inference-for-multiple-linear-regression" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Simulation-based Inference for multiple linear regression<a class="anchor" aria-label="anchor" href="#simulation-based-inference-for-multiple-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<div id="confidence-intervals-for-the-partial-slopes-using-infer" class="section level3" number="10.6.1">
<h3>
<span class="header-section-number">10.6.1</span> Confidence intervals for the partial slopes using <code>infer</code><a class="anchor" aria-label="anchor" href="#confidence-intervals-for-the-partial-slopes-using-infer"><i class="fas fa-link"></i></a>
</h3>
<p>We’ll now use the simulation-based methods you previously learned in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> and <a href="hypothesis-testing.html#hypothesis-testing">9</a> to compute ranges of plausible values for partial slopes with multiple linear regression. Recall that simulation-based methods provide an alternative to the theory-based methods in that they do not rely on the assumptions of normality or large sample sizes. We’ll use the <code>infer</code> package as we did with simple linear regression in Section <a href="inference-for-regression.html#infer-regression">10.3</a>, but this time using the <code><a href="https://generics.r-lib.org/reference/fit.html">fit()</a></code> function.</p>
<div id="getting-the-observed-fitted-model" class="section level4 unnumbered">
<h4>Getting the observed fitted model<a class="anchor" aria-label="anchor" href="#getting-the-observed-fitted-model"><i class="fas fa-link"></i></a>
</h4>
<p>We will revisit using our full model on the <code>coffee_data</code> with the factor of <code>continent_of_origin</code> and three numerical regressors in <code>aroma</code>, <code>flavor</code>, and <code>moisture_percentage</code>. As we did with hypothesis testing in Chapter <a href="hypothesis-testing.html#hypothesis-testing">9</a> and in Section <a href="inference-for-regression.html#infer-hypo-slr">10.3.2</a>, we can retrieve the observed statistic. In this case, we get the observed coefficients of the model using the <code><a href="https://infer.tidymodels.org/reference/specify.html">specify()</a></code> function on our full model with formula syntax combined with <code><a href="https://generics.r-lib.org/reference/fit.html">fit()</a></code>:</p>
<div class="sourceCode" id="cb526"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">observed_fit</span> <span class="op">&lt;-</span> <span class="va">coffee_data</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span></span>
<span>    <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span> <span class="op">+</span> <span class="va">continent_of_origin</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">observed_fit</span></span></code></pre></div>
<pre><code># A tibble: 7 × 2
  term                                estimate
  &lt;chr&gt;                                  &lt;dbl&gt;
1 intercept                        37.3214    
2 aroma                             1.73160   
3 flavor                            4.31600   
4 moisture_percentage              -0.00807976
5 continent_of_originAsia          -0.392936  
6 continent_of_originNorth America -0.273427  
7 continent_of_originSouth America -0.478137  </code></pre>
<p>As we would expect, these values match up with the values of <code>mod_mult_table</code> given in the first two columns there in Table <a href="inference-for-regression.html#tab:mod-mult-table-again">10.16</a>:</p>

<div class="sourceCode" id="cb528"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod_mult_table</span></span></code></pre></div>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:mod-mult-table-again">TABLE 10.16: </span>The regression table for <code>mod_mult</code>
</caption>
<thead><tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
37.321
</td>
<td style="text-align:right;">
1.116
</td>
<td style="text-align:right;">
33.45
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
34.705
</td>
<td style="text-align:right;">
39.938
</td>
</tr>
<tr>
<td style="text-align:left;">
aroma
</td>
<td style="text-align:right;">
1.732
</td>
<td style="text-align:right;">
0.222
</td>
<td style="text-align:right;">
7.80
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.211
</td>
<td style="text-align:right;">
2.252
</td>
</tr>
<tr>
<td style="text-align:left;">
flavor
</td>
<td style="text-align:right;">
4.316
</td>
<td style="text-align:right;">
0.226
</td>
<td style="text-align:right;">
19.09
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
3.786
</td>
<td style="text-align:right;">
4.846
</td>
</tr>
<tr>
<td style="text-align:left;">
moisture_percentage
</td>
<td style="text-align:right;">
-0.008
</td>
<td style="text-align:right;">
0.030
</td>
<td style="text-align:right;">
-0.27
</td>
<td style="text-align:right;">
0.787
</td>
<td style="text-align:right;">
-0.078
</td>
<td style="text-align:right;">
0.062
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: Asia
</td>
<td style="text-align:right;">
-0.393
</td>
<td style="text-align:right;">
0.121
</td>
<td style="text-align:right;">
-3.24
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.678
</td>
<td style="text-align:right;">
-0.108
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: North America
</td>
<td style="text-align:right;">
-0.273
</td>
<td style="text-align:right;">
0.127
</td>
<td style="text-align:right;">
-2.15
</td>
<td style="text-align:right;">
0.033
</td>
<td style="text-align:right;">
-0.571
</td>
<td style="text-align:right;">
0.025
</td>
</tr>
<tr>
<td style="text-align:left;">
continent_of_origin: South America
</td>
<td style="text-align:right;">
-0.478
</td>
<td style="text-align:right;">
0.142
</td>
<td style="text-align:right;">
-3.38
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.810
</td>
<td style="text-align:right;">
-0.146
</td>
</tr>
</tbody>
</table></div>
<p>The <code>observed_fit</code> values will be our point estimates for the partial slopes in the confidence intervals.</p>
</div>
<div id="bootstrap-distribution-for-the-partial-slopes" class="section level4 unnumbered">
<h4>Bootstrap distribution for the partial slopes<a class="anchor" aria-label="anchor" href="#bootstrap-distribution-for-the-partial-slopes"><i class="fas fa-link"></i></a>
</h4>
<p>We will now find the bootstrap distribution for the partial slopes using the <code>infer</code> workflow. Just like in Section <a href="inference-for-regression.html#infer-ci-slr">10.3.1</a>, we are now resampling entire rows of values to construct the bootstrap distribution for the fitted partial slopes using our full sample of 207 coffees:</p>
<ol style="list-style-type: decimal">
<li>
<code><a href="https://infer.tidymodels.org/reference/specify.html">specify()</a></code> the variables of interest in <code>coffee_data</code> with the formula: <code>total_cup_points ~ aroma + flavor + moisture_percentage + continent_of_origin</code>.</li>
<li>
<code><a href="https://infer.tidymodels.org/reference/generate.html">generate()</a></code> replicates by using <code>bootstrap</code> resampling with replacement
from the original sample of 207 coffees. We generate
<code>reps = 1000</code> replicates using <code>type = "bootstrap"</code> for a total of
<span class="math inline">\(207 \cdot 1000 = 207,000\)</span>
rows.</li>
</ol>
<div class="sourceCode" id="cb529"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span></span>
<span>    <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">continent_of_origin</span> <span class="op">+</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/generate.html">generate</a></span><span class="op">(</span>reps <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">"bootstrap"</span><span class="op">)</span></span></code></pre></div>
<pre><code>Response: total_cup_points (numeric)
Explanatory: continent_of_origin (factor), aroma (numeric), flavor (numeric), mo...
# A tibble: 207,000 × 6
# Groups:   replicate [1,000]
   replicate total_cup_points continent_of_origin aroma flavor moisture_percentage
       &lt;int&gt;            &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;               &lt;dbl&gt;
 1         1            83.25 Africa               7.92   7.67                10.4
 2         1            83.67 Asia                 7.58   7.75                 9.2
 3         1            85.5  Asia                 8.17   8.08                10.6
 4         1            82    North America        7.42   7.42                11.5
 5         1            84.42 Asia                 7.83   8                   10.1
 6         1            86.08 Asia                 8.17   8.08                10.2
 7         1            84.08 North America        7.67   7.83                11  
 8         1            83.67 Asia                 7.83   7.83                10.2
 9         1            82.75 North America        7.33   7.5                 11.8
10         1            84.33 North America        7.83   7.83                10.3
# ℹ 206,990 more rows</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Lastly, <code><a href="https://generics.r-lib.org/reference/fit.html">fit()</a></code> models for each of the replicates in the <code>boot_distribution_mlr</code> variable. Here, <code>mlr</code> stands for multiple linear regression.</li>
</ol>
<div class="sourceCode" id="cb531"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">boot_distribution_mlr</span> <span class="op">&lt;-</span> <span class="va">coffee_quality</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span></span>
<span>    <span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">continent_of_origin</span> <span class="op">+</span> <span class="va">aroma</span> <span class="op">+</span> <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/generate.html">generate</a></span><span class="op">(</span>reps <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">"bootstrap"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">boot_distribution_mlr</span></span></code></pre></div>
<pre><code># A tibble: 7,000 × 3
# Groups:   replicate [1,000]
   replicate term                                estimate
       &lt;int&gt; &lt;chr&gt;                                  &lt;dbl&gt;
 1         1 intercept                        37.4459    
 2         1 continent_of_originAsia          -0.267451  
 3         1 continent_of_originNorth America -0.203579  
 4         1 continent_of_originSouth America -0.458541  
 5         1 aroma                             1.80789   
 6         1 flavor                            4.20492   
 7         1 moisture_percentage              -0.00643938
 8         2 intercept                        34.3890    
 9         2 continent_of_originAsia          -0.425770  
10         2 continent_of_originNorth America -0.237750  
# ℹ 6,990 more rows</code></pre>
<p>Since we have 7 coefficients in our model corresponding to the intercept, three levels of <code>continent_of_origin</code>, <code>aroma</code>, <code>flavor</code>, and <code>moisture_percentage</code>, we have 7 rows for each <code>replicate</code>. This results in a total of 7000 rows in the <code>boot_distribution_mlr</code> data frame. We can visualize the bootstrap distribution for the partial slopes in Figure <a href="inference-for-regression.html#fig:boot-distn-slopes">10.18</a>.</p>
<div class="sourceCode" id="cb533"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://infer.tidymodels.org/reference/visualize.html">visualize</a></span><span class="op">(</span><span class="va">boot_distribution_mlr</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boot-distn-slopes"></span>
<img src="ModernDive_files/figure-html/boot-distn-slopes-1.png" alt="Bootstrap distributions of partial slopes." width="68%"><p class="caption">
FIGURE 10.18: Bootstrap distributions of partial slopes.
</p>
</div>
</div>
<div id="confidence-intervals-for-the-partial-slopes" class="section level4 unnumbered">
<h4>Confidence intervals for the partial slopes<a class="anchor" aria-label="anchor" href="#confidence-intervals-for-the-partial-slopes"><i class="fas fa-link"></i></a>
</h4>
<p>As we did in Section <a href="inference-for-regression.html#infer-ci-slr">10.3.1</a>, we can construct 95% confidence intervals for the partial slopes. Here, we focus on using the percentile-based method with the <code><a href="https://infer.tidymodels.org/reference/get_confidence_interval.html">get_confidence_interval()</a></code> function and <code>type = "percentile"</code> to construct these intervals.</p>
<div class="sourceCode" id="cb534"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">confidence_intervals_mlr</span> <span class="op">&lt;-</span> <span class="va">boot_distribution_mlr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/get_confidence_interval.html">get_confidence_interval</a></span><span class="op">(</span></span>
<span>    level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"percentile"</span>,</span>
<span>    point_estimate <span class="op">=</span> <span class="va">observed_fit</span><span class="op">)</span></span>
<span><span class="va">confidence_intervals_mlr</span></span></code></pre></div>
<pre><code># A tibble: 7 × 3
  term                               lower_ci   upper_ci
  &lt;chr&gt;                                 &lt;dbl&gt;      &lt;dbl&gt;
1 aroma                             1.33584    2.13218  
2 continent_of_originAsia          -0.657425  -0.143179 
3 continent_of_originNorth America -0.557557   0.0131542
4 continent_of_originSouth America -0.809466  -0.153730 
5 flavor                            3.88508    4.74376  
6 intercept                        34.5254    40.0449   
7 moisture_percentage              -0.0924515  0.0417502</code></pre>
<p>In reviewing the confidence intervals, we note that the confidence intervals for <code>aroma</code> and <code>flavor</code> do not include 0. This suggests that they are statistically significant. This indicates that <code>aroma</code> and <code>flavor</code> have a meaningful and reliable impact on the response variable in the presence of other predictors.</p>
<p>We also note that 0 is included in the confidence interval for <code>moisture_percentage</code>. This again provides evidence that it might not be a useful regressor in this multiple linear regression model. This implies that <code>moisture_percentage</code> does not significantly contribute to explaining the variability in the response variable after accounting for other predictors in the model.</p>
<p>We can also visualize these confidence intervals. This is done in Figure <a href="inference-for-regression.html#fig:ci-slopes-multiple">10.19</a>.</p>
<div class="sourceCode" id="cb536"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://infer.tidymodels.org/reference/visualize.html">visualize</a></span><span class="op">(</span><span class="va">boot_distribution_mlr</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/shade_confidence_interval.html">shade_confidence_interval</a></span><span class="op">(</span>endpoints <span class="op">=</span> <span class="va">confidence_intervals_mlr</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ci-slopes-multiple"></span>
<img src="ModernDive_files/figure-html/ci-slopes-multiple-1.png" alt="95% confidence intervals for the partial slopes." width="\textwidth"><p class="caption">
FIGURE 10.19: 95% confidence intervals for the partial slopes.
</p>
</div>
</div>
</div>
<div id="hypothesis-testing-for-the-partial-slopes-using-infer" class="section level3" number="10.6.2">
<h3>
<span class="header-section-number">10.6.2</span> Hypothesis testing for the partial slopes using <code>infer</code><a class="anchor" aria-label="anchor" href="#hypothesis-testing-for-the-partial-slopes-using-infer"><i class="fas fa-link"></i></a>
</h3>
<p>We can also conduct hypothesis tests for the partial slopes in multiple linear regression. We use the permutation test to test the null hypothesis <span class="math inline">\(H_0: \beta_i = 0\)</span> versus the alternative hypothesis <span class="math inline">\(H_A: \beta_i \neq 0\)</span> for each of the partial slopes. The <code>infer</code> package constructs the null distribution of the partial slopes under the null hypothesis of independence.</p>
<div id="null-distribution-for-the-partial-slopes" class="section level4 unnumbered">
<h4>Null distribution for the partial slopes<a class="anchor" aria-label="anchor" href="#null-distribution-for-the-partial-slopes"><i class="fas fa-link"></i></a>
</h4>
<p>We can also compute the null distribution for the partial slopes using the <code>infer</code> workflow. We will shuffle the values of the response variable <code>total_cup_points</code> across the values of the regressors <code>continent_of_origin</code>, <code>aroma</code>, <code>flavor</code>, and <code>moisture_percentage</code> in the <code>coffee_data</code> dataset. This is done under the assumption of independence between the response and the regressors. The syntax is similar to constructing the bootstrap distribution, but we use <code>type = "permute"</code> and set <code>hypothesize</code> to <code>null = "independence"</code>. We set our pseudo-random number generation seed to 2024 in order for the reader to get the same results with the shuffling.</p>
<div class="sourceCode" id="cb537"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2024</span><span class="op">)</span></span>
<span><span class="va">null_distribution_mlr</span> <span class="op">&lt;-</span> <span class="va">coffee_quality</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/specify.html">specify</a></span><span class="op">(</span><span class="va">total_cup_points</span> <span class="op">~</span> <span class="va">continent_of_origin</span> <span class="op">+</span> <span class="va">aroma</span> <span class="op">+</span> </span>
<span>      <span class="va">flavor</span> <span class="op">+</span> <span class="va">moisture_percentage</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/hypothesize.html">hypothesize</a></span><span class="op">(</span>null <span class="op">=</span> <span class="st">"independence"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/generate.html">generate</a></span><span class="op">(</span>reps <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">"permute"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">null_distribution_mlr</span></span></code></pre></div>
<pre><code># A tibble: 7,000 × 3
# Groups:   replicate [1,000]
   replicate term                               estimate
       &lt;int&gt; &lt;chr&gt;                                 &lt;dbl&gt;
 1         1 intercept                        82.3301   
 2         1 continent_of_originAsia          -0.193739 
 3         1 continent_of_originNorth America -0.371403 
 4         1 continent_of_originSouth America -0.0830341
 5         1 aroma                            -1.21891  
 6         1 flavor                            1.52397  
 7         1 moisture_percentage              -0.0747986
 8         2 intercept                        82.8068   
 9         2 continent_of_originAsia          -0.239054 
10         2 continent_of_originNorth America -0.617409 
# ℹ 6,990 more rows</code></pre>
</div>
<div id="hypothesis-tests-for-the-partial-slopes" class="section level4 unnumbered">
<h4>Hypothesis tests for the partial slopes<a class="anchor" aria-label="anchor" href="#hypothesis-tests-for-the-partial-slopes"><i class="fas fa-link"></i></a>
</h4>
<p>We can now conduct hypothesis tests for the partial slopes in multiple linear regression. We can use the permutation test to test the null hypothesis <span class="math inline">\(H_0: \beta_i = 0\)</span> versus the alternative hypothesis <span class="math inline">\(H_A: \beta_i \neq 0\)</span> for each of the partial slopes. Let’s use a significance level of <span class="math inline">\(\alpha = 0.05\)</span>. We can visualize the <span class="math inline">\(p\)</span>-values in the null distribution by comparing them to the observed test statistics. We do this by adding a <code><a href="https://infer.tidymodels.org/reference/shade_p_value.html">shade_p_value()</a></code> layer <code><a href="https://infer.tidymodels.org/reference/visualize.html">visualize()</a></code>.</p>
<div class="sourceCode" id="cb539"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://infer.tidymodels.org/reference/visualize.html">visualize</a></span><span class="op">(</span><span class="va">null_distribution_mlr</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/shade_p_value.html">shade_p_value</a></span><span class="op">(</span>obs_stat <span class="op">=</span> <span class="va">observed_fit</span>, direction <span class="op">=</span> <span class="st">"two-sided"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:shaded-p-values-partial"></span>
<img src="ModernDive_files/figure-html/shaded-p-values-partial-1.png" alt="Shaded p-values for the partial slopes in this multiple regression." width="55%"><p class="caption">
FIGURE 10.20: Shaded p-values for the partial slopes in this multiple regression.
</p>
</div>
<p>From these visualizations in Figure <a href="inference-for-regression.html#fig:shaded-p-values-partial">10.20</a>, we can surmise that <code>aroma</code> and <code>flavor</code> are statistically significant, as their observed test statistics fall far to the right of the null distribution. On the other hand, <code>moisture_percentage</code> is not statistically significant, as its observed test statistic falls within the null distribution. We can also compute the numerical <span class="math inline">\(p\)</span>-values using the <code><a href="https://infer.tidymodels.org/reference/get_p_value.html">get_p_value()</a></code> function.</p>
<div class="sourceCode" id="cb540"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">null_distribution_mlr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://infer.tidymodels.org/reference/get_p_value.html">get_p_value</a></span><span class="op">(</span>obs_stat <span class="op">=</span> <span class="va">observed_fit</span>, direction <span class="op">=</span> <span class="st">"two-sided"</span><span class="op">)</span></span></code></pre></div>
<pre><code># A tibble: 7 × 2
  term                             p_value
  &lt;chr&gt;                              &lt;dbl&gt;
1 aroma                              0.034
2 continent_of_originAsia            0.332
3 continent_of_originNorth America   0.56 
4 continent_of_originSouth America   0.352
5 flavor                             0    
6 intercept                          0    
7 moisture_percentage                0.918</code></pre>
<p>These results match up with our findings from the visualizations of the shaded <span class="math inline">\(p\)</span>-values with the null distribution and in the regression table in Table <a href="inference-for-regression.html#tab:mult-model-3">10.13</a>. We reject the null hypothesis <span class="math inline">\(H_0: \beta_i = 0\)</span> for <code>aroma</code> and <code>flavor</code>, but fail to reject it for <code>moisture_percentage</code>.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.23)</strong> Why might one prefer to use simulation-based methods (e.g., bootstrapping) for inference in multiple linear regression?</p>
<ul>
<li>A. Because simulation-based methods require larger sample sizes than theory-based methods.</li>
<li>B. Because simulation-based methods are always faster to compute than theory-based methods.</li>
<li>C. Because simulation-based methods guarantee the correct model is used.</li>
<li>D. Because simulation-based methods do not rely on the assumptions of normality or large sample sizes.</li>
</ul>
<p><strong>(LC10.24)</strong> What is the purpose of constructing a bootstrap distribution for the partial slopes in multiple linear regression?</p>
<ul>
<li>A. To replace the original data with random numbers.</li>
<li>B. To approximate the sampling distribution of the partial slopes by resampling with replacement.</li>
<li>C. To calculate the exact values of the coefficients in the population.</li>
<li>D. To test if the model assumptions are violated.</li>
</ul>
<p><strong>(LC10.25)</strong> If a 95% confidence interval for a partial slope in multiple linear regression includes 0, what does this suggest about the variable?</p>
<ul>
<li>A. The variable does not have a statistically significant relationship with the response variable.</li>
<li>B. The variable is statistically significant.</li>
<li>C. The variable’s coefficient estimate is always negative.</li>
<li>D. The variable was removed from the model during bootstrapping.</li>
</ul>
<p><strong>(LC10.26)</strong> In hypothesis testing for the partial slopes using permutation tests, what does it mean if an observed test statistic falls far to the right of the null distribution?</p>
<ul>
<li>A. The variable is likely to have no effect on the response.</li>
<li>B. The null hypothesis should be accepted.</li>
<li>C. The variable is likely statistically significant, and we should reject the null.</li>
<li>D. The observed data should be discarded.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
</div>
<div id="inference-conclusion" class="section level2" number="10.7">
<h2>
<span class="header-section-number">10.7</span> Conclusion<a class="anchor" aria-label="anchor" href="#inference-conclusion"><i class="fas fa-link"></i></a>
</h2>
<div id="summary-of-statistical-inference" class="section level3" number="10.7.1">
<h3>
<span class="header-section-number">10.7.1</span> Summary of statistical inference<a class="anchor" aria-label="anchor" href="#summary-of-statistical-inference"><i class="fas fa-link"></i></a>
</h3>
<p>We’ve finished the last two scenarios, which we re-display in Table <a href="inference-for-regression.html#tab:table-ch10">10.17</a>.</p>
<div class="inline-table"><table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:table-ch10">TABLE 10.17: </span>Scenarios of sampling for inference
</caption>
<thead><tr>
<th style="text-align:right;">
Scenario
</th>
<th style="text-align:left;">
Population parameter
</th>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
Point estimate
</th>
<th style="text-align:left;">
Symbol(s)
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;width: 0.5in; ">
1
</td>
<td style="text-align:left;width: 1.5in; ">
Population proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
2
</td>
<td style="text-align:left;width: 1.5in; ">
Population mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\mu\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\overline{x}\)</span> or <span class="math inline">\(\widehat{\mu}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
3
</td>
<td style="text-align:left;width: 1.5in; ">
Difference in population proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p_1 - p_2\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Difference in sample proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}_1 - \widehat{p}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
4
</td>
<td style="text-align:left;width: 1.5in; ">
Difference in population means
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\mu_1 - \mu_2\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Difference in sample means
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\overline{x}_1 - \overline{x}_2\)</span> or <span class="math inline">\(\widehat{\mu}_1 - \widehat{\mu}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
5
</td>
<td style="text-align:left;width: 1.5in; ">
Population regression slope
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\beta_1\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Fitted regression slope
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(b_1\)</span> or <span class="math inline">\(\widehat{\beta}_1\)</span>
</td>
</tr>
</tbody>
</table></div>
<p>Armed with the regression modeling techniques you learned in Chapters <a href="regression.html#regression">5</a> and <a href="multiple-regression.html#multiple-regression">6</a>, your understanding of sampling for inference in Chapter <a href="sampling.html#sampling">7</a>, and the tools for statistical inference like confidence intervals and hypothesis tests in Chapters <a href="confidence-intervals.html#confidence-intervals">8</a> and <a href="hypothesis-testing.html#hypothesis-testing">9</a>, you’re now equipped to study the significance of relationships between variables in a wide array of data! Many of the ideas presented here can be extended into multiple regression and other more advanced modeling techniques.</p>
</div>
<div id="additional-resources-8" class="section level3" number="10.7.2">
<h3>
<span class="header-section-number">10.7.2</span> Additional resources<a class="anchor" aria-label="anchor" href="#additional-resources-8"><i class="fas fa-link"></i></a>
</h3>
<p>An R script file of all R code used in this chapter is available <a href="scripts/10-inference-for-regression.R">here</a>.</p>
</div>
<div id="whats-to-come-9" class="section level3" number="10.7.3">
<h3>
<span class="header-section-number">10.7.3</span> What’s to come<a class="anchor" aria-label="anchor" href="#whats-to-come-9"><i class="fas fa-link"></i></a>
</h3>
<p>You’ve now concluded the last major part of the book on “Statistical Inference with <code>infer</code>.” The closing Chapter <a href="thinking-with-data.html#thinking-with-data">11</a> concludes this book with various short case studies involving real data, such as house prices in the city of Seattle, Washington in the US. You’ll see how the principles in this book can help you become a great storyteller with data!</p>

</div>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></div>
<div class="next"><a href="thinking-with-data.html"><span class="header-section-number">11</span> Tell Your Story with Data</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#inference-for-regression"><span class="header-section-number">10</span> Inference for Regression</a></li>
<li><a class="nav-link" href="#inf-packages">Needed packages</a></li>
<li>
<a class="nav-link" href="#the-simple-linear-regression-model"><span class="header-section-number">10.1</span> The simple linear regression model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#un-member-states-revisited"><span class="header-section-number">10.1.1</span> UN member states revisited</a></li>
<li><a class="nav-link" href="#simple-linear-model"><span class="header-section-number">10.1.2</span> The model</a></li>
<li><a class="nav-link" href="#sample-regression-inference"><span class="header-section-number">10.1.3</span> Using a sample for inference</a></li>
<li><a class="nav-link" href="#least-squares"><span class="header-section-number">10.1.4</span> The method of least squares</a></li>
<li><a class="nav-link" href="#properties-least-squares"><span class="header-section-number">10.1.5</span> Properties of the least squares estimators</a></li>
<li><a class="nav-link" href="#relating-basic-regression-to-other-methods"><span class="header-section-number">10.1.6</span> Relating basic regression to other methods</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#theory-simple-regression"><span class="header-section-number">10.2</span> Theory-based inference for simple linear regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#framework-simple-lm"><span class="header-section-number">10.2.1</span> Conceptual framework</a></li>
<li><a class="nav-link" href="#se-simple-lm"><span class="header-section-number">10.2.2</span> Standard errors for least-squares estimators</a></li>
<li><a class="nav-link" href="#conf-intervals-b0-b1"><span class="header-section-number">10.2.3</span> Confidence intervals for the least-squares estimators</a></li>
<li><a class="nav-link" href="#hypo-test-simple-lm"><span class="header-section-number">10.2.4</span> Hypothesis test for population slope</a></li>
<li><a class="nav-link" href="#regression-table"><span class="header-section-number">10.2.5</span> The regression table in R</a></li>
<li><a class="nav-link" href="#model-fit"><span class="header-section-number">10.2.6</span> Model fit and model assumptions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#infer-regression"><span class="header-section-number">10.3</span> Simulation-based inference for simple linear regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#infer-ci-slr"><span class="header-section-number">10.3.1</span> Confidence intervals for the population slope using infer</a></li>
<li><a class="nav-link" href="#infer-hypo-slr"><span class="header-section-number">10.3.2</span> Hypothesis test for population slope using infer</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-multiple-linear-regression-model"><span class="header-section-number">10.4</span> The multiple linear regression model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple-linear-model"><span class="header-section-number">10.4.1</span> The model</a></li>
<li><a class="nav-link" href="#example-coffee-quality-rating-scores"><span class="header-section-number">10.4.2</span> Example: coffee quality rating scores</a></li>
<li><a class="nav-link" href="#least-squares-multiple"><span class="header-section-number">10.4.3</span> Least squares for multiple regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#theory-multiple-regression"><span class="header-section-number">10.5</span> Theory-based inference for multiple linear regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#model-dependency"><span class="header-section-number">10.5.1</span> Model dependency of estimators</a></li>
<li><a class="nav-link" href="#confidence-intervals-1"><span class="header-section-number">10.5.2</span> Confidence intervals</a></li>
<li><a class="nav-link" href="#hypo-test-mult-lm"><span class="header-section-number">10.5.3</span> Hypothesis test for a single coefficient</a></li>
<li><a class="nav-link" href="#hypo-test-mult-lm-1"><span class="header-section-number">10.5.4</span> Hypothesis test for model comparison</a></li>
<li><a class="nav-link" href="#model-fit-mult"><span class="header-section-number">10.5.5</span> Model fit and diagnostics</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#simulation-based-inference-for-multiple-linear-regression"><span class="header-section-number">10.6</span> Simulation-based Inference for multiple linear regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#confidence-intervals-for-the-partial-slopes-using-infer"><span class="header-section-number">10.6.1</span> Confidence intervals for the partial slopes using infer</a></li>
<li><a class="nav-link" href="#hypothesis-testing-for-the-partial-slopes-using-infer"><span class="header-section-number">10.6.2</span> Hypothesis testing for the partial slopes using infer</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#inference-conclusion"><span class="header-section-number">10.7</span> Conclusion</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#summary-of-statistical-inference"><span class="header-section-number">10.7.1</span> Summary of statistical inference</a></li>
<li><a class="nav-link" href="#additional-resources-8"><span class="header-section-number">10.7.2</span> Additional resources</a></li>
<li><a class="nav-link" href="#whats-to-come-9"><span class="header-section-number">10.7.3</span> What’s to come</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/moderndive/ModernDive_book//blob/v2/10-inference-for-regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/moderndive/ModernDive_book//edit/v2/10-inference-for-regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Inference via Data Science</strong>: A ModernDive into R and the Tidyverse <br> (Second Edition)" was written by Chester Ismay, Albert Y. Kim, and Arturo Valdivia <br> Foreword by Kelly S. McConville. It was last built on August 06, 2025.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
