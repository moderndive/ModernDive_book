<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Estimation, Confidence Intervals, and Bootstrapping | Statistical Inference via Data Science</title>
  <meta name="description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="generator" content="bookdown 0.40.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Estimation, Confidence Intervals, and Bootstrapping | Statistical Inference via Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://moderndive.com//images/logos/book_cover.png" />
  <meta property="og:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="github-repo" content="moderndive/ModernDive_book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Estimation, Confidence Intervals, and Bootstrapping | Statistical Inference via Data Science" />
  <meta name="twitter:site" content="@ModernDive" />
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="twitter:image" content="https://moderndive.com//images/logos/book_cover.png" />

<meta name="author" content="Chester Ismay, Albert Y. Kim, and Arturo Valdivia   Foreword by Kelly S. McConville" />


<meta name="date" content="2024-09-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="images/logos/favicons/apple-touch-icon.png" />
  <link rel="shortcut icon" href="images/logos/favicons/favicon.ico" type="image/x-icon" />
<link rel="prev" href="7-sampling.html"/>
<link rel="next" href="9-hypothesis-testing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YVFBK9P73R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YVFBK9P73R');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to ModernDive</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#introduction-for-students"><i class="fa fa-check"></i>Introduction for students</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-we-hope-you-will-learn-from-this-book"><i class="fa fa-check"></i>What we hope you will learn from this book</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#datascience-pipeline"><i class="fa fa-check"></i>Data/science pipeline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#reproducible-research"><i class="fa fa-check"></i>Reproducible research</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#final-note-for-students"><i class="fa fa-check"></i>Final note for students</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#introduction-for-instructors"><i class="fa fa-check"></i>Introduction for instructors</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#resources"><i class="fa fa-check"></i>Resources</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#why-did-we-write-this-book"><i class="fa fa-check"></i>Why did we write this book?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#connect-and-contribute"><i class="fa fa-check"></i>Connect and contribute</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#versions-of-r-packages-used"><i class="fa fa-check"></i>Versions of R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="1" data-path="1-getting-started.html"><a href="1-getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started with Data in R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#r-rstudio"><i class="fa fa-check"></i><b>1.1</b> What are R and RStudio?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#installing"><i class="fa fa-check"></i><b>1.1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#using-r-via-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Using R via RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#code"><i class="fa fa-check"></i><b>1.2</b> How do I code in R?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1-getting-started.html"><a href="1-getting-started.html#programming-concepts"><i class="fa fa-check"></i><b>1.2.1</b> Basic programming concepts and terminology</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-getting-started.html"><a href="1-getting-started.html#messages"><i class="fa fa-check"></i><b>1.2.2</b> Errors, warnings, and messages</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-getting-started.html"><a href="1-getting-started.html#tips-code"><i class="fa fa-check"></i><b>1.2.3</b> Tips on learning to code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-getting-started.html"><a href="1-getting-started.html#packages"><i class="fa fa-check"></i><b>1.3</b> What are R packages?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-getting-started.html"><a href="1-getting-started.html#package-installation"><i class="fa fa-check"></i><b>1.3.1</b> Package installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-getting-started.html"><a href="1-getting-started.html#package-loading"><i class="fa fa-check"></i><b>1.3.2</b> Package loading</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-getting-started.html"><a href="1-getting-started.html#package-use"><i class="fa fa-check"></i><b>1.3.3</b> Package use</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights"><i class="fa fa-check"></i><b>1.4</b> Explore your first datasets</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights23-package"><i class="fa fa-check"></i><b>1.4.1</b> <code>nycflights23</code> package</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-getting-started.html"><a href="1-getting-started.html#flights-data-frame"><i class="fa fa-check"></i><b>1.4.2</b> <code>flights</code> data frame</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-getting-started.html"><a href="1-getting-started.html#exploredataframes"><i class="fa fa-check"></i><b>1.4.3</b> Exploring data frames</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-getting-started.html"><a href="1-getting-started.html#identification-vs-measurement-variables"><i class="fa fa-check"></i><b>1.4.4</b> Identification and measurement variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-getting-started.html"><a href="1-getting-started.html#help-files"><i class="fa fa-check"></i><b>1.4.5</b> Help files</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-getting-started.html"><a href="1-getting-started.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="1-getting-started.html"><a href="1-getting-started.html#additional-resources"><i class="fa fa-check"></i><b>1.5.1</b> Additional resources</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-getting-started.html"><a href="1-getting-started.html#whats-to-come"><i class="fa fa-check"></i><b>1.5.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Data Science with tidyverse</b></span></li>
<li class="chapter" data-level="2" data-path="2-viz.html"><a href="2-viz.html"><i class="fa fa-check"></i><b>2</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="" data-path="2-viz.html"><a href="2-viz.html#needed-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="2.1" data-path="2-viz.html"><a href="2-viz.html#grammarofgraphics"><i class="fa fa-check"></i><b>2.1</b> The grammar of graphics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2-viz.html"><a href="2-viz.html#components-of-the-grammar"><i class="fa fa-check"></i><b>2.1.1</b> Components of the grammar</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-viz.html"><a href="2-viz.html#gapminder"><i class="fa fa-check"></i><b>2.1.2</b> Gapminder data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-viz.html"><a href="2-viz.html#other-components"><i class="fa fa-check"></i><b>2.1.3</b> Other components</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-viz.html"><a href="2-viz.html#ggplot2-package"><i class="fa fa-check"></i><b>2.1.4</b> ggplot2 package</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-viz.html"><a href="2-viz.html#FiveNG"><i class="fa fa-check"></i><b>2.2</b> Five named graphs - the 5NG</a></li>
<li class="chapter" data-level="2.3" data-path="2-viz.html"><a href="2-viz.html#scatterplots"><i class="fa fa-check"></i><b>2.3</b> 5NG#1: Scatterplots</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-viz.html"><a href="2-viz.html#geompoint"><i class="fa fa-check"></i><b>2.3.1</b> Scatterplots via <code>geom_point</code></a></li>
<li class="chapter" data-level="2.3.2" data-path="2-viz.html"><a href="2-viz.html#overplotting"><i class="fa fa-check"></i><b>2.3.2</b> Overplotting</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-viz.html"><a href="2-viz.html#summary"><i class="fa fa-check"></i><b>2.3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-viz.html"><a href="2-viz.html#linegraphs"><i class="fa fa-check"></i><b>2.4</b> 5NG#2: Linegraphs</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-viz.html"><a href="2-viz.html#geomline"><i class="fa fa-check"></i><b>2.4.1</b> Linegraphs via <code>geom_line</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="2-viz.html"><a href="2-viz.html#summary-1"><i class="fa fa-check"></i><b>2.4.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-viz.html"><a href="2-viz.html#histograms"><i class="fa fa-check"></i><b>2.5</b> 5NG#3: Histograms</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="2-viz.html"><a href="2-viz.html#geomhistogram"><i class="fa fa-check"></i><b>2.5.1</b> Histograms via <code>geom_histogram</code></a></li>
<li class="chapter" data-level="2.5.2" data-path="2-viz.html"><a href="2-viz.html#adjustbins"><i class="fa fa-check"></i><b>2.5.2</b> Adjusting the bins</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-viz.html"><a href="2-viz.html#summary-2"><i class="fa fa-check"></i><b>2.5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-viz.html"><a href="2-viz.html#facets"><i class="fa fa-check"></i><b>2.6</b> Facets</a></li>
<li class="chapter" data-level="2.7" data-path="2-viz.html"><a href="2-viz.html#boxplots"><i class="fa fa-check"></i><b>2.7</b> 5NG#4: Boxplots</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="2-viz.html"><a href="2-viz.html#geomboxplot"><i class="fa fa-check"></i><b>2.7.1</b> Boxplots via <code>geom_boxplot</code></a></li>
<li class="chapter" data-level="2.7.2" data-path="2-viz.html"><a href="2-viz.html#summary-3"><i class="fa fa-check"></i><b>2.7.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="2-viz.html"><a href="2-viz.html#geombar"><i class="fa fa-check"></i><b>2.8</b> 5NG#5: Barplots</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="2-viz.html"><a href="2-viz.html#barplots-via-geom_bar-or-geom_col"><i class="fa fa-check"></i><b>2.8.1</b> Barplots via <code>geom_bar</code> or <code>geom_col</code></a></li>
<li class="chapter" data-level="2.8.2" data-path="2-viz.html"><a href="2-viz.html#must-avoid-pie-charts"><i class="fa fa-check"></i><b>2.8.2</b> Must avoid pie charts!</a></li>
<li class="chapter" data-level="2.8.3" data-path="2-viz.html"><a href="2-viz.html#two-categ-barplot"><i class="fa fa-check"></i><b>2.8.3</b> Two categorical variables</a></li>
<li class="chapter" data-level="2.8.4" data-path="2-viz.html"><a href="2-viz.html#summary-4"><i class="fa fa-check"></i><b>2.8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="2-viz.html"><a href="2-viz.html#data-vis-conclusion"><i class="fa fa-check"></i><b>2.9</b> Conclusion</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="2-viz.html"><a href="2-viz.html#summary-table"><i class="fa fa-check"></i><b>2.9.1</b> Summary table</a></li>
<li class="chapter" data-level="2.9.2" data-path="2-viz.html"><a href="2-viz.html#function-argument-specification"><i class="fa fa-check"></i><b>2.9.2</b> Function argument specification</a></li>
<li class="chapter" data-level="2.9.3" data-path="2-viz.html"><a href="2-viz.html#additional-resources-1"><i class="fa fa-check"></i><b>2.9.3</b> Additional resources</a></li>
<li class="chapter" data-level="2.9.4" data-path="2-viz.html"><a href="2-viz.html#whats-to-come-3"><i class="fa fa-check"></i><b>2.9.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-wrangling.html"><a href="3-wrangling.html"><i class="fa fa-check"></i><b>3</b> Data Wrangling</a>
<ul>
<li class="chapter" data-level="" data-path="3-wrangling.html"><a href="3-wrangling.html#wrangling-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="3.1" data-path="3-wrangling.html"><a href="3-wrangling.html#piping"><i class="fa fa-check"></i><b>3.1</b> The pipe operator: <code>|&gt;</code></a></li>
<li class="chapter" data-level="3.2" data-path="3-wrangling.html"><a href="3-wrangling.html#filter"><i class="fa fa-check"></i><b>3.2</b> <code>filter</code> rows</a></li>
<li class="chapter" data-level="3.3" data-path="3-wrangling.html"><a href="3-wrangling.html#summarize"><i class="fa fa-check"></i><b>3.3</b> <code>summarize</code> variables</a></li>
<li class="chapter" data-level="3.4" data-path="3-wrangling.html"><a href="3-wrangling.html#groupby"><i class="fa fa-check"></i><b>3.4</b> <code>group_by</code> rows</a></li>
<li class="chapter" data-level="3.5" data-path="3-wrangling.html"><a href="3-wrangling.html#mutate"><i class="fa fa-check"></i><b>3.5</b> <code>mutate</code> existing variables</a></li>
<li class="chapter" data-level="3.6" data-path="3-wrangling.html"><a href="3-wrangling.html#arrange"><i class="fa fa-check"></i><b>3.6</b> <code>arrange</code> and sort rows</a></li>
<li class="chapter" data-level="3.7" data-path="3-wrangling.html"><a href="3-wrangling.html#joins"><i class="fa fa-check"></i><b>3.7</b> <code>join</code> data frames</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="3-wrangling.html"><a href="3-wrangling.html#matching-key-variable-names"><i class="fa fa-check"></i><b>3.7.1</b> Matching key variable names</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-wrangling.html"><a href="3-wrangling.html#diff-key"><i class="fa fa-check"></i><b>3.7.2</b> Different key variable names</a></li>
<li class="chapter" data-level="3.7.3" data-path="3-wrangling.html"><a href="3-wrangling.html#multiple-key-variables"><i class="fa fa-check"></i><b>3.7.3</b> Multiple key variables</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-wrangling.html"><a href="3-wrangling.html#normal-forms"><i class="fa fa-check"></i><b>3.7.4</b> Normal forms</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-wrangling.html"><a href="3-wrangling.html#other-verbs"><i class="fa fa-check"></i><b>3.8</b> Other verbs</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="3-wrangling.html"><a href="3-wrangling.html#select"><i class="fa fa-check"></i><b>3.8.1</b> <code>select</code> variables</a></li>
<li class="chapter" data-level="3.8.2" data-path="3-wrangling.html"><a href="3-wrangling.html#relocate"><i class="fa fa-check"></i><b>3.8.2</b> <code>relocate</code> variables</a></li>
<li class="chapter" data-level="3.8.3" data-path="3-wrangling.html"><a href="3-wrangling.html#rename"><i class="fa fa-check"></i><b>3.8.3</b> <code>rename</code> variables</a></li>
<li class="chapter" data-level="3.8.4" data-path="3-wrangling.html"><a href="3-wrangling.html#top_n-values-of-a-variable"><i class="fa fa-check"></i><b>3.8.4</b> <code>top_n</code> values of a variable</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3-wrangling.html"><a href="3-wrangling.html#wrangling-conclusion"><i class="fa fa-check"></i><b>3.9</b> Conclusion</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="3-wrangling.html"><a href="3-wrangling.html#summary-table-1"><i class="fa fa-check"></i><b>3.9.1</b> Summary table</a></li>
<li class="chapter" data-level="3.9.2" data-path="3-wrangling.html"><a href="3-wrangling.html#additional-resources-2"><i class="fa fa-check"></i><b>3.9.2</b> Additional resources</a></li>
<li class="chapter" data-level="3.9.3" data-path="3-wrangling.html"><a href="3-wrangling.html#whats-to-come-1"><i class="fa fa-check"></i><b>3.9.3</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-tidy.html"><a href="4-tidy.html"><i class="fa fa-check"></i><b>4</b> Data Importing and Tidy Data</a>
<ul>
<li class="chapter" data-level="" data-path="4-tidy.html"><a href="4-tidy.html#tidy-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="4.1" data-path="4-tidy.html"><a href="4-tidy.html#csv"><i class="fa fa-check"></i><b>4.1</b> Importing data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4-tidy.html"><a href="4-tidy.html#using-the-console"><i class="fa fa-check"></i><b>4.1.1</b> Using the console</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-tidy.html"><a href="4-tidy.html#using-rstudios-interface"><i class="fa fa-check"></i><b>4.1.2</b> Using RStudio’s interface</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-tidy.html"><a href="4-tidy.html#tidy-data-ex"><i class="fa fa-check"></i><b>4.2</b> Tidy data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-tidy.html"><a href="4-tidy.html#tidy-definition"><i class="fa fa-check"></i><b>4.2.1</b> Definition of tidy data</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-tidy.html"><a href="4-tidy.html#converting-to-tidy-data"><i class="fa fa-check"></i><b>4.2.2</b> Converting to tidy data</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-tidy.html"><a href="4-tidy.html#nycflights23-package-1"><i class="fa fa-check"></i><b>4.2.3</b> <code>nycflights23</code> package</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-tidy.html"><a href="4-tidy.html#case-study-tidy"><i class="fa fa-check"></i><b>4.3</b> Case study: democracy in Guatemala</a></li>
<li class="chapter" data-level="4.4" data-path="4-tidy.html"><a href="4-tidy.html#tidyverse-package"><i class="fa fa-check"></i><b>4.4</b> <code>tidyverse</code> package</a></li>
<li class="chapter" data-level="4.5" data-path="4-tidy.html"><a href="4-tidy.html#tidy-data-conclusion"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="4-tidy.html"><a href="4-tidy.html#additional-resources-3"><i class="fa fa-check"></i><b>4.5.1</b> Additional resources</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-tidy.html"><a href="4-tidy.html#whats-to-come-2"><i class="fa fa-check"></i><b>4.5.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Statistical Modeling with moderndive</b></span></li>
<li class="chapter" data-level="5" data-path="5-regression.html"><a href="5-regression.html"><i class="fa fa-check"></i><b>5</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="" data-path="5-regression.html"><a href="5-regression.html#reg-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="5.1" data-path="5-regression.html"><a href="5-regression.html#model1"><i class="fa fa-check"></i><b>5.1</b> One numerical explanatory variable</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="5-regression.html"><a href="5-regression.html#model1EDA"><i class="fa fa-check"></i><b>5.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-regression.html"><a href="5-regression.html#model1table"><i class="fa fa-check"></i><b>5.1.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-regression.html"><a href="5-regression.html#model1points"><i class="fa fa-check"></i><b>5.1.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-regression.html"><a href="5-regression.html#model2"><i class="fa fa-check"></i><b>5.2</b> One categorical explanatory variable</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5-regression.html"><a href="5-regression.html#model2EDA"><i class="fa fa-check"></i><b>5.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-regression.html"><a href="5-regression.html#model2table"><i class="fa fa-check"></i><b>5.2.2</b> Linear regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-regression.html"><a href="5-regression.html#model2points"><i class="fa fa-check"></i><b>5.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-regression.html"><a href="5-regression.html#reg-related-topics"><i class="fa fa-check"></i><b>5.3</b> Related topics</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="5-regression.html"><a href="5-regression.html#correlation-is-not-causation"><i class="fa fa-check"></i><b>5.3.1</b> Correlation is not necessarily causation</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-regression.html"><a href="5-regression.html#leastsquares"><i class="fa fa-check"></i><b>5.3.2</b> Best-fitting line</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-regression.html"><a href="5-regression.html#underthehood"><i class="fa fa-check"></i><b>5.3.3</b> get_regression_x() functions</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-regression.html"><a href="5-regression.html#reg-conclusion"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="5-regression.html"><a href="5-regression.html#additional-resources-basic-regression"><i class="fa fa-check"></i><b>5.4.1</b> Additional resources</a></li>
<li class="chapter" data-level="5.4.2" data-path="5-regression.html"><a href="5-regression.html#whats-to-come-4"><i class="fa fa-check"></i><b>5.4.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#mult-reg-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="6.1" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model4"><i class="fa fa-check"></i><b>6.1</b> One numerical and one categorical explanatory variable</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model4EDA"><i class="fa fa-check"></i><b>6.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model4interactiontable"><i class="fa fa-check"></i><b>6.1.2</b> Model with interactions</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model4table"><i class="fa fa-check"></i><b>6.1.3</b> Model without interactions</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model4points"><i class="fa fa-check"></i><b>6.1.4</b> Observed responses, fitted values, and residuals</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model3"><i class="fa fa-check"></i><b>6.2</b> Two numerical explanatory variables</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model3EDA"><i class="fa fa-check"></i><b>6.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model3table"><i class="fa fa-check"></i><b>6.2.2</b> Multiple regression with two numerical regressors</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#model3points"><i class="fa fa-check"></i><b>6.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#mult-reg-conclusion"><i class="fa fa-check"></i><b>6.3</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#additional-resources-4"><i class="fa fa-check"></i><b>6.3.1</b> Additional resources</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-multiple-regression.html"><a href="6-multiple-regression.html#whats-to-come-5"><i class="fa fa-check"></i><b>6.3.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Statistical Inference with infer</b></span></li>
<li class="chapter" data-level="7" data-path="7-sampling.html"><a href="7-sampling.html"><i class="fa fa-check"></i><b>7</b> Sampling</a>
<ul>
<li class="chapter" data-level="" data-path="7-sampling.html"><a href="7-sampling.html#sampling-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="7.1" data-path="7-sampling.html"><a href="7-sampling.html#sampling-activity"><i class="fa fa-check"></i><b>7.1</b> First activity: red balls</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7-sampling.html"><a href="7-sampling.html#population-proportion"><i class="fa fa-check"></i><b>7.1.1</b> The proportion of red balls in the bowl</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-sampling.html"><a href="7-sampling.html#sampling-manual"><i class="fa fa-check"></i><b>7.1.2</b> Manual sampling</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-sampling.html"><a href="7-sampling.html#sampling-simulation"><i class="fa fa-check"></i><b>7.1.3</b> Virtual sampling</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-sampling.html"><a href="7-sampling.html#sampling-framework"><i class="fa fa-check"></i><b>7.2</b> Sampling framework</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="7-sampling.html"><a href="7-sampling.html#terminology-and-notation"><i class="fa fa-check"></i><b>7.2.1</b> Population, sample, and the sampling distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-sampling.html"><a href="7-sampling.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.3</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="7-sampling.html"><a href="7-sampling.html#random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Random variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-sampling.html"><a href="7-sampling.html#the-sampling-distribution-using-random-variables"><i class="fa fa-check"></i><b>7.3.2</b> The sampling distribution using random variables</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-sampling.html"><a href="7-sampling.html#the-center-of-the-distribution-the-expected-value"><i class="fa fa-check"></i><b>7.3.3</b> The center of the distribution: the expected value</a></li>
<li class="chapter" data-level="7.3.4" data-path="7-sampling.html"><a href="7-sampling.html#sampling-variation"><i class="fa fa-check"></i><b>7.3.4</b> Sampling variation: standard deviation and standard error</a></li>
<li class="chapter" data-level="7.3.5" data-path="7-sampling.html"><a href="7-sampling.html#summary-5"><i class="fa fa-check"></i><b>7.3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-sampling.html"><a href="7-sampling.html#sampling-activity-mean"><i class="fa fa-check"></i><b>7.4</b> Second activity: chocolate-covered almonds</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7-sampling.html"><a href="7-sampling.html#population-mean"><i class="fa fa-check"></i><b>7.4.1</b> The population mean weight of almonds in the bowl</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-sampling.html"><a href="7-sampling.html#resampling-tactile-bowl"><i class="fa fa-check"></i><b>7.4.2</b> Manual sampling and sample means</a></li>
<li class="chapter" data-level="7.4.3" data-path="7-sampling.html"><a href="7-sampling.html#virtual-samples-mean-bowl"><i class="fa fa-check"></i><b>7.4.3</b> Virtual sampling</a></li>
<li class="chapter" data-level="7.4.4" data-path="7-sampling.html"><a href="7-sampling.html#the-sampling-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>7.4.4</b> The sampling distribution of the sample mean</a></li>
<li class="chapter" data-level="7.4.5" data-path="7-sampling.html"><a href="7-sampling.html#random-variable-sample-mean"><i class="fa fa-check"></i><b>7.4.5</b> Random variables</a></li>
<li class="chapter" data-level="7.4.6" data-path="7-sampling.html"><a href="7-sampling.html#CLT-mean"><i class="fa fa-check"></i><b>7.4.6</b> The Central Limit Theorem revisited</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-sampling.html"><a href="7-sampling.html#sampling-other-scenarios"><i class="fa fa-check"></i><b>7.5</b> The sampling distribution in other scenarios</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="7-sampling.html"><a href="7-sampling.html#sampling-distribution-for-two-samples"><i class="fa fa-check"></i><b>7.5.1</b> Sampling distribution for two samples</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7-sampling.html"><a href="7-sampling.html#sampling-final-remarks"><i class="fa fa-check"></i><b>7.6</b> Summary and final remarks</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="7-sampling.html"><a href="7-sampling.html#summary-of-scenarios"><i class="fa fa-check"></i><b>7.6.1</b> Summary of scenarios</a></li>
<li class="chapter" data-level="7.6.2" data-path="7-sampling.html"><a href="7-sampling.html#additional-resources-5"><i class="fa fa-check"></i><b>7.6.2</b> Additional resources</a></li>
<li class="chapter" data-level="7.6.3" data-path="7-sampling.html"><a href="7-sampling.html#whats-to-come-6"><i class="fa fa-check"></i><b>7.6.3</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html"><i class="fa fa-check"></i><b>8</b> Estimation, Confidence Intervals, and Bootstrapping</a>
<ul>
<li class="chapter" data-level="" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#CI-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="8.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#theory-based-CI"><i class="fa fa-check"></i><b>8.1</b> Tying the sampling distribution to estimation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#revisit-almond"><i class="fa fa-check"></i><b>8.1.1</b> Revisiting the almond activity for estimation</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#the-normal-distribution"><i class="fa fa-check"></i><b>8.1.2</b> The normal distribution</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#CI-general"><i class="fa fa-check"></i><b>8.1.3</b> The confidence interval</a></li>
<li class="chapter" data-level="8.1.4" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#t-distribution-CI"><i class="fa fa-check"></i><b>8.1.4</b> The t distribution</a></li>
<li class="chapter" data-level="8.1.5" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#interpreting-confidence-intervals"><i class="fa fa-check"></i><b>8.1.5</b> Interpreting confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#simulation-based-CI"><i class="fa fa-check"></i><b>8.2</b> Estimation with the bootstrap</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#revisit-almond-bootstrap"><i class="fa fa-check"></i><b>8.2.1</b> Bootstrap samples: revisiting the almond activity</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#bootstrap-process"><i class="fa fa-check"></i><b>8.2.2</b> Confidence intervals and the bootstrap: original workflow</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#infer-workflow"><i class="fa fa-check"></i><b>8.2.3</b> The <code>infer</code> package workflow:</a></li>
<li class="chapter" data-level="8.2.4" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#conf-int-infer"><i class="fa fa-check"></i><b>8.2.4</b> Confidence intervals using bootstrap samples with <code>infer</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#boot-remarks"><i class="fa fa-check"></i><b>8.3</b> Additional remarks about the bootstrap</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#the-bootstrap-and-other-resampling-methods"><i class="fa fa-check"></i><b>8.3.1</b> The bootstrap and other resampling methods</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#rate-convergence"><i class="fa fa-check"></i><b>8.3.2</b> Confidence intervals and rate of convergence</a></li>
<li class="chapter" data-level="8.3.3" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#why-bootstrap-methods"><i class="fa fa-check"></i><b>8.3.3</b> Why bootstrap methods</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#case-study-two-prop-ci"><i class="fa fa-check"></i><b>8.4</b> Case study: is yawning contagious?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#mythbusters-study-data"><i class="fa fa-check"></i><b>8.4.1</b> <em>Mythbusters</em> study data</a></li>
<li class="chapter" data-level="8.4.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#sampling-scenario"><i class="fa fa-check"></i><b>8.4.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="8.4.3" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#ci-build"><i class="fa fa-check"></i><b>8.4.3</b> Constructing the confidence interval</a></li>
<li class="chapter" data-level="8.4.4" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#interpreting-the-confidence-interval"><i class="fa fa-check"></i><b>8.4.4</b> Interpreting the confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#summary-CI"><i class="fa fa-check"></i><b>8.5</b> Summary and final remarks</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#additional-resources-6"><i class="fa fa-check"></i><b>8.5.1</b> Additional resources</a></li>
<li class="chapter" data-level="8.5.2" data-path="8-confidence-intervals.html"><a href="8-confidence-intervals.html#whats-to-come-7"><i class="fa fa-check"></i><b>8.5.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#nhst-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="9.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#tying-CI-hypo"><i class="fa fa-check"></i><b>9.1</b> Tying confidence intervals to hypothesis testing</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#one-sample-hyp"><i class="fa fa-check"></i><b>9.1.1</b> The one-sample hypothesis test for the population mean</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>9.1.2</b> Hypothesis tests and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#ht-activity"><i class="fa fa-check"></i><b>9.2</b> Music popularity activity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#is-metal-music-more-popular-than-deep-house-music"><i class="fa fa-check"></i><b>9.2.1</b> Is metal music more popular than deep house music?</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#shuffling-once"><i class="fa fa-check"></i><b>9.2.2</b> Shuffling once</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#ht-what-did-we-just-do"><i class="fa fa-check"></i><b>9.2.3</b> What did we just do?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#understanding-ht"><i class="fa fa-check"></i><b>9.3</b> Understanding hypothesis tests</a></li>
<li class="chapter" data-level="9.4" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#ht-infer"><i class="fa fa-check"></i><b>9.4</b> Conducting hypothesis tests</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#infer-workflow-ht"><i class="fa fa-check"></i><b>9.4.1</b> <code>infer</code> package workflow</a></li>
<li class="chapter" data-level="9.4.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#comparing-infer-workflows"><i class="fa fa-check"></i><b>9.4.2</b> Comparison with confidence intervals</a></li>
<li class="chapter" data-level="9.4.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#only-one-test"><i class="fa fa-check"></i><b>9.4.3</b> There is only one test</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#ht-interpretation"><i class="fa fa-check"></i><b>9.5</b> Interpreting hypothesis tests</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#trial"><i class="fa fa-check"></i><b>9.5.1</b> Two possible outcomes</a></li>
<li class="chapter" data-level="9.5.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#types-of-errors"><i class="fa fa-check"></i><b>9.5.2</b> Types of errors</a></li>
<li class="chapter" data-level="9.5.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#choosing-alpha"><i class="fa fa-check"></i><b>9.5.3</b> How do we choose alpha?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#ht-case-study"><i class="fa fa-check"></i><b>9.6</b> Case study: are action or romance movies rated higher?</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#imdb-data"><i class="fa fa-check"></i><b>9.6.1</b> IMDb ratings data</a></li>
<li class="chapter" data-level="9.6.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#sampling-scenario-1"><i class="fa fa-check"></i><b>9.6.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="9.6.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#conducting-the-hypothesis-test"><i class="fa fa-check"></i><b>9.6.3</b> Conducting the hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#nhst-conclusion"><i class="fa fa-check"></i><b>9.7</b> Summary and Final Remarks</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#theory-hypo"><i class="fa fa-check"></i><b>9.7.1</b> Theory-based approach for two-sample hypothesis tests</a></li>
<li class="chapter" data-level="9.7.2" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#when-inference-is-not-needed"><i class="fa fa-check"></i><b>9.7.2</b> When inference is not needed</a></li>
<li class="chapter" data-level="9.7.3" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#problems-with-p-values"><i class="fa fa-check"></i><b>9.7.3</b> Problems with p-values</a></li>
<li class="chapter" data-level="9.7.4" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#additional-resources-7"><i class="fa fa-check"></i><b>9.7.4</b> Additional resources</a></li>
<li class="chapter" data-level="9.7.5" data-path="9-hypothesis-testing.html"><a href="9-hypothesis-testing.html#whats-to-come-8"><i class="fa fa-check"></i><b>9.7.5</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html"><i class="fa fa-check"></i><b>10</b> Inference for Regression</a>
<ul>
<li class="chapter" data-level="" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#inf-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="10.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#the-simple-linear-regression-model"><i class="fa fa-check"></i><b>10.1</b> The simple linear regression model</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#un-member-states-revisited"><i class="fa fa-check"></i><b>10.1.1</b> UN member states revisited</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#simple-linear-model"><i class="fa fa-check"></i><b>10.1.2</b> The model</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#sample-regression-inference"><i class="fa fa-check"></i><b>10.1.3</b> Using a sample for inference</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#least-squares"><i class="fa fa-check"></i><b>10.1.4</b> The method of least squares</a></li>
<li class="chapter" data-level="10.1.5" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#properties-least-squares"><i class="fa fa-check"></i><b>10.1.5</b> Properties of the least squares estimators</a></li>
<li class="chapter" data-level="10.1.6" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#relating-basic-regression-to-other-methods"><i class="fa fa-check"></i><b>10.1.6</b> Relating basic regression to other methods</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#theory-simple-regression"><i class="fa fa-check"></i><b>10.2</b> Theory-based inference for simple linear regression</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#framework-simple-lm"><i class="fa fa-check"></i><b>10.2.1</b> Conceptual framework</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#se-simple-lm"><i class="fa fa-check"></i><b>10.2.2</b> Standard errors for least-squares estimators</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#conf-intervals-b0-b1"><i class="fa fa-check"></i><b>10.2.3</b> Confidence intervals for the least-squares estimators</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#hypo-test-simple-lm"><i class="fa fa-check"></i><b>10.2.4</b> Hypothesis test for population slope</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#regression-table"><i class="fa fa-check"></i><b>10.2.5</b> The regression table in R</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#model-fit"><i class="fa fa-check"></i><b>10.2.6</b> Model fit and model assumptions</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#infer-regression"><i class="fa fa-check"></i><b>10.3</b> Simulation-based inference for simple linear regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#infer-ci-slr"><i class="fa fa-check"></i><b>10.3.1</b> Confidence intervals for the population slope using <code>infer</code></a></li>
<li class="chapter" data-level="10.3.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#infer-hypo-slr"><i class="fa fa-check"></i><b>10.3.2</b> Hypothesis test for population slope using <code>infer</code></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#the-multiple-linear-regression-model"><i class="fa fa-check"></i><b>10.4</b> The multiple linear regression model</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#multiple-linear-model"><i class="fa fa-check"></i><b>10.4.1</b> The model</a></li>
<li class="chapter" data-level="10.4.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#example-coffee-quality-rating-scores"><i class="fa fa-check"></i><b>10.4.2</b> Example: coffee quality rating scores</a></li>
<li class="chapter" data-level="10.4.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#least-squares-multiple"><i class="fa fa-check"></i><b>10.4.3</b> Least squares for multiple regression</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#theory-multiple-regression"><i class="fa fa-check"></i><b>10.5</b> Theory-based inference for multiple linear regression</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#model-dependency"><i class="fa fa-check"></i><b>10.5.1</b> Model dependency of estimators</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#confidence-intervals-1"><i class="fa fa-check"></i><b>10.5.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="10.5.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#hypo-test-mult-lm"><i class="fa fa-check"></i><b>10.5.3</b> Hypothesis test for a single coefficient</a></li>
<li class="chapter" data-level="10.5.4" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#hypo-test-mult-lm-1"><i class="fa fa-check"></i><b>10.5.4</b> Hypothesis test for model comparison</a></li>
<li class="chapter" data-level="10.5.5" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#model-fit-mult"><i class="fa fa-check"></i><b>10.5.5</b> Model fit and diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#simulation-based-inference-for-multiple-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Simulation-based Inference for multiple linear regression</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#confidence-intervals-for-the-partial-slopes-using-infer"><i class="fa fa-check"></i><b>10.6.1</b> Confidence intervals for the partial slopes using <code>infer</code></a></li>
<li class="chapter" data-level="10.6.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#hypothesis-testing-for-the-partial-slopes-using-infer"><i class="fa fa-check"></i><b>10.6.2</b> Hypothesis testing for the partial slopes using <code>infer</code></a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#inference-conclusion"><i class="fa fa-check"></i><b>10.7</b> Conclusion</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#summary-of-statistical-inference"><i class="fa fa-check"></i><b>10.7.1</b> Summary of statistical inference</a></li>
<li class="chapter" data-level="10.7.2" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#additional-resources-8"><i class="fa fa-check"></i><b>10.7.2</b> Additional resources</a></li>
<li class="chapter" data-level="10.7.3" data-path="10-inference-for-regression.html"><a href="10-inference-for-regression.html#whats-to-come-9"><i class="fa fa-check"></i><b>10.7.3</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Conclusion</b></span></li>
<li class="chapter" data-level="11" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html"><i class="fa fa-check"></i><b>11</b> Tell Your Story with Data</a>
<ul>
<li class="chapter" data-level="11.1" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#review"><i class="fa fa-check"></i><b>11.1</b> Review</a>
<ul>
<li class="chapter" data-level="" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#story-packages"><i class="fa fa-check"></i>Needed packages</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#seattle-house-prices"><i class="fa fa-check"></i><b>11.2</b> Case study: Seattle house prices</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#house-prices-EDA-I"><i class="fa fa-check"></i><b>11.2.1</b> Exploratory data analysis: part I</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#house-prices-EDA-II"><i class="fa fa-check"></i><b>11.2.2</b> Exploratory data analysis: part II</a></li>
<li class="chapter" data-level="11.2.3" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#house-prices-regression"><i class="fa fa-check"></i><b>11.2.3</b> Regression modeling</a></li>
<li class="chapter" data-level="11.2.4" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#house-prices-making-predictions"><i class="fa fa-check"></i><b>11.2.4</b> Making predictions</a></li>
<li class="chapter" data-level="11.2.5" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#house-prices-inference-for-regression"><i class="fa fa-check"></i><b>11.2.5</b> Inference for multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#data-journalism"><i class="fa fa-check"></i><b>11.3</b> Case study: effective data storytelling</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#bechdel-test-for-hollywood-gender-representation"><i class="fa fa-check"></i><b>11.3.1</b> Bechdel test for Hollywood gender representation</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#us-births-in-1999"><i class="fa fa-check"></i><b>11.3.2</b> US Births in 1999</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#scripts-of-r-code"><i class="fa fa-check"></i><b>11.3.3</b> Scripts of R code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="11-thinking-with-data.html"><a href="11-thinking-with-data.html#concluding-remarks"><i class="fa fa-check"></i>Concluding remarks</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-appendixA.html"><a href="A-appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a>
<ul>
<li class="chapter" data-level="A.1" data-path="A-appendixA.html"><a href="A-appendixA.html#appendix-stat-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="A-appendixA.html"><a href="A-appendixA.html#mean"><i class="fa fa-check"></i><b>A.1.1</b> Mean</a></li>
<li class="chapter" data-level="A.1.2" data-path="A-appendixA.html"><a href="A-appendixA.html#median"><i class="fa fa-check"></i><b>A.1.2</b> Median</a></li>
<li class="chapter" data-level="A.1.3" data-path="A-appendixA.html"><a href="A-appendixA.html#appendix-sd-variance"><i class="fa fa-check"></i><b>A.1.3</b> Standard deviation and variance</a></li>
<li class="chapter" data-level="A.1.4" data-path="A-appendixA.html"><a href="A-appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.4</b> Five-number summary</a></li>
<li class="chapter" data-level="A.1.5" data-path="A-appendixA.html"><a href="A-appendixA.html#distribution"><i class="fa fa-check"></i><b>A.1.5</b> Distribution</a></li>
<li class="chapter" data-level="A.1.6" data-path="A-appendixA.html"><a href="A-appendixA.html#outliers"><i class="fa fa-check"></i><b>A.1.6</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-appendixA.html"><a href="A-appendixA.html#appendix-normal-curve"><i class="fa fa-check"></i><b>A.2</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="A-appendixA.html"><a href="A-appendixA.html#additional-normal-calculations"><i class="fa fa-check"></i><b>A.2.1</b> Additional normal calculations</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="A-appendixA.html"><a href="A-appendixA.html#the-t-distribution-calculations"><i class="fa fa-check"></i><b>A.3</b> The t distribution calculations</a></li>
<li class="chapter" data-level="A.4" data-path="A-appendixA.html"><a href="A-appendixA.html#appendix-log10-transformations"><i class="fa fa-check"></i><b>A.4</b> log10 transformations</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-appendixB.html"><a href="B-appendixB.html"><i class="fa fa-check"></i><b>B</b> Inference Examples</a>
<ul>
<li class="chapter" data-level="" data-path="B-appendixB.html"><a href="B-appendixB.html#needed-packages-1"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="B.1" data-path="B-appendixB.html"><a href="B-appendixB.html#inference-mind-map"><i class="fa fa-check"></i><b>B.1</b> Inference mind map</a></li>
<li class="chapter" data-level="B.2" data-path="B-appendixB.html"><a href="B-appendixB.html#one-mean"><i class="fa fa-check"></i><b>B.2</b> One mean</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="B-appendixB.html"><a href="B-appendixB.html#problem-statement"><i class="fa fa-check"></i><b>B.2.1</b> Problem statement</a></li>
<li class="chapter" data-level="B.2.2" data-path="B-appendixB.html"><a href="B-appendixB.html#competing-hypotheses"><i class="fa fa-check"></i><b>B.2.2</b> Competing hypotheses</a></li>
<li class="chapter" data-level="B.2.3" data-path="B-appendixB.html"><a href="B-appendixB.html#exploring-the-sample-data"><i class="fa fa-check"></i><b>B.2.3</b> Exploring the sample data</a></li>
<li class="chapter" data-level="B.2.4" data-path="B-appendixB.html"><a href="B-appendixB.html#non-traditional-methods"><i class="fa fa-check"></i><b>B.2.4</b> Non-traditional methods</a></li>
<li class="chapter" data-level="B.2.5" data-path="B-appendixB.html"><a href="B-appendixB.html#traditional-methods"><i class="fa fa-check"></i><b>B.2.5</b> Traditional methods</a></li>
<li class="chapter" data-level="B.2.6" data-path="B-appendixB.html"><a href="B-appendixB.html#comparing-results"><i class="fa fa-check"></i><b>B.2.6</b> Comparing results</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="B-appendixB.html"><a href="B-appendixB.html#one-proportion"><i class="fa fa-check"></i><b>B.3</b> One proportion</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="B-appendixB.html"><a href="B-appendixB.html#problem-statement-1"><i class="fa fa-check"></i><b>B.3.1</b> Problem statement</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-appendixB.html"><a href="B-appendixB.html#competing-hypotheses-1"><i class="fa fa-check"></i><b>B.3.2</b> Competing hypotheses</a></li>
<li class="chapter" data-level="B.3.3" data-path="B-appendixB.html"><a href="B-appendixB.html#exploring-the-sample-data-1"><i class="fa fa-check"></i><b>B.3.3</b> Exploring the sample data</a></li>
<li class="chapter" data-level="B.3.4" data-path="B-appendixB.html"><a href="B-appendixB.html#non-traditional-methods-1"><i class="fa fa-check"></i><b>B.3.4</b> Non-traditional methods</a></li>
<li class="chapter" data-level="B.3.5" data-path="B-appendixB.html"><a href="B-appendixB.html#traditional-methods-1"><i class="fa fa-check"></i><b>B.3.5</b> Traditional methods</a></li>
<li class="chapter" data-level="B.3.6" data-path="B-appendixB.html"><a href="B-appendixB.html#comparing-results-1"><i class="fa fa-check"></i><b>B.3.6</b> Comparing results</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-appendixB.html"><a href="B-appendixB.html#two-proportions"><i class="fa fa-check"></i><b>B.4</b> Two proportions</a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="B-appendixB.html"><a href="B-appendixB.html#problem-statement-2"><i class="fa fa-check"></i><b>B.4.1</b> Problem statement</a></li>
<li class="chapter" data-level="B.4.2" data-path="B-appendixB.html"><a href="B-appendixB.html#competing-hypotheses-2"><i class="fa fa-check"></i><b>B.4.2</b> Competing hypotheses</a></li>
<li class="chapter" data-level="B.4.3" data-path="B-appendixB.html"><a href="B-appendixB.html#exploring-the-sample-data-2"><i class="fa fa-check"></i><b>B.4.3</b> Exploring the sample data</a></li>
<li class="chapter" data-level="B.4.4" data-path="B-appendixB.html"><a href="B-appendixB.html#non-traditional-methods-2"><i class="fa fa-check"></i><b>B.4.4</b> Non-traditional methods</a></li>
<li class="chapter" data-level="B.4.5" data-path="B-appendixB.html"><a href="B-appendixB.html#traditional-methods-2"><i class="fa fa-check"></i><b>B.4.5</b> Traditional methods</a></li>
<li class="chapter" data-level="B.4.6" data-path="B-appendixB.html"><a href="B-appendixB.html#test-statistic-2"><i class="fa fa-check"></i><b>B.4.6</b> Test statistic</a></li>
<li class="chapter" data-level="B.4.7" data-path="B-appendixB.html"><a href="B-appendixB.html#state-conclusion-2"><i class="fa fa-check"></i><b>B.4.7</b> State conclusion</a></li>
<li class="chapter" data-level="B.4.8" data-path="B-appendixB.html"><a href="B-appendixB.html#comparing-results-2"><i class="fa fa-check"></i><b>B.4.8</b> Comparing results</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="B-appendixB.html"><a href="B-appendixB.html#two-means-independent-samples"><i class="fa fa-check"></i><b>B.5</b> Two means (independent samples)</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="B-appendixB.html"><a href="B-appendixB.html#problem-statement-3"><i class="fa fa-check"></i><b>B.5.1</b> Problem statement</a></li>
<li class="chapter" data-level="B.5.2" data-path="B-appendixB.html"><a href="B-appendixB.html#competing-hypotheses-3"><i class="fa fa-check"></i><b>B.5.2</b> Competing hypotheses</a></li>
<li class="chapter" data-level="B.5.3" data-path="B-appendixB.html"><a href="B-appendixB.html#exploring-the-sample-data-3"><i class="fa fa-check"></i><b>B.5.3</b> Exploring the sample data</a></li>
<li class="chapter" data-level="B.5.4" data-path="B-appendixB.html"><a href="B-appendixB.html#non-traditional-methods-3"><i class="fa fa-check"></i><b>B.5.4</b> Non-traditional methods</a></li>
<li class="chapter" data-level="B.5.5" data-path="B-appendixB.html"><a href="B-appendixB.html#traditional-methods-3"><i class="fa fa-check"></i><b>B.5.5</b> Traditional methods</a></li>
<li class="chapter" data-level="B.5.6" data-path="B-appendixB.html"><a href="B-appendixB.html#test-statistic-3"><i class="fa fa-check"></i><b>B.5.6</b> Test statistic</a></li>
<li class="chapter" data-level="B.5.7" data-path="B-appendixB.html"><a href="B-appendixB.html#compute-p-value-1"><i class="fa fa-check"></i><b>B.5.7</b> Compute <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="B.5.8" data-path="B-appendixB.html"><a href="B-appendixB.html#state-conclusion-3"><i class="fa fa-check"></i><b>B.5.8</b> State conclusion</a></li>
<li class="chapter" data-level="B.5.9" data-path="B-appendixB.html"><a href="B-appendixB.html#comparing-results-3"><i class="fa fa-check"></i><b>B.5.9</b> Comparing results</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="B-appendixB.html"><a href="B-appendixB.html#two-means-paired-samples"><i class="fa fa-check"></i><b>B.6</b> Two means (paired samples)</a>
<ul>
<li class="chapter" data-level="" data-path="B-appendixB.html"><a href="B-appendixB.html#problem-statement-4"><i class="fa fa-check"></i>Problem statement</a></li>
<li class="chapter" data-level="B.6.1" data-path="B-appendixB.html"><a href="B-appendixB.html#competing-hypotheses-4"><i class="fa fa-check"></i><b>B.6.1</b> Competing hypotheses</a></li>
<li class="chapter" data-level="B.6.2" data-path="B-appendixB.html"><a href="B-appendixB.html#exploring-the-sample-data-4"><i class="fa fa-check"></i><b>B.6.2</b> Exploring the sample data</a></li>
<li class="chapter" data-level="B.6.3" data-path="B-appendixB.html"><a href="B-appendixB.html#non-traditional-methods-4"><i class="fa fa-check"></i><b>B.6.3</b> Non-traditional methods</a></li>
<li class="chapter" data-level="B.6.4" data-path="B-appendixB.html"><a href="B-appendixB.html#traditional-methods-4"><i class="fa fa-check"></i><b>B.6.4</b> Traditional methods</a></li>
<li class="chapter" data-level="B.6.5" data-path="B-appendixB.html"><a href="B-appendixB.html#comparing-results-4"><i class="fa fa-check"></i><b>B.6.5</b> Comparing results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-appendixC.html"><a href="C-appendixC.html"><i class="fa fa-check"></i><b>C</b> Tips and Tricks</a>
<ul>
<li class="chapter" data-level="" data-path="C-appendixC.html"><a href="C-appendixC.html#needed-packages-2"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="C.1" data-path="C-appendixC.html"><a href="C-appendixC.html#data-wrangling"><i class="fa fa-check"></i><b>C.1</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-missing-values"><i class="fa fa-check"></i><b>C.1.1</b> Dealing with missing values</a></li>
<li class="chapter" data-level="C.1.2" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-reordering-bars"><i class="fa fa-check"></i><b>C.1.2</b> Reordering bars in a barplot</a></li>
<li class="chapter" data-level="C.1.3" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-money-on-axis"><i class="fa fa-check"></i><b>C.1.3</b> Showing money on an axis</a></li>
<li class="chapter" data-level="C.1.4" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-changing-values"><i class="fa fa-check"></i><b>C.1.4</b> Changing values inside cells</a></li>
<li class="chapter" data-level="C.1.5" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-convert-numerical-categorical"><i class="fa fa-check"></i><b>C.1.5</b> Converting a numerical variable to a categorical one</a></li>
<li class="chapter" data-level="C.1.6" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-prop"><i class="fa fa-check"></i><b>C.1.6</b> Computing proportions</a></li>
<li class="chapter" data-level="C.1.7" data-path="C-appendixC.html"><a href="C-appendixC.html#appendix-commas"><i class="fa fa-check"></i><b>C.1.7</b> Dealing with %, commas, and $</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="C-appendixC.html"><a href="C-appendixC.html#interactive-graphics"><i class="fa fa-check"></i><b>C.2</b> Interactive graphics</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="C-appendixC.html"><a href="C-appendixC.html#interactive-linegraphs"><i class="fa fa-check"></i><b>C.2.1</b> Interactive linegraphs</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="C-appendixC.html"><a href="C-appendixC.html#the-theory-behind-the-bootstrap-method"><i class="fa fa-check"></i><b>C.3</b> The theory behind the bootstrap method</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference via Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<html>
<img src='https://moderndive.com/wide_format.png' alt="ModernDive">
</html>
<div id="confidence-intervals" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Estimation, Confidence Intervals, and Bootstrapping<a href="8-confidence-intervals.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We studied sampling in Chapter <a href="7-sampling.html#sampling">7</a>. Recall, for example, getting many random samples of red and white balls from a bowl, finding the sample proportions of red balls from each of those samples, and studying the distribution of the sample proportions. We can summarize our findings as follows:</p>
<ul>
<li>the sampling distribution of the sample proportion follows, approximately, the normal distribution,</li>
<li>the expected value of the sample proportion, located at the center of the distribution, is exactly equal to the population proportion, and</li>
<li>the sampling variation, measured by the standard error of the sample proportion, is equal to the standard deviation of the population divided by the square root of the sample size used to collect the samples.</li>
</ul>
<p>Similarly, when sampling chocolate-covered almonds and getting the sample mean weight from each sample, the characteristics described above are also encountered in the sampling distribution of the sample mean; namely,</p>
<ul>
<li>the sampling distribution of the sample mean follows, approximately, the normal distribution;</li>
<li>the expected value of the sample mean is the population mean, and</li>
<li>the standard error of the sample mean is the standard deviation of the population divided by the square root of the sample size.</li>
</ul>
<p>Moreover, these characteristics also apply to sampling distributions for the difference of sample means, the difference of sample proportions, and others. Recall that the sampling distribution is not restricted by the distribution of the population. As long as the samples taken are fairly large and we use the appropriate standard error, we can generalize these results appropriately.</p>
<p>The study of the sampling distribution is motivated by another question we have not yet answered: how can we determine the average weight of all the almonds if we do not have access to the entire bowl? We have seen by using simulations in Chapter <a href="7-sampling.html#sampling">7</a> that the average of the sample means, derived from many random samples, will be fairly close to the expected value of the sample mean, which is precisely the population mean weight.</p>
<p>However, in real-life situations, we do not have access to many random samples, only to a single random sample. This chapter introduces methods and techniques that can help us approximate the information of the entire population, such as the population mean weight, by using a single random sample from this population. This undertaking is called <strong>estimation</strong>, and it is central to Statistics and Data Science.</p>
<p>We introduce some statistical jargon about estimation. If we are using a sample statistic to <strong>estimate</strong> a population parameter, e.g., using the sample mean from a random sample to estimate the population mean, we call this statistic a <strong>point estimate</strong> to make emphasis that it is a single value that is used to estimate the parameter of interest.
Now, you may recall that, due to sampling variation, the sample mean typically does not match the population mean exactly, even if the sample is large.
To account for this variation, we use an interval to estimate the parameter instead of a single value, and appropriately call it an <strong>interval estimate</strong> or, if given some level of accuracy, a <strong>confidence interval</strong> of the population parameter. In this chapter, we explain how to find confidence intervals, the advantages of using them, and the different methods that can be used to determine them.</p>
<p>In Section <a href="8-confidence-intervals.html#theory-based-CI">8.1</a> we introduce a method to build a confidence interval for the population mean that uses the random sample taken and theoretical characteristics of the sampling distribution discussed in Chapter <a href="7-sampling.html#sampling">7</a>.
We call this the theory-based approach for constructing intervals.
In Section <a href="8-confidence-intervals.html#simulation-based-CI">8.2</a> we introduce another method, called the bootstrap, that produces confidence intervals by resampling a large number of times from the original sample. Since resampling is done via simulations, we call this the simulation-based approach for constructing confidence intervals. <!-- In Section \@ref(theory-ci) we provide some theoretical foundations to explain the logic behind the bootstrap, introduce some alternatives within this approach, compare it with the theory-based approach, and show the advantages and disadvantages of different approaches. -->
Finally, in Section <a href="8-confidence-intervals.html#summary-CI">8.5</a> we summarize and present extensions of these methods.</p>
<div id="CI-packages" class="section level2 unnumbered hasAnchor">
<h2>Needed packages<a href="8-confidence-intervals.html#CI-packages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If needed, read Section <a href="1-getting-started.html#packages">1.3</a> for information on how to install and load R packages.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="8-confidence-intervals.html#cb285-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb285-2"><a href="8-confidence-intervals.html#cb285-2" tabindex="-1"></a><span class="fu">library</span>(moderndive)</span>
<span id="cb285-3"><a href="8-confidence-intervals.html#cb285-3" tabindex="-1"></a><span class="fu">library</span>(infer)</span></code></pre></div>
<p>Recall that loading the <code>tidyverse</code> package loads many packages that we have encountered earlier. For details refer to Section <a href="4-tidy.html#tidyverse-package">4.4</a>. The packages <code>moderndive</code> and <code>infer</code> contain functions and data frames that will be used in this chapter.</p>
</div>
<div id="theory-based-CI" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Tying the sampling distribution to estimation<a href="8-confidence-intervals.html#theory-based-CI" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section we revisit the chocolate-covered almonds example introduced in Chapter <a href="7-sampling.html#sampling">7</a> and the results from the sampling distribution of the sample mean weight of almonds, but this time we use this information in the context of estimation.</p>
<p>We start by introducing or reviewing some terminology using the almonds example. The bowl of chocolate-covered almonds is the population of interest. The parameter of interest is the <em>population mean</em> weight of almonds in the bowl, <span class="math inline">\(\mu\)</span>. This is the quantity we want to estimate.</p>
<p>We want to use the sample mean to estimate this parameter. So we call the sample mean an <strong>estimator</strong> or an <strong>estimate</strong> of <span class="math inline">\(\mu\)</span>, the population mean weight.
The difference between estimator and estimate is worth discussing.</p>
<p>As an illustration, we decide to take a random sample of 100 almonds from the bowl and use its sample mean weight to estimate the population mean weight. In other words, we intend to sum 100 almonds’ weights, divide this sum by 100, and use this value to estimate the population mean weight. When we refer to the <em>sample mean</em> to describe this process via an equation, the sample mean weight is called an <strong>estimator</strong> of the population mean weight.
Since different samples produce different sample means, the sample mean as an estimator is the random variable <span class="math inline">\(\overline X\)</span> described in Section <a href="7-sampling.html#random-variable-sample-mean">7.4.5</a>. As we have learned studying the sampling distribution in Chapter <a href="7-sampling.html#sampling">7</a>, we know that this <strong>estimator</strong> follows, approximately, a normal distribution; its expected value is equal to the population mean weight. Its standard deviation, also called standard error, is</p>
<p><span class="math display">\[SE(\bar x) = \frac{\sigma}{\sqrt{n}}\]</span></p>
<p>where <span class="math inline">\(n = 100\)</span> in this example and <span class="math inline">\(\sigma\)</span> is the population standard deviation of almonds’ weights.</p>
<p>We took a random sample of 100 almonds’ weights such as the one shown here and stored it in the <code>moderndive</code> package with the same name:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="8-confidence-intervals.html#cb286-1" tabindex="-1"></a>almonds_sample_100</span></code></pre></div>
<pre><code># A tibble: 100 × 3
# Groups:   replicate [1]
   replicate    ID weight
       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
 1         1   166    4.2
 2         1  1215    4.2
 3         1  1899    3.9
 4         1  1912    3.8
 5         1  4637    3.3
 6         1   511    3.5
 7         1   127    4  
 8         1  4419    3.5
 9         1  4729    4.2
10         1  2574    4.1
# ℹ 90 more rows</code></pre>
<p>We can use it to calculate the sample mean weight:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="8-confidence-intervals.html#cb288-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb288-2"><a href="8-confidence-intervals.html#cb288-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">sample_mean =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  replicate sample_mean
      &lt;int&gt;       &lt;dbl&gt;
1         1       3.682</code></pre>
<p>Then <span class="math inline">\(\overline{x} = 3.682\)</span> grams is an <strong>estimate</strong> of the population mean weight.
In summary, the <strong>estimator</strong> is the procedure, equation, or method that will be used on a sample to estimate a parameter before the sample has been retrieved and has many useful properties discussed in Chapter <a href="7-sampling.html#sampling">7</a>. The moment a sample is taken and the equation of a sample mean is applied to this sample, the resulting number is an <strong>estimate</strong>.</p>
<p>The sample mean, as an estimator or estimate of the population mean, will be a central component of the material developed in this chapter.
But, note that it is not the only quantity of interest.
For example, the <em>population standard deviation</em> of the almonds’ weight, denoted by the Greek letter <span class="math inline">\(\sigma\)</span>, is a parameter and the <em>sample standard deviation</em> can be an <strong>estimator</strong> or <strong>estimate</strong> of this parameter.</p>
<p>Furthermore, we have shown in Chapter <a href="7-sampling.html#sampling">7</a> that the expected value of the sample mean is equal to the population mean. When this happens, we call the sample mean an <strong>unbiased</strong> estimator of the population mean. This does not mean that any sample mean will be equal to the population mean; some sample means will be greater while others will be smaller but, on average, they will be equal to the population mean. In general, when the expected value of an estimator is equal to the parameter it is trying to estimate, we call the estimator <strong>unbiased</strong>. If it is not, the estimator is <strong>biased</strong>.</p>
<p>We now revisit the almond activity and study how the sampling distribution of the sample mean can help us build interval estimates for the population mean.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.1)</strong> What is the expected value of the sample mean weight of almonds in a large sample according to the sampling distribution theory?</p>
<ul>
<li>A. It is always larger than the population mean.</li>
<li>B. It is always smaller than the population mean.</li>
<li>C. It is exactly equal to the population mean.</li>
<li>D. It is equal to the population mean on average but may vary in any single sample.</li>
</ul>
<p><strong>(LC8.2)</strong> What is a <strong>point estimate</strong> and how does it differ from an <strong>interval estimate</strong> in the context of statistical estimation?</p>
<ul>
<li>A. A point estimate uses multiple values to estimate a parameter; an interval estimate uses a single value.</li>
<li>B. A point estimate is a single value used to estimate a parameter; an interval estimate provides a range of values within which the parameter likely falls.</li>
<li>C. A point estimate is the mean of multiple samples; an interval estimate is the median.</li>
<li>D. A point estimate and an interval estimate are the same and can be used interchangeably.</li>
</ul>
<div class="learncheck">

</div>
<div id="revisit-almond" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Revisiting the almond activity for estimation<a href="8-confidence-intervals.html#revisit-almond" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <a href="7-sampling.html#sampling">7</a> one of the activities was to take many random samples of size 100 from a bowl of 5,000 chocolate-covered almonds. Since we have access to the contents of the entire bowl, we can compute the population parameters:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="8-confidence-intervals.html#cb290-1" tabindex="-1"></a>almonds_bowl <span class="sc">|&gt;</span> </span>
<span id="cb290-2"><a href="8-confidence-intervals.html#cb290-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">population_mean =</span> <span class="fu">mean</span>(weight), </span>
<span id="cb290-3"><a href="8-confidence-intervals.html#cb290-3" tabindex="-1"></a>            <span class="at">population_sd =</span> <span class="fu">pop_sd</span>(weight))</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  population_mean population_sd
            &lt;dbl&gt;         &lt;dbl&gt;
1         3.64496      0.392070</code></pre>
<p>The total number of almonds in the bowl is 5,000. The population mean is</p>
<p><span class="math display">\[\mu = \sum_{i=1}^{5000}\frac{x_i}{5000}=3.645,\]</span></p>
<p>and the population standard deviation, <code>pop_sd()</code>, from <code>moderndive</code>, is defined as</p>
<p><span class="math display">\[\sigma = \sum_{i=1}^{5000} \frac{(x_i - \mu)^2}{5000}=0.392.\]</span></p>
<p>We keep those numbers for future reference to determine how well our methods of estimation are doing, but recall that in real-life situations we do not have access to the population values and the population mean <span class="math inline">\(\mu\)</span> is unknown. All we have is the information from one random sample. In our example, we assume that all we know is the <code>almonds_sample_100</code> object stored in <code>moderndive</code>.
Its <code>ID</code> variable shows the almond chosen from the bowl and its corresponding <code>weight</code>. Using this sample we calculate some sample statistics:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="8-confidence-intervals.html#cb292-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb292-2"><a href="8-confidence-intervals.html#cb292-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight), </span>
<span id="cb292-3"><a href="8-confidence-intervals.html#cb292-3" tabindex="-1"></a>            <span class="at">sd_weight =</span> <span class="fu">sd</span>(weight), </span>
<span id="cb292-4"><a href="8-confidence-intervals.html#cb292-4" tabindex="-1"></a>            <span class="at">sample_size =</span> <span class="fu">n</span>())</span></code></pre></div>
<pre><code># A tibble: 1 × 4
  replicate mean_weight sd_weight sample_size
      &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;
1         1       3.682  0.362199         100</code></pre>
<p>In one of the activities performed in Chapter <a href="7-sampling.html#sampling">7</a> we took many random samples, calculated their sample means, constructed a histogram using these sample means, and showed how the histogram is a good approximation of the sampling distribution of the sample mean.
We redraw Figure <a href="7-sampling.html#fig:sample-mean-100-with-normal">7.26</a> here as Figure <a href="8-confidence-intervals.html#fig:sample-mean-100-with-normal-redraw">8.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sample-mean-100-with-normal-redraw"></span>
<img src="ModernDive_files/figure-html/sample-mean-100-with-normal-redraw-1.png" alt="The distribution of the sample mean." width="\textwidth" />
<p class="caption">
FIGURE 8.1: The distribution of the sample mean.
</p>
</div>
<p>The histogram in Figure <a href="8-confidence-intervals.html#fig:sample-mean-100-with-normal-redraw">8.1</a> is drawn using many sample mean weights from random samples of size <span class="math inline">\(n=100\)</span>.
The added red smooth curve is the density curve for the normal distribution with the appropriate expected value and standard error calculated from the sample distribution.
The red dot represents the population mean <span class="math inline">\(\mu\)</span>, the unknown parameter we are trying to estimate. The blue right is the sample mean <span class="math inline">\(\overline{x} = 3.682\)</span> from the random sample stored in <code>almonds_sample_100</code>.</p>
<p>In real-life applications, a sample mean is taken from a sample, but the distribution of the population and the population mean are unknown, so the location of the blue dot with respect to the red dot is also unknown. However, if we construct an interval centered on the blue dot, as long as it is wide enough the interval will contain the red dot.
To understand this better, we need to learn a few additional properties of the normal distribution.</p>
</div>
<div id="the-normal-distribution" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> The normal distribution<a href="8-confidence-intervals.html#the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable can take on different values. When those values can be represented by one or more intervals, the likelihood of those values can be expressed graphically by a density curve on a Cartesian coordinate system in two dimensions. The horizontal axis (X-axis) represents the values that the random variable can take and the height of density curve (Y-axis) provides a graphical representation of the likelihood of those values; the higher the curve the more likely those values are. In addition, the total area under a density curve is always equal to 1. The set of values a random variable can take alongside their likelihood is what we call the distribution of a random variable.</p>
<p>The normal distribution is the distribution of a special type of random variable. Its density curve has a distinctive bell shape, and it is fully defined by two values: (1) the mean or expected value of the random variable, <span class="math inline">\(\mu\)</span>, which is located on the X-axis at the center of the density curve (its highest point), and (2) the standard deviation, <span class="math inline">\(\sigma\)</span>, which reflects the dispersion of the random variable; the greater the standard deviation is the wider the curve appears.
In Figure <a href="8-confidence-intervals.html#fig:normal-curves">8.2</a>, we plot the density curves of three random variables, all following normal distributions:</p>
<ol style="list-style-type: decimal">
<li>The solid line represents a normal distribution with <span class="math inline">\(\mu = 5\)</span> &amp; <span class="math inline">\(\sigma = 2\)</span>.</li>
<li>The dotted line represents a normal distribution with <span class="math inline">\(\mu = 5\)</span> &amp; <span class="math inline">\(\sigma = 5\)</span>.</li>
<li>The dashed line represents a normal distribution with <span class="math inline">\(\mu = 15\)</span> &amp; <span class="math inline">\(\sigma = 2\)</span>.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curves"></span>
<img src="ModernDive_files/figure-html/normal-curves-1.png" alt="Three normal distributions." width="\textwidth" />
<p class="caption">
FIGURE 8.2: Three normal distributions.
</p>
</div>
<p>A random variable that follows a normal distribution can take any values in the real line, but those values (on the X-axis) that correspond to the peak of the density curve are more likely than those corresponding to the tails.
The density curve drawn with a solid line has the same mean as the one drawn with a dotted line, <span class="math inline">\(\mu = 5\)</span>, but the former exhibits less dispersion, measured by the standard deviation <span class="math inline">\(\sigma =2\)</span>, than the latter, <span class="math inline">\(\sigma = 5\)</span>. Since the total area under any density curve is equal to 1, the wider curve has to be shorter in height to preserve this property. On the other hand, the density curve drawn with a solid line has the same standard deviation as the one drawn with a dashed line, <span class="math inline">\(\sigma = 2\)</span>, but the latter has a greater mean, <span class="math inline">\(\mu = 15\)</span>, than the former, <span class="math inline">\(\mu = 5\)</span>, so they do look the same but the latter is centered farther to the right on the X-axis than the former.</p>
<div id="the-standard-normal-distribution" class="section level4 unnumbered hasAnchor">
<h4>The standard normal distribution<a href="8-confidence-intervals.html#the-standard-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A special normal distribution has mean <span class="math inline">\(\mu\)</span> = 0 and standard deviation <span class="math inline">\(\sigma\)</span> = 1. It is called the <em>standard normal distribution</em>, and it is represented by a density curve called the <em><span class="math inline">\(z\)</span>-curve</em>. If a random variable <span class="math inline">\(Z\)</span> follows the standard normal distribution, a realization of this random variable is called a standard value or <span class="math inline">\(z\)</span>-value. The <span class="math inline">\(z\)</span>-value also represents the number of standard deviations above the mean, if positive, or below the mean, if negative. For example, if <span class="math inline">\(z=5\)</span>, the value observed represents a realization of the random variable <span class="math inline">\(Z\)</span> that is five standard deviation above the mean, <span class="math inline">\(\mu = 0\)</span>.</p>
</div>
<div id="linear-transformations-of-random-variables-that-follow-the-normal-distribution" class="section level4 unnumbered hasAnchor">
<h4>Linear transformations of random variables that follow the normal distribution<a href="8-confidence-intervals.html#linear-transformations-of-random-variables-that-follow-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A linear transformation of a random variable transforms the original variable into a new random variable by adding, subtracting, multiplying, or dividing constants to the original values. The resulting random variable could have a different mean and standard deviation. The most interesting transformation is turning a random variable into another with <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = 1\)</span>. When this happens we say that the random variable has been standardized.</p>
<p>A property of the normal distribution is that any linear transformation of a random variable that follows the normal distribution results in a new random variable that also follows a normal distribution, potentially with different mean and standard deviation. In particular, we can turn any random variable that follows the normal distribution into a random variable that follows the standard normal distribution. For example, if a value <span class="math inline">\(x = 11\)</span> comes from a normal distribution with mean <span class="math inline">\(\mu =5\)</span> and standard deviation <span class="math inline">\(\sigma = 2\)</span>, the <span class="math inline">\(z\)</span>-value</p>
<p><span class="math display">\[z = \frac{x - \mu}{\sigma} = \frac{11 - 5}{2} = 3\]</span>
is the corresponding value in a standard normal curve. Moreover, we have determined that <span class="math inline">\(x = 11\)</span> for this example is precisely <span class="math inline">\(3\)</span> standard deviations above the mean.</p>
</div>
<div id="finding-probabilies-under-a-density-curve" class="section level4 unnumbered hasAnchor">
<h4>Finding probabilies under a density curve<a href="8-confidence-intervals.html#finding-probabilies-under-a-density-curve" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When a random variable can be represented by a density curve, the probability that the random variable takes a value in any given interval (on the X-axis) is equal to the area under the density curve for that interval. If we know the equation that represents the density curve, we could use the mathematical technique from calculus known as integration to determine this area. In the case of the normal curve, the integral for any interval does not have a close form solution, and the solution is calculated using numerical approximations.</p>
<p>Please review <a href="https://moderndive.com/A-appendixA.html">Appendix A online</a> where we provide R code to work with different areas, probabilities, and values under a normal density curve. Here, we place focus on the insights of specific values and areas without dedicating time to those calculations.</p>
<p>We assume that a random variable <span class="math inline">\(Z\)</span> follows a standard normal distribution. We would like to know how likely it is for this random variable to take a value that is within one standard deviation from the mean. Equivalently, what is the probability that the observed value <span class="math inline">\(z\)</span> (in the X-axis) is between -1 and 1 as shown in Figure <a href="8-confidence-intervals.html#fig:normal-curve-shaded-1a">8.3</a>?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curve-shaded-1a"></span>
<img src="ModernDive_files/figure-html/normal-curve-shaded-1a-1.png" alt="Normal area within one standard deviation." width="\textwidth" />
<p class="caption">
FIGURE 8.3: Normal area within one standard deviation.
</p>
</div>
<p>Calculations show that this area is 0.6827 or about 68.27% of the total area under the curve. This is equivalent to saying that the probability of getting a value between <span class="math inline">\(-1\)</span> and 1 on a standard normal is 68.27%. This also means that if a random variable representing an experiment follows a normal distribution, the probability that the outcome of this experiment is within one standard deviation from the mean is 68.27%. Similarly, the area under the standard normal density curve between -2 and 2 is shown in Figure <a href="8-confidence-intervals.html#fig:normal-curve-shaded-2a">8.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curve-shaded-2a"></span>
<img src="ModernDive_files/figure-html/normal-curve-shaded-2a-1.png" alt="Normal area within two standard deviations." width="\textwidth" />
<p class="caption">
FIGURE 8.4: Normal area within two standard deviations.
</p>
</div>
<p>Calculations show that this area is equal to 0.9545 or 95.45%. If a random variable representing an experiment follows a normal distribution, the probability that the outcome of this experiment is within two standard deviations from the mean is 95.45%. It is also common practice to use the exact number of standard deviations that correspond to an area around the mean exactly equal to 95% (instead of 95.45%).
Please see <a href="https://moderndive.com/A-appendixA.html">Appendix A online</a> to produce these or other calculations in R.
The result is that the area under the density curve around the mean that is exactly equal to 0.95, or 95%, is the area within 1.96 standard deviation from the mean. Remember this number as it will be used a few times in future sections.</p>
<p>In summary, if the possible outcomes of an experiment can be expressed as a random variable that follows the normal distribution, the probability of getting a result that is within one standard deviation from the mean is about 68.27%, within 2 standard deviations form the mean is 95.45%, within 1.96 standard deviations from the mean is 95%, and within 3 standard deviations from the mean is about 99.73%, to name a few.
Spend a few moments grasping this idea; observe, for example, that it is almost impossible to observe an outcome represented by a number that is five standard deviations above the mean as the chances of that happening are near zero. We are now ready to return to the our main goal: how to find an interval estimate of the population mean based on a single sample.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.3)</strong> What does the population mean (<span class="math inline">\(\mu\)</span>) represent in the context of the almond activity?</p>
<ul>
<li>A. The average weight of 100 randomly sampled almonds.</li>
<li>B. The weight of the heaviest almond in the bowl.</li>
<li>C. The average weight of all 5,000 almonds in the bowl.</li>
<li>D. The total weight of all almonds in the bowl.</li>
</ul>
<p><strong>(LC8.4)</strong> Which of the following statements best describes the population standard deviation (<span class="math inline">\(\sigma\)</span>) in the almond activity?</p>
<ul>
<li>A. It measures the average difference between each almond’s weight and the sample mean weight.</li>
<li>B. It measures the average difference between each almond’s weight and the population mean weight.</li>
<li>C. It is equal to the square root of the sample variance.</li>
<li>D. It is always smaller than the population mean.</li>
</ul>
<p><strong>(LC8.5)</strong> Why do we use the sample mean to estimate the population mean in the almond activity?</p>
<ul>
<li>A. Because the sample mean is always larger than the population mean.</li>
<li>B. Because the sample mean is a good estimator of the population mean due to its unbiasedness.</li>
<li>C. Because the sample mean requires less computational effort than the population mean.</li>
<li>D. Because the sample mean eliminates all sampling variation.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="CI-general" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> The confidence interval<a href="8-confidence-intervals.html#CI-general" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue using the example where we try to estimate the population mean weight of almonds with a random sample of 100 almonds. We showed in Chapter <a href="7-sampling.html#sampling">7</a> that the sampling distribution of the sample mean weight of almonds approximates a normal distribution with expected value equal to the population mean weight of almonds and a standard error equal to</p>
<p><span class="math display">\[SE(\bar x) = \frac{\sigma}{\sqrt {100}}.\]</span>
In Subsection <a href="8-confidence-intervals.html#revisit-almond">8.1.1</a> we showed that for the population of almonds, <span class="math inline">\(\mu =3.645\)</span> grams and <span class="math inline">\(\sigma = 0.392\)</span>, so the standard error for the sampling distribution is</p>
<p><span class="math display">\[SE(\bar x) = \frac{\sigma}{\sqrt{100}} = \frac{0.392}{\sqrt{100}} = 0.039\]</span>
grams. In Figure <a href="8-confidence-intervals.html#fig:normal-curve-1">8.5</a> we plot the density curve for this distribution using these values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curve-1"></span>
<img src="ModernDive_files/figure-html/normal-curve-1-1.png" alt="Normal density curve for the sample mean weight of almonds." width="90%" />
<p class="caption">
FIGURE 8.5: Normal density curve for the sample mean weight of almonds.
</p>
</div>
<p>The horizontal axis (X-axis) represents the sample means that we can determine from all the possible random samples of 100 almonds. The red dot represents the expected value of the sampling distribution, <span class="math inline">\(\mu = 3.64\)</span>, located on the X-axis at the center of the distribution. The density curve’s height can be thought of as how likely those sample means are to be observed. For example, it is more likely to get a random sample with a sample mean around <span class="math inline">\(3.645\)</span> grams (which corresponds to the highest point of the curve) than it is to get a sample with a sample mean at around <span class="math inline">\(3.5\)</span> grams, since the curve’s height is almost zero at that value. The blue dot is the sample mean from our sample of 100 almonds, <span class="math inline">\(\overline{x} = 3.682\)</span> grams. It is located 0.037 grams above the population mean weight. How far is 0.037 grams? It is helpful to express this distance in standardized values:</p>
<p><span class="math display">\[\frac{3.682 - 3.645}{0.039} = 0.945\]</span></p>
<p>so 0.037 more grams is about 0.945 standard errors above the population mean.</p>
<p>In real-life situations, the population mean, <span class="math inline">\(\mu\)</span>, is unknown so the distance from the sample mean to <span class="math inline">\(\mu\)</span> is also unknown.
On the other hand, the sampling distribution of the sample mean follows a normal distribution. Based on our earlier discussion about areas under the normal curve, there is a 95% chance that the value observed is within 1.96 standard deviations from the mean. In the context of our problem, there is a 95% chance that the sample mean weight is within 1.96 standard errors from the population mean weight. As shown earlier, the sample mean calculated in our example was 0.945 standard errors above the population mean, well within the reasonable range.</p>
<p>Think about this result. If we were to take a different random sample of 100 almonds, the sample mean will likely be different, but you still have a 95% chance that the new sample mean will be within 1.96 standard errors from the population mean.</p>
<p>We can finally construct an interval estimate that takes advantage of this configuration. We center our interval at the sample mean observed and then extend to each side the magnitude equivalent to 1.96 standard errors. The lower and upper bounds of this interval are:</p>
<p><span class="math display">\[\begin{aligned}\left(\overline{x} - 1.96 \frac{\sigma}{\sqrt{n}},\quad \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}}\right) &amp;= \left(3.682 - 1.96 \cdot \frac{0.392}{\sqrt{100}},\quad 3.682 + 1.96 \cdot \frac{0.392}{\sqrt{100}}\right)\\
&amp;= (3.605, 3.759)\end{aligned}\]</span></p>
<p>Here is R code that can be used to calculate these lower and upper bounds:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="8-confidence-intervals.html#cb294-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb294-2"><a href="8-confidence-intervals.html#cb294-2" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb294-3"><a href="8-confidence-intervals.html#cb294-3" tabindex="-1"></a>    <span class="at">sample_mean =</span> <span class="fu">mean</span>(weight),</span>
<span id="cb294-4"><a href="8-confidence-intervals.html#cb294-4" tabindex="-1"></a>    <span class="at">lower_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> sigma <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">length</span>(weight)),</span>
<span id="cb294-5"><a href="8-confidence-intervals.html#cb294-5" tabindex="-1"></a>    <span class="at">upper_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> sigma <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">length</span>(weight))</span>
<span id="cb294-6"><a href="8-confidence-intervals.html#cb294-6" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code># A tibble: 1 × 4
  replicate sample_mean lower_bound upper_bound
      &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
1         1       3.682     3.60515     3.75885</code></pre>
<p>The functions <code>mean()</code> and <code>length()</code> find the sample mean weight and sample size, respectively, from the sample of almonds’ weights in <code>almonds_sample_100</code>. The number 1.96 corresponds to the number of standard errors needed to get a 95% area under the normal distribution and the population standard deviation <code>sigma</code> of 0.392 was found in Subsection <a href="8-confidence-intervals.html#revisit-almond">8.1.1</a>. Figure <a href="8-confidence-intervals.html#fig:normal-curve-2">8.6</a> shows this interval as a horizontal blue line. Observe how the population mean <span class="math inline">\(\mu\)</span> is part of this interval.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curve-2"></span>
<img src="ModernDive_files/figure-html/normal-curve-2-1.png" alt="Is the population mean in the interval?" width="90%" />
<p class="caption">
FIGURE 8.6: Is the population mean in the interval?
</p>
</div>
<p>Since 1.96 standard errors were used on the construction of this interval, we call this a 95% confidence interval. A confidence interval can be viewed as an interval estimator of the population mean. Compare an interval estimator with the sample mean that is a point estimator. The latter estimates the parameter with a single number, the former provides an entire interval to account for the location of the parameter. An apt analogy involves fishing. Imagine that there is a single fish swimming in murky water. The fish is not visible but its movement produces ripples on the surface that can provide some limited information about the fish’s location. To capture the fish, one could use a spear or a net. Because the information is limited, throwing the spear at the ripples may capture the fish but likely will miss it.</p>
<p>Throwing a net around the ripples, on the other hand, may give a much higher likelihood of capturing the fish. Using the sample mean only to estimate the population mean is like throwing a spear at the ripples in the hopes of capturing the fish. Constructing a confidence interval that may include the population mean is like throwing a net to surround the ripples. Keep this analogy in mind, as we will revisit it in future sections.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.6)</strong> How is the standard error of the sample mean weight of almonds calculated in the context of this example?</p>
<ul>
<li>A. By dividing the sample mean by the population standard deviation.</li>
<li>B. By dividing the population standard deviation by the square root of the sample size.</li>
<li>C. By multiplying the sample mean by the square root of the sample size.</li>
<li>D. By dividing the population mean by the sample size.</li>
</ul>
<p><strong>(LC8.7)</strong> What does a 95% confidence interval represent in the context of the almond weight estimation?</p>
<ul>
<li>A. There is a 95% chance that the sample mean is within 1.96 standard deviations from the population mean.</li>
<li>B. The interval will contain 95% of the almond weights from the sample.</li>
<li>C. There is a 95% chance that the population mean falls within 1.96 standard errors from the sample mean.</li>
<li>D. The sample mean is exactly equal to the population mean 95% of the time.</li>
</ul>
<div class="learncheck">

</div>
</div>
<div id="t-distribution-CI" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> The t distribution<a href="8-confidence-intervals.html#t-distribution-CI" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that due to the Central Limit Theorem, the sampling distribution of the sample mean was approximately normal with mean equal to the population mean <span class="math inline">\(\mu\)</span> and standard deviation given by the standard error <span class="math inline">\(SE(\overline X) = \sigma/\sqrt{n}\)</span>. We can standardize this for any sample mean <span class="math inline">\(\overline{x}\)</span> such that</p>
<p><span class="math display">\[z = \frac{\overline{x} - \mu}{\sigma/\sqrt{n}}\]</span></p>
<p>is the corresponding value of the standard normal distribution.</p>
<p>In the construction of the interval in Figure <a href="8-confidence-intervals.html#fig:normal-curve-2">8.6</a> we have assumed the population standard deviation, <span class="math inline">\(\sigma\)</span>, was known, and therefore we have used it to find the confidence interval. Nevertheless, in real-life applications, the population standard deviation is also unknown.
Instead, we use the sample standard deviation, <span class="math inline">\(s\)</span>, from the sample we have, as an estimator of the population standard deviation <span class="math inline">\(\sigma\)</span>. Our estimated standard error is given by</p>
<p><span class="math display">\[\widehat{SE}(\overline X) = \frac{s}{\sqrt n}.\]</span></p>
<p>When using the sample standard deviation to estimate the standard error, we are introducing additional uncertainty in our model. For example, if we try to standardize this value, we get</p>
<p><span class="math display">\[t = \frac{\overline{x} - \mu}{s/\sqrt{n}}.\]</span></p>
<p>Because we are using the sample standard deviation in this equation and since the sample standard deviation changes from sample to sample, the additional uncertainty makes the values <span class="math inline">\(t\)</span> no longer normal. Instead they follow a new distribution called the <span class="math inline">\(t\)</span> distribution.</p>
<p>The <span class="math inline">\(t\)</span> distribution is similar to the standard normal; its density curve is also bell-shaped, and it is symmetric around zero, but the tails of the <span class="math inline">\(t\)</span> distribution are a little thicker than those of the standard normal.
In addition, the <span class="math inline">\(t\)</span> distribution requires one additional parameter, the degrees of freedom. For sample mean problems, the degrees of freedom needed are exactly <span class="math inline">\(n-1\)</span>, the size of the sample minus one. Figure <a href="8-confidence-intervals.html#fig:t-curve-1">8.7</a> shows the density curves of</p>
<ul>
<li>the standard normal density curve, in black,</li>
<li>a <span class="math inline">\(t\)</span> density curve for a t distribution with 2 degrees of freedom, in dotted blue, and</li>
<li>a <span class="math inline">\(t\)</span> density curve for a t distribution with 10 degrees of freedom, in dashed red.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-curve-1"></span>
<img src="ModernDive_files/figure-html/t-curve-1-1.png" alt="The standard normal and two t-distributions." width="\textwidth" />
<p class="caption">
FIGURE 8.7: The standard normal and two t-distributions.
</p>
</div>
<p>Observe how the <span class="math inline">\(t\)</span> density curve in dashed red (<span class="math inline">\(t\)</span> with 10 degrees of freedom) gets closer to the standard normal density curve, or <span class="math inline">\(z\)</span>-curve, in solid black, than the <span class="math inline">\(t\)</span> curve in dotted blue (<span class="math inline">\(t\)</span> with 2 degrees of freedom). The greater the number of degrees of freedom, the closer the <span class="math inline">\(t\)</span> density curve is from the <span class="math inline">\(z\)</span> curve. This change makes our calculations slightly different.</p>
<p>Please see <a href="https://moderndive.com/A-appendixA.html">Appendix A online</a> for calculations of probabilities for <span class="math inline">\(t\)</span> density curves with different degrees of freedom.
Using that knowledge, the calculation for our specific example shows that 95% of the sample means are within 1.98 standard errors from the population mean weight. The number of standard errors needed is not that different from before, 1.98 versus 1.96, because the degrees of freedom are fairly large.</p>
<p>Using this information, we can construct the 95% confidence interval based entirely on our sample information and using the sample mean and sample standard deviation. We calculate those values again for <code>almonds_sample_100</code>:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="8-confidence-intervals.html#cb296-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb296-2"><a href="8-confidence-intervals.html#cb296-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">sample_mean =</span> <span class="fu">mean</span>(weight), <span class="at">sample_sd =</span> <span class="fu">sd</span>(weight))</span></code></pre></div>
<pre><code># A tibble: 1 × 3
  replicate sample_mean sample_sd
      &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;
1         1       3.682  0.362199</code></pre>
<p>Observe that the sample standard deviation is <span class="math inline">\(s = 0.362\)</span> which is not that different from the population standard deviation of <span class="math inline">\(\sigma = 0.392\)</span>. We again center the confidence interval at the observed sample mean but now extend the interval by 1.98 standard errors to each side. The lower and upper bounds of this confidence interval are:</p>
<p><span class="math display">\[
\begin{aligned}
\left(\overline{x} - 1.98 \frac{s}{\sqrt{n}},\quad \overline{x} + 1.98 \frac{s}{\sqrt{n}}\right) &amp;= \left(3.682 - 1.98 \cdot \frac{0.362}{\sqrt{100}}, 3.682 + 1.98 \cdot \frac{0.362}{\sqrt{100}}\right) \\
&amp;= (3.498, 3.846))
\end{aligned}
\]</span></p>
<p>We can also compute these lower and upper bounds:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="8-confidence-intervals.html#cb298-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb298-2"><a href="8-confidence-intervals.html#cb298-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">sample_mean =</span> <span class="fu">mean</span>(weight), <span class="at">sample_sd =</span> <span class="fu">sd</span>(weight),</span>
<span id="cb298-3"><a href="8-confidence-intervals.html#cb298-3" tabindex="-1"></a>            <span class="at">lower_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">-</span> <span class="fl">1.98</span><span class="sc">*</span><span class="fu">sd</span>(weight)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(weight)),</span>
<span id="cb298-4"><a href="8-confidence-intervals.html#cb298-4" tabindex="-1"></a>            <span class="at">upper_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">+</span> <span class="fl">1.98</span><span class="sc">*</span><span class="fu">sd</span>(weight)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(weight)))</span></code></pre></div>
<pre><code># A tibble: 1 × 5
  replicate sample_mean sample_sd lower_bound upper_bound
      &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
1         1       3.682  0.362199     3.61028     3.75372</code></pre>
<p>The confidence interval computed here, using the sample standard deviation and a <span class="math inline">\(t\)</span> distribution, is almost the same as the one attained using the population standard deviation and the standard normal distribution, the difference is about 0.005 units for the upper and lower bound. This happens because with a sample size of 100, the <span class="math inline">\(t\)</span>-curve and <span class="math inline">\(z\)</span>-curve are almost identical and also because the sample standard deviation was very similar to the population standard deviation. This does not have to be always the case and occasionally we can observe greater differences; but, in general, the results are fairly similar.</p>
<p>More importantly, the confidence interval constructed here contains the population mean of <span class="math inline">\(\mu = 3.645\)</span>, which is the result we needed. Recall that a confidence interval is an interval estimate of the parameter of interest, the population mean weight of almonds.
We can summarize the results so far:</p>
<ul>
<li>If the size used for your random sample is large enough, the sampling distribution of the sample mean follows, approximately, the normal distribution.</li>
<li>Using the sample mean observed and the standard error of the sampling distribution, we can construct 95% confidence intervals for the population mean. The formula for these intervals is given by</li>
</ul>
<p><span class="math display">\[\left(\overline{x} - 1.96 \frac{\sigma}{\sqrt{n}},\quad \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}}\right)\]</span>
where <span class="math inline">\(n\)</span> is the sample size used.</p>
<ul>
<li>When the population standard deviation is unknown (which is almost always the case), the sample standard deviation is used to estimate the standard error. This produces additional variability and the standardized values follow a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. The formula for 95% confidence intervals when the sample size is <span class="math inline">\(n=100\)</span> is given by</li>
</ul>
<p><span class="math display">\[\left(\overline{x} - 1.98 \frac{s}{\sqrt{100}},\quad \overline{x} + 1.98 \frac{s}{\sqrt{100}}\right)\]</span></p>
<ul>
<li>The method to construct 95% confidence intervals guarantees that in the long-run for 95% of the possible samples, the intervals determined will include the population mean. It also guarantees that 5% of the possible samples will lead to intervals that do not include the population mean.</li>
<li>As we have constructed intervals with a 95% level of confidence, we can construct intervals with any level of confidence. The only change in the equations will be the number of standard errors needed.</li>
</ul>
</div>
<div id="interpreting-confidence-intervals" class="section level3 hasAnchor" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Interpreting confidence intervals<a href="8-confidence-intervals.html#interpreting-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have used the sample <code>almonds_sample_100</code>, constructed a 95% confidence interval for the population mean weight of almonds, and showed that the interval contained this population. This result is not surprising as we expect intervals such as this to include the population mean for 95% of the possible random samples. We repeat this interval construction for many random samples. Figure <a href="8-confidence-intervals.html#fig:almond-mean-cis">8.8</a> presents the results for one hundred 95% confidence intervals.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:almond-mean-cis"></span>
<img src="ModernDive_files/figure-html/almond-mean-cis-1.png" alt="One hundred 95% confidence intervals and whether the population mean is captured in each." width="\textwidth" />
<p class="caption">
FIGURE 8.8: One hundred 95% confidence intervals and whether the population mean is captured in each.
</p>
</div>
<p>Note that each interval was built using a different random sample. The red vertical line is drawn at the location of the population mean weight, <span class="math inline">\(\mu = 3.645\)</span>. The horizontal lines represent the one hundred 95% confidence intervals found. The gray confidence intervals cross the red vertical line so they contain the population mean. The black confidence intervals do not.</p>
<p>This result motivates the meaning of a 95% confidence interval: If you could construct intervals using the procedure described earlier for every possible random sample, then 95% of these intervals will include the population mean and 5% of them will not.</p>
<p>Of course, in most situations it would be impractical or impossible to take every possible random sample. Still, for a large number of random samples, this result is approximately correct. In Figure <a href="8-confidence-intervals.html#fig:almond-mean-cis">8.8</a>, for example, 5 out of 100 confidence intervals do not include the population mean, and 95% do. It won’t always match up perfectly like this, but the proportions should match pretty close to the confidence level chosen.</p>
<p>The term “95% confidence” invites us to think we are talking about probabilities or chances. Indeed we are, but in a subtle way. Before a random sample has been procured, there is a 95% chance that when a confidence interval is constructed using the prospective random sample, this interval will contain the population mean. The moment a random sample has been attained, the interval constructed either contains the population mean or it does not; with certainty, there is no longer a chance involved. This is true even if we do not know what the population mean is.
So the 95% confidence refers to the method or process to be used on a prospective sample. We are confident that if we follow the process to construct the interval, 95% of the time the random sample attained will lead us to produce an interval that contains the population mean.</p>
<p>On the other hand, it would be improper to say that… “there is a 95% chance that the confidence interval contains the population mean.” Looking at Figure <a href="8-confidence-intervals.html#fig:almond-mean-cis">8.8</a>, each of the confidence intervals either does or does not contain <span class="math inline">\(\mu\)</span>. Once the confidence interval is determined, either the population mean is included or not.</p>
<p>In the literature, this explanation has been encapsulated in a short-hand version: we are 95% confident that the interval contains the population parameter. For example, in Subsection <a href="8-confidence-intervals.html#t-distribution-CI">8.1.4</a> the 95% confidence interval for the population mean weight of almonds was (3.498, 3.846), and we would say: “We are 95% confident that the population mean weight of almonds is between 3.498 and 3.846 grams”.</p>
<p>It is perfectly acceptable to use the short-hand statement, but always remember that the 95% confidence refers to the process, or method, and can be thought of as a chance or probability only before the random sample has been acquired. To further ensure that the probability-type of language is not misused, quotation marks are sometimes put around “95% confident” to emphasize that it is a short-hand version of the more accurate explanation.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.8)</strong> Why does the <span class="math inline">\(t\)</span> distribution have thicker tails compared to the standard normal distribution?</p>
<ul>
<li>A. Because the sample mean is considered more likely to match the population mean closely.</li>
<li>B. Because the <span class="math inline">\(t\)</span> distribution is designed to work when the data does not follow a normal distribution.</li>
<li>C. Because it assumes that the sample size is always smaller when applying the <span class="math inline">\(t\)</span> distribution.</li>
<li>D. Because it accounts for the extra uncertainty that comes from using the sample standard deviation instead of the population standard deviation.</li>
</ul>
<p><strong>(LC8.9)</strong> What is the effect of increasing the degrees of freedom on the <span class="math inline">\(t\)</span> distribution?</p>
<ul>
<li>A. The tails of the distribution become thicker.</li>
<li>B. The tails of the distribution becomes thinner.</li>
<li>C. The distribution does not change with degrees of freedom.</li>
<li>D. The distribution becomes skewed to the right.</li>
</ul>
<div class="learncheck">

</div>
<div id="ci-width" class="section level4 unnumbered hasAnchor">
<h4>Understanding the width of a confidence interval<a href="8-confidence-intervals.html#ci-width" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A confidence interval is an estimator of a population parameter. In the case of the almonds’ bowl we constructed a confidence interval for the population mean. The equation to construct a 95% confidence interval was</p>
<p><span class="math display">\[\left(\overline{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}}\right)\]</span>
Observe that the confidence interval is centered at the sample mean and it extends to each side 1.96 standard errors <span class="math inline">\(1.96\cdot \sigma / \sqrt{n}\)</span>. This quantity is exactly half the width of your confidence interval, and it is called the <strong>margin of error</strong>. The value of the population standard deviation, <span class="math inline">\(\sigma\)</span>, is beyond our control, as it is determined by the distribution of the experiment or phenomenon studied. The sample mean, <span class="math inline">\(\overline{x}\)</span>, is a result that depends on your random sample exclusively. On the other hand, the number 1.96 and the sample size, <span class="math inline">\(n\)</span>, are values that can be changed by the researcher or practitioner. They play an important role on the width of the confidence interval. We study each of them separately.</p>
<div id="the-confidence-level" class="section level5 unnumbered hasAnchor">
<h5>The confidence level<a href="8-confidence-intervals.html#the-confidence-level" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We mentioned earlier that the number 1.96 relates to a 95% confidence process but we did not show how to determine this value. The level of confidence is a decision of the practitioner. If you want to be more confident, say 98% or 99% confident, you just need to adjust the appropriate number of standard errors needed. We show how to determine this number, and use Figure <a href="8-confidence-intervals.html#fig:normal-curve-shaded-3a">8.9</a> to illustrate this process.</p>
<ul>
<li>If the confidence level is 0.95 (or 95%), the area in the middle of the standard normal distribution is 0.95. This area is shaded in Figure <a href="8-confidence-intervals.html#fig:normal-curve-shaded-3a">8.9</a>.</li>
<li>We construct <span class="math inline">\(\alpha = 1 - \text{confidence level} = 1 - 0.95 = 0.05\)</span>. Think of <span class="math inline">\(\alpha\)</span> as the total area on both tails.</li>
<li>Since the normal distribution is symmetric, the area on each tail is <span class="math inline">\(\alpha/2 = 0.05/2 = 0.025\)</span>.</li>
<li>We need the exact number of standard deviations that produces the shaded area. Since the center of a standard normal density curve is zero, as shown in Figure <a href="8-confidence-intervals.html#fig:normal-curve-shaded-3a">8.9</a>, and the normal curve is symmetric, the number of standard deviations can be represented by <span class="math inline">\(-q\)</span> and <span class="math inline">\(q\)</span>, the same magnitude but one positive and the other negative.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normal-curve-shaded-3a"></span>
<img src="ModernDive_files/figure-html/normal-curve-shaded-3a-1.png" alt="Normal curve with the shaded middle area being 0.95" width="\textwidth" />
<p class="caption">
FIGURE 8.9: Normal curve with the shaded middle area being 0.95
</p>
</div>
<p>In R, the function <code>qnorm()</code> finds the value of <span class="math inline">\(q\)</span> when the area under this curve to the left of this value <span class="math inline">\(q\)</span> is given. In our example the area to the left of <span class="math inline">\(-q\)</span> is <span class="math inline">\(\alpha/2 = 0.05/2 = 0.025\)</span>, so</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="8-confidence-intervals.html#cb300-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.025</span>)</span></code></pre></div>
<pre><code>[1] -1.96</code></pre>
<p>or 1.96 standard deviation below the mean. Similarly, the total area under the curve to the left of <span class="math inline">\(q\)</span> is the total shaded area, 0.95, plus the the small white area on the left tail, <span class="math inline">\(0.025\)</span>, and <span class="math inline">\(0.95 + 0.025 = 0.975\)</span>, so</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="8-confidence-intervals.html#cb302-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span></code></pre></div>
<pre><code>[1] 1.96</code></pre>
<p>That is the reason we use 1.96 standard deviation when calculating 95% confidence intervals. What if we want to retrieve a 90% confidence interval? We follow the same procedure:</p>
<ul>
<li>The confidence level is 0.90.</li>
<li><span class="math inline">\(\alpha = 1 - \text{confidence level} = 1 - 0.90 = 0.05\)</span>.</li>
<li>The area on each tail is <span class="math inline">\(\alpha/2 = 0.10/2 = 0.05\)</span>.</li>
<li>The area needed to find <span class="math inline">\(q\)</span> is <span class="math inline">\(0.90+0.05 = 0.95\)</span>.</li>
</ul>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="8-confidence-intervals.html#cb304-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>[1] 1.65</code></pre>
<p>If we want to determine a 90% confidence interval, we need to use 1.645 standard errors in our calculations. We can update the R code to calculate the lower and upper bounds of a 90% confidence interval:</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="8-confidence-intervals.html#cb306-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb306-2"><a href="8-confidence-intervals.html#cb306-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">sample_mean =</span> <span class="fu">mean</span>(weight),</span>
<span id="cb306-3"><a href="8-confidence-intervals.html#cb306-3" tabindex="-1"></a>            <span class="at">lower_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>)<span class="sc">*</span>sigma<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(weight)),</span>
<span id="cb306-4"><a href="8-confidence-intervals.html#cb306-4" tabindex="-1"></a>            <span class="at">upper_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>)<span class="sc">*</span>sigma<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(weight)))</span></code></pre></div>
<pre><code># A tibble: 1 × 4
  replicate sample_mean lower_bound upper_bound
      &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
1         1       3.682     3.61751     3.74649</code></pre>
<p>Let’s do one more. If we want an 80% confidence interval, <span class="math inline">\(1 - 0.8 = 0.2\)</span>, <span class="math inline">\(0.2/2 = 0.1\)</span>, and <span class="math inline">\(0.8+0.1 = 0.9\)</span>, so</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="8-confidence-intervals.html#cb308-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.9</span>)</span></code></pre></div>
<pre><code>[1] 1.28</code></pre>
<p>When you want to calculate an 80%, 90%, or 95% confidence interval, you need to construct your interval using 1.282, 1.645, or 1.96 standard errors, respectively. The more confident you want to be, the larger the number of standard errors you need to use, and the wider your confidence interval becomes. But a confidence interval is an estimator of the population mean, the narrower it is, the more useful it is for practical reasons. So there is a trade-off between the width of a confidence interval and the confidence you want to have.</p>
</div>
<div id="the-sample-size" class="section level5 unnumbered hasAnchor">
<h5>The sample size<a href="8-confidence-intervals.html#the-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>As we studied changes to the confidence level, we can determine how big is the random sample used. The margin of error for a 95% confidence interval is</p>
<p><span class="math display">\[1.96\cdot \frac{\sigma}{\sqrt{n}}.\]</span></p>
<p>If the sample size increases, the margin of error decreases proportional to the square root of the sample size. For example, if we secure a random sample of size 25, <span class="math inline">\(1/\sqrt{25} = 0.2\)</span>, and if we draw a sample of size 100, <span class="math inline">\(1/\sqrt{100} = 0.1\)</span>. By choosing a larger sample size, four times larger, we produce a confidence interval that is half the width. This result is worth considering.</p>
<p>A confidence interval is an estimator of the parameter of interest, such as the population mean weight of almonds. Ideally we would like to build a confidence interval with a high level of confidence, for example 95% confidence. But we also want an interval that is narrow enough to provide useful information. For example, assume we get the following 95% confidence intervals for the population mean weight of almonds:</p>
<ul>
<li>between 2 and 4 grams, or</li>
<li>between 3.51 and 3.64 grams, or</li>
<li>between 3.539 and 3.545 grams.</li>
</ul>
<p>The first interval does not seem useful at all, the second works better, and the third is tremendously accurate, as we are 95% confident that the population mean is within 0.006 grams. Obviously, we always prefer narrower intervals, but there are trade-offs we need to consider. We always prefer high levels of confidence, but the more confident we want to be the wider the interval will be. In addition, the larger the random sample used, the narrower the confidence interval will be. Using a large sample is always a preferred choice, but the trade-offs are often external; collecting large samples could be expensive and time-consuming. The construction of confidence intervals needs to take into account all these considerations.</p>
<p>We have concluded the theory-based approach to construct confidence intervals. In the next section we explore a completely different approach to construct confidence intervals and in later sections we will make comparisons of these methods.</p>
</div>
</div>
</div>
</div>
<div id="simulation-based-CI" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Estimation with the bootstrap<a href="8-confidence-intervals.html#simulation-based-CI" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In 1979, Brad Efron published an article introducing a method called the bootstrap<span class="citation">(<a href="#ref-Efron1979">Efron 1979</a>)</span> that is next summarized. A random sample of size <span class="math inline">\(n\)</span> is taken from the population.
This sample is used to find another sample, with replacement, also of size <span class="math inline">\(n\)</span>. This is called <em>resampling with replacement</em> and the resulting sample is called a <em>bootstrap sample</em>. For example, if the original sample is <span class="math inline">\(\{4,2,5,4,1,3,7,4,6,1\}\)</span>, one particular bootstrap sample could be <span class="math inline">\(\{6, 4, 7, 4, 2, 7, 2, 5, 4, 1\}.\)</span>
Observe that the number 7 appears once in the original sample, but twice in the bootstrap sample;
similarly, the number 3 in the original sample does not appear in the bootstrap sample. This is not uncommon for a bootstrap sample, some of the numbers in the original sample are repeated and others are not included.</p>
<p>The basic idea of the bootstrap is to gain a large number of bootstrap samples, all drawn from the same original sample. Then, we use all these bootstrap samples to find estimates of population parameters, standard errors, or even the density curve of the population. Using them we can construct confidence intervals, perform hypothesis testing, and other inferential methods.</p>
<p>This method takes advantage of the large number of bootstrap samples that can be determined. In several respects, this exercise is not different from the sampling distribution explained in Chapter <a href="7-sampling.html#sampling">7</a>. The only difference, albeit an important one, is that we are not sampling from the population, we are sampling from the original sample.
How many different bootstrap samples could we get from a single sample? A very large number, actually. If the original sample has 10 numbers, as the one shown above, each possible bootstrap sample of size 10 is determined by sampling 10 times with replacement, so the total number of bootstrap samples is <span class="math inline">\(10^{10}\)</span> or 10 billion different bootstrap samples. If the original sample has 20 numbers, the number of bootstrap samples is <span class="math inline">\(20^{20}\)</span>, a number greater than the total number of stars in the universe.
Even with modern powerful computers, it would be an onerous task to calculate every possible bootstrap sample. Instead, a thousand or so bootstrap samples are retrieved, similar to the simulations performed in Chapter <a href="7-sampling.html#sampling">7</a>, and this number is often large enough to provide useful results.</p>
<p>Since Efron <span class="citation">(<a href="#ref-Efron1979">Efron 1979</a>)</span> proposed the bootstrap, the statistical community embraced this method. During the 1980s and 1990s, many theoretical and empirical results were presented showing the strength of bootstrap methods. As an illustration, Efron <span class="citation">(<a href="#ref-Efron1979">Efron 1979</a>)</span>, Hall <span class="citation">(<a href="#ref-Hall1986">Hall 1986</a>)</span>, Efron and Tibshirani<span class="citation">(<a href="#ref-EfronTibshi1986">Efron and Tibshirani 1986</a>)</span>, and Hall <span class="citation">(<a href="#ref-Hall1988">Hall 1988</a>)</span> showed that bootstrapping was at least as good if not better than existent methods, when the goal was to estimate the standard error of an estimator or find the confidence intervals of a parameter. Modifications were proposed to improve the algorithm in situations where the basic method was not producing accurate results. With the continuous improvement of computing power and speed, and the advantages of having ready to use statistical software for its implementation, the use of the bootstrap has become more and more popular in many fields.</p>
<p>As an illustration, if we are interested in the mean of the population, <span class="math inline">\(\mu\)</span>, and we have collected one random sample, we can gain a large number of bootstrap samples from this original sample, use them to calculate sample means, order the sample means from smallest to largest, and choose the interval that contains the middle 95% of these sample means. This will be the simplest way to find a confidence interval based on the bootstrap. In the next few subsections, we explore how to incorporate this and similar methods to construct confidence intervals.</p>
<div id="revisit-almond-bootstrap" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Bootstrap samples: revisiting the almond activity<a href="8-confidence-intervals.html#revisit-almond-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To study and understand the behavior of bootstrap samples, we return to our example of the chocolate-covered almonds in a bowl. Recall that the bowl is considered the population of almonds, and we are interested in estimating the population mean weight of almonds.</p>
<p>As we did before, we only have access to a single random sample. In this section, we use the data frame <code>almonds_sample_100</code>, a random sample of 100 almonds taken earlier. We call this the original sample, and it is used in this section to create the bootstrap samples.
The first 10 rows are shown below:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="8-confidence-intervals.html#cb310-1" tabindex="-1"></a>almonds_sample_100</span></code></pre></div>
<pre><code># A tibble: 100 × 3
# Groups:   replicate [1]
   replicate    ID weight
       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
 1         1   166    4.2
 2         1  1215    4.2
 3         1  1899    3.9
 4         1  1912    3.8
 5         1  4637    3.3
 6         1   511    3.5
 7         1   127    4  
 8         1  4419    3.5
 9         1  4729    4.2
10         1  2574    4.1
# ℹ 90 more rows</code></pre>
<div id="constructing-a-bootstrap-sample-resampling-once" class="section level4 unnumbered hasAnchor">
<h4>Constructing a bootstrap sample: resampling once<a href="8-confidence-intervals.html#constructing-a-bootstrap-sample-resampling-once" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We start by constructing one bootstrap sample of 100 almonds from the original sample of 100 almonds. These are the steps needed to perform this task manually:</p>
<p><strong>Step 1</strong>: Place the original sample of 100 almonds into a bag or hat.</p>
<p><strong>Step 2</strong>: Mix the bag contents, draw one almond, weigh it, and record the weight as seen in Figure <a href="8-confidence-intervals.html#fig:tactile-resampling-2">8.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tactile-resampling-2"></span>
<img src="images/sampling/almonds/one-almond.png" alt="Step 2: Weighing one almond at random." width="30%" />
<p class="caption">
FIGURE 8.10: Step 2: Weighing one almond at random.
</p>
</div>
<p><strong>Step 3</strong>: Put the almond back into the bag! In other words, replace it as seen in Figure <a href="8-confidence-intervals.html#fig:tactile-resampling-4">8.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tactile-resampling-4"></span>
<img src="images/sampling/pennies/tactile_simulation/4_put_it_back.png" alt="Step 3: Replacing almond." width="50%" />
<p class="caption">
FIGURE 8.11: Step 3: Replacing almond.
</p>
</div>
<p><strong>Step 4</strong>: Repeat Steps 2 and 3 a total of 99 more times, resulting in 100 weights.</p>
<p>These steps describe <em>resampling with replacement</em>, and the resulting sample is called a <em>bootstrap sample</em>. This procedure results in some almonds being chosen more than once and other almonds not being chosen at all. Resampling with replacement induces <em>sampling variation</em>, so every bootstrap sample can be different than any other.</p>
<p>This activity can be performed manually following the steps described above. We can also take advantage of the R code we have introduced in Chapter <a href="7-sampling.html#sampling">7</a> and do this virtually.
The data frame <code>almonds_sample_100</code> contains the random sample of almonds taken from the population. We show selected rows from this sample.</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="8-confidence-intervals.html#cb312-1" tabindex="-1"></a>almonds_sample_100 <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb312-2"><a href="8-confidence-intervals.html#cb312-2" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">|&gt;</span> </span>
<span id="cb312-3"><a href="8-confidence-intervals.html#cb312-3" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>replicate)</span>
<span id="cb312-4"><a href="8-confidence-intervals.html#cb312-4" tabindex="-1"></a>almonds_sample_100</span></code></pre></div>
<pre><code># A tibble: 100 × 2
      ID weight
   &lt;int&gt;  &lt;dbl&gt;
 1   166    4.2
 2  1215    4.2
 3  1899    3.9
 4  1912    3.8
 5  4637    3.3
 6   511    3.5
 7   127    4  
 8  4419    3.5
 9  4729    4.2
10  2574    4.1
# ℹ 90 more rows</code></pre>
<p>We use <code>ungroup()</code> and <code>select</code> to eliminate the variable <code>replicate</code> from the <code>almonds_sample_100</code> as this variable may create clutter when resampling. We can now create a bootstrap sample also of size 100 by resampling with replacement once.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="8-confidence-intervals.html#cb314-1" tabindex="-1"></a>boot_sample <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb314-2"><a href="8-confidence-intervals.html#cb314-2" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">1</span>)</span></code></pre></div>
<p>We have used this type of R syntax many times in Chapter <a href="7-sampling.html#sampling">7</a>.
We first select the data frame <code>almonds_sample_100</code> that contains the almonds’ weights in the original sample.
We then perform resampling with replacement once: we resample by using <code>rep_sample_n()</code>, a sample of size 100 by setting <code>size = 100</code>, with replacement by adding the argument <code>replace = TRUE</code>, and one time by setting <code>reps = 1</code>.
The object <code>boot_sample</code> is a bootstrap sample of 100 almonds’ weights gained from the original sample of 100 almonds’ weights. We show the first ten rows of <code>boot_sample</code>:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="8-confidence-intervals.html#cb315-1" tabindex="-1"></a>boot_sample</span></code></pre></div>
<pre><code># A tibble: 100 × 3
# Groups:   replicate [1]
   replicate    ID weight
       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
 1         1  2105    3.1
 2         1  4529    3.8
 3         1  1146    4.2
 4         1  2993    3.2
 5         1  1535    3.2
 6         1  2294    3.7
 7         1   438    3.8
 8         1  4419    3.5
 9         1  1674    3.5
10         1  1146    4.2
# ℹ 90 more rows</code></pre>
<p>We can also study some of the characteristics of this bootstrap sample, such as its sample mean:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="8-confidence-intervals.html#cb317-1" tabindex="-1"></a>boot_sample <span class="sc">|&gt;</span> </span>
<span id="cb317-2"><a href="8-confidence-intervals.html#cb317-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  replicate mean_weight
      &lt;int&gt;       &lt;dbl&gt;
1         1       3.702</code></pre>
<p>By using <code>summarize()</code> and <code>mean()</code> on the bootstrap sample <code>boot_sample</code>, we determine that the mean weight is 3.702 grams. Recall that the sample mean of the original sample was found in the previous subsection as 3.682. So, the sample mean of the bootstrap sample is different than the sample mean of the original sample. This variation is induced by resampling with replacement, the method for finding the bootstrap sample. We can also compare the histogram of <code>weight</code>s for the bootstrap sample with the histogram of <code>weight</code>s for the original sample.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="8-confidence-intervals.html#cb319-1" tabindex="-1"></a><span class="fu">ggplot</span>(boot_sample, <span class="fu">aes</span>(<span class="at">x =</span> weight)) <span class="sc">+</span></span>
<span id="cb319-2"><a href="8-confidence-intervals.html#cb319-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.1</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb319-3"><a href="8-confidence-intervals.html#cb319-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Resample of 100 weights&quot;</span>)</span>
<span id="cb319-4"><a href="8-confidence-intervals.html#cb319-4" tabindex="-1"></a><span class="fu">ggplot</span>(almonds_sample_100, <span class="fu">aes</span>(<span class="at">x =</span> weight)) <span class="sc">+</span></span>
<span id="cb319-5"><a href="8-confidence-intervals.html#cb319-5" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.1</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb319-6"><a href="8-confidence-intervals.html#cb319-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Original sample of 100 weights&quot;</span>)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:origandresample"></span>
<img src="ModernDive_files/figure-html/origandresample-1.png" alt="Comparing weight in the resampled boot_sample with the original sample almonds_sample_100." width="\textwidth" />
<p class="caption">
FIGURE 8.12: Comparing <code>weight</code> in the resampled <code>boot_sample</code> with the original sample <code>almonds_sample_100</code>.
</p>
</div>
<p>Observe in Figure <a href="8-confidence-intervals.html#fig:origandresample">8.12</a> that while the general shapes of both distributions of <code>weight</code>s are roughly similar, they are not identical.
This is the typical behavior of bootstrap samples. They are samples that have been determined from the original sample, but because replacement is used before each new observation is attained, some values often appear more than once while others often do not appear at all.</p>
</div>
<div id="replicates" class="section level4 unnumbered hasAnchor">
<h4>Many bootstrap samples: resampling multiple times<a href="8-confidence-intervals.html#replicates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this subsection we take full advantage of resampling with replacement by taking many bootstrap samples and study relevant information, such as the variability of their sample means. We can start by using the R syntax we used before, this time for 35 replications.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="8-confidence-intervals.html#cb320-1" tabindex="-1"></a>bootstrap_samples_35 <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb320-2"><a href="8-confidence-intervals.html#cb320-2" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">35</span>)</span>
<span id="cb320-3"><a href="8-confidence-intervals.html#cb320-3" tabindex="-1"></a>bootstrap_samples_35</span></code></pre></div>
<pre><code># A tibble: 3,500 × 3
# Groups:   replicate [35]
   replicate    ID weight
       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
 1         1  1459    3.6
 2         1  2972    3.4
 3         1  1215    4.2
 4         1  1381    3.4
 5         1  1264    3.5
 6         1   199    3.4
 7         1   476    3.8
 8         1  4806    3.7
 9         1  3169    4.1
10         1  2265    3.4
# ℹ 3,490 more rows</code></pre>
<p>The syntax is the same as before, but this time we set <code>reps =</code> 35 to get 35 bootstrap samples.
The resulting data frame, <code>bootstrap_samples</code>, has 35 <span class="math inline">\(\cdot\)</span> 100 = 3500 rows corresponding to 35 resamples of 100 almonds’ weights. Let’s now compute the resulting 35 sample means using the same <code>dplyr</code> code as we did in the previous section:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="8-confidence-intervals.html#cb322-1" tabindex="-1"></a>boot_means <span class="ot">&lt;-</span> bootstrap_samples_35 <span class="sc">|&gt;</span> </span>
<span id="cb322-2"><a href="8-confidence-intervals.html#cb322-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight))</span>
<span id="cb322-3"><a href="8-confidence-intervals.html#cb322-3" tabindex="-1"></a>boot_means</span></code></pre></div>
<pre><code># A tibble: 35 × 2
   replicate mean_weight
       &lt;int&gt;       &lt;dbl&gt;
 1         1       3.68 
 2         2       3.688
 3         3       3.632
 4         4       3.68 
 5         5       3.679
 6         6       3.675
 7         7       3.678
 8         8       3.706
 9         9       3.643
10        10       3.68 
# ℹ 25 more rows</code></pre>
<p>Observe that <code>boot_means</code> has 35 rows, corresponding to the 35 bootstrap sample means. Furthermore, observe that the values of <code>mean_weight</code> vary as shown in Figure <a href="8-confidence-intervals.html#fig:resampling-35">8.13</a>.</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="8-confidence-intervals.html#cb324-1" tabindex="-1"></a><span class="fu">ggplot</span>(boot_means, <span class="fu">aes</span>(<span class="at">x =</span> mean_weight)) <span class="sc">+</span></span>
<span id="cb324-2"><a href="8-confidence-intervals.html#cb324-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.01</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb324-3"><a href="8-confidence-intervals.html#cb324-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;sample mean weight in grams&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resampling-35"></span>
<img src="ModernDive_files/figure-html/resampling-35-1.png" alt="Distribution of 35 sample means from 35 bootrap samples." width="\textwidth" />
<p class="caption">
FIGURE 8.13: Distribution of 35 sample means from 35 bootrap samples.
</p>
</div>
<p>This histogram highlights the variation of the sample mean weights. Since we have only used 35 bootstrap samples, the histogram looks a little coarse.
To improve our perception of this variation, we find 1000 bootstrap samples and their sample means:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="8-confidence-intervals.html#cb325-1" tabindex="-1"></a><span class="co"># Retrieve 1000 bootstrap samples</span></span>
<span id="cb325-2"><a href="8-confidence-intervals.html#cb325-2" tabindex="-1"></a>bootstrap_samples <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb325-3"><a href="8-confidence-intervals.html#cb325-3" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">1000</span>)</span>
<span id="cb325-4"><a href="8-confidence-intervals.html#cb325-4" tabindex="-1"></a></span>
<span id="cb325-5"><a href="8-confidence-intervals.html#cb325-5" tabindex="-1"></a><span class="co"># Compute sample means from the bootstrap samples</span></span>
<span id="cb325-6"><a href="8-confidence-intervals.html#cb325-6" tabindex="-1"></a>boot_means <span class="ot">&lt;-</span> bootstrap_samples <span class="sc">|&gt;</span> </span>
<span id="cb325-7"><a href="8-confidence-intervals.html#cb325-7" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
<p>We can combine these two operations into a single chain of pipe (<code>|&gt;</code>) operators:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="8-confidence-intervals.html#cb326-1" tabindex="-1"></a>boot_means <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb326-2"><a href="8-confidence-intervals.html#cb326-2" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span> </span>
<span id="cb326-3"><a href="8-confidence-intervals.html#cb326-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight))</span>
<span id="cb326-4"><a href="8-confidence-intervals.html#cb326-4" tabindex="-1"></a>boot_means</span></code></pre></div>
<pre><code># A tibble: 1,000 × 2
   replicate mean_weight
       &lt;int&gt;       &lt;dbl&gt;
 1         1       3.68 
 2         2       3.688
 3         3       3.632
 4         4       3.68 
 5         5       3.679
 6         6       3.675
 7         7       3.678
 8         8       3.706
 9         9       3.643
10        10       3.68 
# ℹ 990 more rows</code></pre>
<p>The data frame <code>boot_means</code> contains 1000 sample mean weights. Each is calculated from a different bootstrap sample and visualized in Figure <a href="8-confidence-intervals.html#fig:one-thousand-sample-means">8.14</a>.</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="8-confidence-intervals.html#cb328-1" tabindex="-1"></a><span class="fu">ggplot</span>(boot_means, <span class="fu">aes</span>(<span class="at">x =</span> mean_weight)) <span class="sc">+</span></span>
<span id="cb328-2"><a href="8-confidence-intervals.html#cb328-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.01</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb328-3"><a href="8-confidence-intervals.html#cb328-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;sample mean weight in grams&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:one-thousand-sample-means"></span>
<img src="ModernDive_files/figure-html/one-thousand-sample-means-1.png" alt="Histogram of 1000 bootstrap sample mean weights of almonds." width="\textwidth" />
<p class="caption">
FIGURE 8.14: Histogram of 1000 bootstrap sample mean weights of almonds.
</p>
</div>
<p>The histogram is a graphical approximation of the <em>bootstrap distribution of the sample mean</em>. This distribution is constructed by getting all the sample means from every bootstrap sample constructed based on the original sample. Since the total number of possible bootstraps is really large, we have not used all of them here, but 1000 of them already provides a good visual approximation.</p>
<p>Observe also that the bootstrap distribution itself can approximate the <em>sampling distribution</em> of the sample mean, a concept we studied in Chapter <a href="7-sampling.html#sampling">7</a> where we took multiple samples from the population. The key difference here is that we resample from a single sample, the original sample, not from the entire population.</p>
<p>By inspecting the histogram in Figure <a href="8-confidence-intervals.html#fig:one-thousand-sample-means">8.14</a>, the bell-shape is apparent. We can also approximate the center and the spread of this distribution by computing the mean and the standard deviation of these 1000 bootstrap sample means:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="8-confidence-intervals.html#cb329-1" tabindex="-1"></a>boot_means <span class="sc">|&gt;</span> </span>
<span id="cb329-2"><a href="8-confidence-intervals.html#cb329-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_of_means =</span> <span class="fu">mean</span>(mean_weight),</span>
<span id="cb329-3"><a href="8-confidence-intervals.html#cb329-3" tabindex="-1"></a>            <span class="at">sd_of_means =</span> <span class="fu">sd</span>(mean_weight))</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  mean_of_means sd_of_means
          &lt;dbl&gt;       &lt;dbl&gt;
1       3.67998   0.0356615</code></pre>
<p>Everything we learned in Chapter <a href="7-sampling.html#sampling">7</a> when studying the sampling distribution of the sample mean applies here. For example, observe that the mean of these bootstrap sample means is near 3.68 grams, very close to the mean of the original sample: 3.682 grams. Our intention is not to study the distribution of the bootstrap samples, but rather to use them to estimate population values, such as the population mean. In the next section, we discuss how can we use these bootstrap samples to construct <em>confidence intervals</em>.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.10)</strong> What is the chief difference between a bootstrap distribution and a sampling distribution?</p>
<p><strong>(LC8.11)</strong> Looking at the bootstrap distribution for the sample mean in Figure <a href="8-confidence-intervals.html#fig:one-thousand-sample-means">8.14</a>, between what two values would you say <em>most</em> values lie?</p>
<p><strong>(LC8.12)</strong> Which of the following is true about the confidence level when constructing a confidence interval?</p>
<ul>
<li>A. The confidence level determines the width of the interval and affects how likely it is to contain the population parameter.</li>
<li>B. The confidence level is always fixed at 95% for all statistical analyses involving confidence intervals.</li>
<li>C. A higher confidence level always results in a narrower confidence interval, making it more useful for practical purposes.</li>
<li>D. The confidence level is only relevant when the population standard deviation is known.</li>
</ul>
<p><strong>(LC8.13)</strong> How does increasing the sample size affect the width of a confidence interval for a given confidence level?</p>
<ul>
<li>A. It increases the width of the confidence interval, making it less precise.</li>
<li>B. It has no effect on the width of the confidence interval since the confidence level is fixed.</li>
<li>C. It decreases the width of the confidence interval, making it more precise by reducing the margin of error.</li>
<li>D. It changes the confidence level directly, regardless of other factors.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="bootstrap-process" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Confidence intervals and the bootstrap: original workflow<a href="8-confidence-intervals.html#bootstrap-process" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The process of determining bootstrap samples and using them for <em>estimation</em> is called <em>bootstrapping</em>.
We can estimate population parameters such as the mean, median, or standard deviation. We can also construct confidence intervals.</p>
<p>In this subsection, we focus on the latter and construct confidence intervals based on bootstrap samples. For this, we review the R syntax and workflow we have already used in previous sections and also introduce a new package: the <code>infer</code> package for tidy and transparent statistical inference.</p>
<div id="original-workflow" class="section level4 unnumbered hasAnchor">
<h4>Original workflow<a href="8-confidence-intervals.html#original-workflow" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that we took bootstrap samples, then calculated the sample means from these samples. Let’s revisit the original workflow using <code>dplyr</code> verbs and the <code>|&gt;</code> operator.</p>
<p>First, we use <code>rep_sample_n()</code> to resample from the original sample <code>almonds_sample_100</code> of 5000 almonds. We set <code>size = 100</code> to generate bootstrap samples of the same size as the original sample, and we resample with replacement by setting <code>replace = TRUE</code>. We create 1000 bootstrap samples by setting <code>reps = 1000</code>:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="8-confidence-intervals.html#cb331-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb331-2"><a href="8-confidence-intervals.html#cb331-2" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">1000</span>)</span></code></pre></div>
<p>Second, we add another pipe followed by <code>summarize()</code> to compute the sample <code>mean()</code> weight for each <code>replicate</code>:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="8-confidence-intervals.html#cb332-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb332-2"><a href="8-confidence-intervals.html#cb332-2" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span> </span>
<span id="cb332-3"><a href="8-confidence-intervals.html#cb332-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_weight =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
<p>For this simple case, all we needed was to use the <code>rep_sample_n()</code> function and a <code>dplyr</code> verb. However, using only <code>dplyr</code> verbs provides us with a limited set of tools that is not ideal when working with more complicated situations. This is the reason we introduce the <code>infer</code> package.</p>
</div>
</div>
<div id="infer-workflow" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> The <code>infer</code> package workflow:<a href="8-confidence-intervals.html#infer-workflow" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>infer</code> package is an R package for statistical inference. It makes efficient use of the <code>|&gt;</code> pipe operator we introduced in Section <a href="3-wrangling.html#piping">3.1</a> to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Just as the <code>dplyr</code> package provides functions with verb-like names to perform data wrangling, the <code>infer</code> package provides functions with intuitive verb-like names to perform statistical inference, such as constructing confidence intervals or performing hypothesis testing. We have discussed the theory-based implementation of the former in section <a href="8-confidence-intervals.html#CI-general">8.1.3</a> and we introduce the latter in Chapter <a href="9-hypothesis-testing.html#hypothesis-testing">9</a>.</p>
<p>Using the example of almonds’ weights, we introduce <code>infer</code> first by comparing its implementation with <code>dplyr</code>. Recall that to calculate a sample statistic or point estimate from a sample, such as the sample mean, when using <code>dplyr</code> we use <code>summarize()</code> and <code>mean()</code>:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="8-confidence-intervals.html#cb333-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb333-2"><a href="8-confidence-intervals.html#cb333-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">stat =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
<p>If we want to use <code>infer</code> instead, we use the functions <code>specify()</code> and <code>calculate()</code> as shown below:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="8-confidence-intervals.html#cb334-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb334-2"><a href="8-confidence-intervals.html#cb334-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span> </span>
<span id="cb334-3"><a href="8-confidence-intervals.html#cb334-3" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;mean&quot;</span>)</span></code></pre></div>
<p>The new structure using <code>infer</code> seems slightly more complicated than the one using <code>dplyr</code> for this simple calculation. These functions will provide three chief benefits moving forward.</p>
<ul>
<li><p>First, the <code>infer</code> verb names better align with the overall simulation-based framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter <a href="9-hypothesis-testing.html#hypothesis-testing">9</a>). We will see flowchart diagrams of this framework in the upcoming Figure <a href="8-confidence-intervals.html#fig:infer-workflow-ci">8.20</a> and in Chapter <a href="9-hypothesis-testing.html#hypothesis-testing">9</a> with Figure <a href="9-hypothesis-testing.html#fig:htdowney">9.11</a>.</p></li>
<li><p>Second, you can transition seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This becomes apparent in Subsection <a href="9-hypothesis-testing.html#comparing-infer-workflows">9.4.2</a> when we compare the <code>infer</code> code for both of these inferential methods.</p></li>
<li><p>Third, the <code>infer</code> workflow is much simpler for conducting inference when you have <em>more than one variable</em>. We introduce <em>two-sample</em> inference where the sample data is collected from two groups, such as in Section <a href="8-confidence-intervals.html#case-study-two-prop-ci">8.4</a> where we study the contagiousness of yawning and in Section <a href="9-hypothesis-testing.html#ht-activity">9.2</a> where we compare the popularity of music genres. Then in Section <a href="10-inference-for-regression.html#infer-regression">10.3</a>, we see situations of <em>inference for regression</em> using the regression models you fit in Chapter <a href="5-regression.html#regression">5</a>.</p></li>
</ul>
<p>We now illustrate the sequence of verbs necessary to construct a confidence interval for <span class="math inline">\(\mu\)</span>, the population mean weight of almonds.</p>
<div id="specify-variables" class="section level4 unnumbered hasAnchor">
<h4>1. <code>specify</code> variables<a href="8-confidence-intervals.html#specify-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:infer-specify"></span>
<img src="images/flowcharts/infer/specify.png" alt="Diagram of the specify() verb." width="40%" height="40%" />
<p class="caption">
FIGURE 8.15: Diagram of the specify() verb.
</p>
</div>
<p>As shown in Figure <a href="8-confidence-intervals.html#fig:infer-specify">8.15</a>, the <code>specify()</code> function is used to choose which variables in a data frame are the focus of our statistical inference. We do this by <code>specify</code>ing the <code>response</code> argument. For example, in our <code>almonds_sample_100</code> data frame of the 100 almonds sampled from the bowl, the variable of interest is <code>weight</code>:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="8-confidence-intervals.html#cb335-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb335-2"><a href="8-confidence-intervals.html#cb335-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight)</span></code></pre></div>
<pre><code>Response: weight (numeric)
# A tibble: 100 × 1
   weight
    &lt;dbl&gt;
 1    4.2
 2    4.2
 3    3.9
 4    3.8
 5    3.3
 6    3.5
 7    4  
 8    3.5
 9    4.2
10    4.1
# ℹ 90 more rows</code></pre>
<p>Notice how the data itself does not change, but the <code>Response: weight (numeric)</code> <em>meta-data</em> does. This is similar to how the <code>group_by()</code> verb from <code>dplyr</code> doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section <a href="3-wrangling.html#groupby">3.4</a>.</p>
<p>We can also specify which variables are the focus of the study by introducing a <code>formula = y ~ x</code> in <code>specify()</code>. This is the same formula notation you saw in Chapters <a href="5-regression.html#regression">5</a> and <a href="6-multiple-regression.html#multiple-regression">6</a> on regression models: the response variable <code>y</code> is separated from the explanatory variable <code>x</code> by a <code>~</code> (“tilde”). The following use of <code>specify()</code> with the <code>formula</code> argument yields the same result seen previously:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="8-confidence-intervals.html#cb337-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb337-2"><a href="8-confidence-intervals.html#cb337-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> weight <span class="sc">~</span> <span class="cn">NULL</span>)</span></code></pre></div>
<p>In the case of almonds we only have a response variable and no explanatory variable of interest. Thus, we set the <code>x</code> on the right-hand side of the <code>~</code> to be <code>NULL</code>.</p>
<p>In cases where inference is focused on a single sample, as it is the almonds’ weights example, either specification works. In examples we present in future sections, the <code>formula</code> specification is simpler and more flexible. In particular, this comes up in the upcoming Section <a href="8-confidence-intervals.html#case-study-two-prop-ci">8.4</a> on comparing two proportions and Section <a href="10-inference-for-regression.html#infer-regression">10.3</a> on inference for regression.</p>
</div>
<div id="generate-replicates" class="section level4 unnumbered hasAnchor">
<h4>2. <code>generate</code> replicates<a href="8-confidence-intervals.html#generate-replicates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:infer-generate"></span>
<img src="images/flowcharts/infer/generate.png" alt="Diagram of generate() replicates." width="40%" height="40%" />
<p class="caption">
FIGURE 8.16: Diagram of generate() replicates.
</p>
</div>
<p>After we <code>specify()</code> the variables of interest, we pipe the results into the <code>generate()</code> function to generate replicates. This is the function that produces the bootstrap samples or performs the similar resampling process a large number of times, based on the variable(s) specified previously, as shown in Figure <a href="8-confidence-intervals.html#fig:infer-generate">8.16</a>. Recall we did this 1000 times.</p>
<p>The <code>generate()</code> function’s first argument is <code>reps</code>, which sets the number of replicates we would like to generate. Since we want to resample the 100 almonds in <code>almonds_sample_100</code> with replacement 1000 times, we set <code>reps = 1000</code>.</p>
<p>The second argument <code>type</code> determines the type of computer simulation used. Setting this to <code>type = "bootstrap"</code> produces bootstrap samples using resampling with replacement. We present different options for <code>type</code> in Chapter <a href="9-hypothesis-testing.html#hypothesis-testing">9</a>.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="8-confidence-intervals.html#cb338-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb338-2"><a href="8-confidence-intervals.html#cb338-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span> </span>
<span id="cb338-3"><a href="8-confidence-intervals.html#cb338-3" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>, <span class="at">type =</span> <span class="st">&quot;bootstrap&quot;</span>)</span></code></pre></div>
<pre><code>Response: weight (numeric)
# A tibble: 100,000 × 2
# Groups:   replicate [1,000]
   replicate weight
       &lt;int&gt;  &lt;dbl&gt;
 1         1    3.6
 2         1    3.4
 3         1    4.2
 4         1    3.4
 5         1    3.5
 6         1    3.4
 7         1    3.8
 8         1    3.7
 9         1    4.1
10         1    3.4
# ℹ 99,990 more rows</code></pre>
<p>Observe that the resulting data frame has 100,000 rows. This is because we have found 1000 bootstrap samples, each with 100 rows.</p>
<p>The variable <code>replicate</code> indicates the bootstrap sample each row belongs to, from <code>1</code> to 1000, each replicate repeated 100 times. The default value of the <code>type</code> argument is <code>"bootstrap"</code> in this scenario, so the inclusion was only made for completeness. If the last line was written simply as <code>generate(reps = 1000)</code>, the result would be the same.</p>
<p><strong>Comparing with original workflow</strong>: Note that the steps of the <code>infer</code> workflow so far produce the same results as the original workflow using the <code>rep_sample_n()</code> function we saw earlier. In other words, the following two code chunks produce similar results:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="8-confidence-intervals.html#cb340-1" tabindex="-1"></a><span class="co"># infer workflow:                   # Original workflow:</span></span>
<span id="cb340-2"><a href="8-confidence-intervals.html#cb340-2" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span>               almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb340-3"><a href="8-confidence-intervals.html#cb340-3" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span>        <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, </span>
<span id="cb340-4"><a href="8-confidence-intervals.html#cb340-4" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>)                             <span class="at">reps =</span> <span class="dv">1000</span>)              </span></code></pre></div>
</div>
<div id="calculate-summary-statistics" class="section level4 unnumbered hasAnchor">
<h4>3. <code>calculate</code> summary statistics<a href="8-confidence-intervals.html#calculate-summary-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:infer-calculate"></span>
<img src="images/flowcharts/infer/calculate.png" alt="Diagram of calculate() summary statistics." width="50%" height="50%" />
<p class="caption">
FIGURE 8.17: Diagram of calculate() summary statistics.
</p>
</div>
<p>After we <code>generate()</code> 1000 bootstrap samples, we want to summarize each of them, for example, by calculating the sample mean of each one of them. As the diagram shows, the <code>calculate()</code> function does this.</p>
<p>In our example, we calculate the mean <code>weight</code> for each bootstrap sample by setting the <code>stat</code> argument equal to <code>"mean"</code> inside the <code>calculate()</code> function. The <code>stat</code> argument can be used for other common summary statistics such as <code>"median"</code>, <code>"sum"</code>, <code>"sd"</code> (standard deviation), and <code>"prop"</code> (proportion). To see a list of other possible summary statistics you can use, type <code>?calculate</code> and read the help file.</p>
<p>Let’s save the result in a data frame called <code>bootstrap_means</code> and explore its contents:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="8-confidence-intervals.html#cb341-1" tabindex="-1"></a>bootstrap_means <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb341-2"><a href="8-confidence-intervals.html#cb341-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span> </span>
<span id="cb341-3"><a href="8-confidence-intervals.html#cb341-3" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span> </span>
<span id="cb341-4"><a href="8-confidence-intervals.html#cb341-4" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb341-5"><a href="8-confidence-intervals.html#cb341-5" tabindex="-1"></a>bootstrap_means</span></code></pre></div>
<pre><code>Response: weight (numeric)
# A tibble: 1,000 × 2
   replicate  stat
       &lt;int&gt; &lt;dbl&gt;
 1         1 3.68 
 2         2 3.688
 3         3 3.632
 4         4 3.68 
 5         5 3.679
 6         6 3.675
 7         7 3.678
 8         8 3.706
 9         9 3.643
10        10 3.68 
# ℹ 990 more rows</code></pre>
<p>Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 <code>replicate</code> values. It also has the mean weight for each bootstrap sample saved in the variable <code>stat</code>.</p>
<p><strong>Comparing with original workflow</strong>: You may have recognized at this point that the <code>calculate()</code> step in the <code>infer</code> workflow produces the same output as the <code>summarize()</code> step in the original workflow.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="8-confidence-intervals.html#cb343-1" tabindex="-1"></a><span class="co"># infer workflow:                   # Original workflow:</span></span>
<span id="cb343-2"><a href="8-confidence-intervals.html#cb343-2" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span>                  almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb343-3"><a href="8-confidence-intervals.html#cb343-3" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span>        <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, </span>
<span id="cb343-4"><a href="8-confidence-intervals.html#cb343-4" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span>                          <span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span>              </span>
<span id="cb343-5"><a href="8-confidence-intervals.html#cb343-5" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;mean&quot;</span>)             <span class="fu">summarize</span>(<span class="at">stat =</span> <span class="fu">mean</span>(weight))</span></code></pre></div>
</div>
<div id="visualize-the-results" class="section level4 unnumbered hasAnchor">
<h4>4. <code>visualize</code> the results<a href="8-confidence-intervals.html#visualize-the-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:infer-visualize"></span>
<img src="images/flowcharts/infer/visualize.png" alt="Diagram of visualize() results." width="60%" />
<p class="caption">
FIGURE 8.18: Diagram of visualize() results.
</p>
</div>
<p>The <code>visualize()</code> verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical <code>stat</code> variable’s values. The pipeline of the main <code>infer</code> verbs used for exploring bootstrap distribution results is shown in Figure <a href="8-confidence-intervals.html#fig:infer-visualize">8.18</a>.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="8-confidence-intervals.html#cb344-1" tabindex="-1"></a><span class="fu">visualize</span>(bootstrap_means)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bootstrap-distribution-infer"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-infer-1.png" alt="Bootstrap distribution." width="\textwidth" />
<p class="caption">
FIGURE 8.19: Bootstrap distribution.
</p>
</div>
<p><strong>Comparing with original workflow</strong>: In fact, <code>visualize()</code> is a <em>wrapper function</em> for the <code>ggplot()</code> function that uses a <code>geom_histogram()</code> layer. Recall that we illustrated the concept of a wrapper function in Figure <a href="5-regression.html#fig:moderndive-figure-wrapper">5.5</a> in Subsection <a href="5-regression.html#model1table">5.1.2</a>.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="8-confidence-intervals.html#cb345-1" tabindex="-1"></a><span class="co"># infer workflow:                    # Original workflow:</span></span>
<span id="cb345-2"><a href="8-confidence-intervals.html#cb345-2" tabindex="-1"></a><span class="fu">visualize</span>(bootstrap_means)           <span class="fu">ggplot</span>(bootstrap_means, <span class="fu">aes</span>(<span class="at">x =</span> stat)) <span class="sc">+</span></span>
<span id="cb345-3"><a href="8-confidence-intervals.html#cb345-3" tabindex="-1"></a>                                        <span class="fu">geom_histogram</span>()</span></code></pre></div>
<p>The <code>visualize()</code> function can take many other arguments to customize the plot further. In future sections we take advantage of this flexibility. In addition, it works with helper functions to add shading of the histogram values corresponding to the confidence interval values.
We have introduced the different elements on the <code>infer</code> workflow for constructing a bootstrap distribution and visualizing it. A summary of these steps is presented in Figure <a href="8-confidence-intervals.html#fig:infer-workflow-ci">8.20</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:infer-workflow-ci"></span>
<img src="images/flowcharts/infer/ci_diagram.png" alt="infer package workflow for confidence intervals." width="80%" />
<p class="caption">
FIGURE 8.20: infer package workflow for confidence intervals.
</p>
</div>
</div>
</div>
<div id="conf-int-infer" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Confidence intervals using bootstrap samples with <code>infer</code><a href="8-confidence-intervals.html#conf-int-infer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are ready to introduce confidence intervals using the bootstrap via <code>infer</code>. We present two different methods for constructing 95% confidence intervals as interval estimates of an unknown population parameter: the <em>percentile method</em> and the <em>standard error method</em>{Bootstrap!standard error method}. Let’s now check out the <code>infer</code> package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the <code>infer</code> package.</p>
<div id="percentile-method-infer" class="section level4 unnumbered hasAnchor">
<h4>Percentile method<a href="8-confidence-intervals.html#percentile-method-infer" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that in Subsection <a href="8-confidence-intervals.html#infer-workflow">8.2.3</a> we have generated 1000 bootstrap samples and stored them in data frame <code>bootstrap_means</code>:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="8-confidence-intervals.html#cb346-1" tabindex="-1"></a>bootstrap_means</span></code></pre></div>
<pre><code>Response: weight (numeric)
# A tibble: 1,000 × 2
   replicate  stat
       &lt;int&gt; &lt;dbl&gt;
 1         1 3.68 
 2         2 3.688
 3         3 3.632
 4         4 3.68 
 5         5 3.679
 6         6 3.675
 7         7 3.678
 8         8 3.706
 9         9 3.643
10        10 3.68 
# ℹ 990 more rows</code></pre>
<p>The sample means stored in <code>bootstrap_means</code> represent a good approximation to the bootstrap distribution of all possible bootstrap samples. The percentile method for constructing 95% confidence intervals sets the lower endpoint of the confidence interval at the 2.5th percentile of <code>bootstrap_means</code> and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean weights of almonds in <code>bootstrap_means</code>. This is the interval estimate of the population mean weight of almonds in the entire bowl.</p>
<p>We can compute the 95% confidence interval by piping <code>bootstrap_means</code> into the <code>get_confidence_interval()</code> function from the <code>infer</code> package, with the confidence <code>level</code> set to 0.95 and the confidence interval <code>type</code> to be <code>"percentile"</code>. We save the results in <code>percentile_ci</code>.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="8-confidence-intervals.html#cb348-1" tabindex="-1"></a>percentile_ci <span class="ot">&lt;-</span> bootstrap_means <span class="sc">|&gt;</span> </span>
<span id="cb348-2"><a href="8-confidence-intervals.html#cb348-2" tabindex="-1"></a>  <span class="fu">get_confidence_interval</span>(<span class="at">level =</span> <span class="fl">0.95</span>, <span class="at">type =</span> <span class="st">&quot;percentile&quot;</span>)</span>
<span id="cb348-3"><a href="8-confidence-intervals.html#cb348-3" tabindex="-1"></a>percentile_ci</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1  3.61198    3.756</code></pre>
<p>Alternatively, we can visualize the interval (3.61, 3.76) by piping the <code>bootstrap_means</code> data frame into the <code>visualize()</code> function and adding a <code>shade_confidence_interval()</code> layer. We set the <code>endpoints</code> argument to be <code>percentile_ci</code>.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="8-confidence-intervals.html#cb350-1" tabindex="-1"></a><span class="fu">visualize</span>(bootstrap_means) <span class="sc">+</span> </span>
<span id="cb350-2"><a href="8-confidence-intervals.html#cb350-2" tabindex="-1"></a>  <span class="fu">shade_confidence_interval</span>(<span class="at">endpoints =</span> percentile_ci)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:percentile-ci-viz"></span>
<img src="ModernDive_files/figure-html/percentile-ci-viz-1.png" alt="Percentile method 95% confidence interval shaded corresponding to potential values." width="\textwidth" />
<p class="caption">
FIGURE 8.21: Percentile method 95% confidence interval shaded corresponding to potential values.
</p>
</div>
<p>Observe in Figure <a href="8-confidence-intervals.html#fig:percentile-ci-viz">8.21</a> that 95% of the sample means stored in the <code>stat</code> variable in <code>bootstrap_means</code> fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right. You also have the option to change the colors of the shading using the <code>color</code> and <code>fill</code> arguments.</p>
<p>The <code>infer</code> package has incorporated a shorter named function <code>shade_ci()</code> that produces the same results. Try out the following code:</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="8-confidence-intervals.html#cb351-1" tabindex="-1"></a><span class="fu">visualize</span>(bootstrap_means) <span class="sc">+</span> </span>
<span id="cb351-2"><a href="8-confidence-intervals.html#cb351-2" tabindex="-1"></a>  <span class="fu">shade_ci</span>(<span class="at">endpoints =</span> percentile_ci, <span class="at">color =</span> <span class="st">&quot;hotpink&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;khaki&quot;</span>)</span></code></pre></div>
</div>
<div id="se-infer" class="section level4 unnumbered hasAnchor">
<h4>Standard error method<a href="8-confidence-intervals.html#se-infer" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Subsection <a href="8-confidence-intervals.html#CI-general">8.1.3</a> we introduced theory-based confidence intervals. We show that a 95% confidence interval can be constructed as</p>
<p><span class="math display">\[\left(\overline{x} - 1.96 \cdot SE(\bar x), \quad \overline{x} + 1.96 \cdot SE(\bar x)\right)\]</span></p>
<p>where <span class="math inline">\(\overline{x}\)</span> is the sample mean of the original sample, 1.96 is the number of standard errors around the mean needed to account for 95% of the area under the density curve (when the distribution is normal), and <span class="math inline">\(SE(\bar x)\)</span> is the standard error of the sample mean that can be computed as <span class="math inline">\(\sigma /\sqrt{n}\)</span> if the population standard deviation is known, or estimated as <span class="math inline">\(s/\sqrt{n}\)</span> if we have to use the sample standard deviation, <span class="math inline">\(s\)</span>, and the sample size, <span class="math inline">\(n\)</span>.</p>
<p>We use the same structure to construct confidence intervals but using the bootstrap sample means to estimate the standard error of <span class="math inline">\(\overline{x}\)</span>.
Thus, the 95% confidence interval for the population mean, <span class="math inline">\(\mu\)</span>, using the standard error estimated via bootstrapping, <span class="math inline">\(SE_\text{boot}\)</span>, is:</p>
<p><span class="math display">\[\left(\overline{x} - 1.96 \cdot SE_{\text{boot}}, \quad \overline{x} + 1.96 \cdot SE_{\text{boot}}\right)\]</span></p>
<p>We can compute this confidence interval using <code>dplyr</code>. First, we calculate the estimated standard error:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="8-confidence-intervals.html#cb352-1" tabindex="-1"></a>SE_boot <span class="ot">&lt;-</span> bootstrap_means <span class="sc">|&gt;</span></span>
<span id="cb352-2"><a href="8-confidence-intervals.html#cb352-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">SE =</span> <span class="fu">sd</span>(stat)) <span class="sc">|&gt;</span></span>
<span id="cb352-3"><a href="8-confidence-intervals.html#cb352-3" tabindex="-1"></a>  <span class="fu">pull</span>(SE)</span>
<span id="cb352-4"><a href="8-confidence-intervals.html#cb352-4" tabindex="-1"></a>SE_boot</span></code></pre></div>
<pre><code>[1] 0.0357</code></pre>
<p>and then use the original sample mean to calculate the 95% confidence interval:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="8-confidence-intervals.html#cb354-1" tabindex="-1"></a>almonds_sample_100 <span class="sc">|&gt;</span></span>
<span id="cb354-2"><a href="8-confidence-intervals.html#cb354-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">lower_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> SE_boot,</span>
<span id="cb354-3"><a href="8-confidence-intervals.html#cb354-3" tabindex="-1"></a>            <span class="at">upper_bound =</span> <span class="fu">mean</span>(weight) <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> SE_boot)</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  lower_bound upper_bound
        &lt;dbl&gt;       &lt;dbl&gt;
1     3.61210     3.75190</code></pre>
<p>Alternatively, computation of the 95% confidence interval can once again be done via <code>infer</code>. We find the sample mean of the original sample and store it in variable <code>x_bar</code></p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="8-confidence-intervals.html#cb356-1" tabindex="-1"></a>x_bar <span class="ot">&lt;-</span> almonds_sample_100 <span class="sc">|&gt;</span> </span>
<span id="cb356-2"><a href="8-confidence-intervals.html#cb356-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">response =</span> weight) <span class="sc">|&gt;</span> </span>
<span id="cb356-3"><a href="8-confidence-intervals.html#cb356-3" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb356-4"><a href="8-confidence-intervals.html#cb356-4" tabindex="-1"></a>x_bar</span></code></pre></div>
<pre><code>Response: weight (numeric)
# A tibble: 1 × 1
   stat
  &lt;dbl&gt;
1 3.682</code></pre>
<p>Now, we pipe the <code>bootstrap_means</code> data frame we created into the <code>get_confidence_interval()</code> function. We set the <code>type</code> argument to be <code>"se"</code> and specify the <code>point_estimate</code> argument to be <code>x_bar</code> in order to set the center of the confidence interval to the sample mean of the original sample.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="8-confidence-intervals.html#cb358-1" tabindex="-1"></a>standard_error_ci <span class="ot">&lt;-</span> bootstrap_means <span class="sc">|&gt;</span> </span>
<span id="cb358-2"><a href="8-confidence-intervals.html#cb358-2" tabindex="-1"></a>  <span class="fu">get_confidence_interval</span>(<span class="at">type =</span> <span class="st">&quot;se&quot;</span>, <span class="at">point_estimate =</span> x_bar, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb358-3"><a href="8-confidence-intervals.html#cb358-3" tabindex="-1"></a>standard_error_ci</span></code></pre></div>
<pre><code># A tibble: 1 × 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1  3.61210  3.75190</code></pre>
<p>The results are the same whether <code>dplyr</code> or <code>infer</code> is used, but as explained earlier, the latter provides more flexibility for other tests.</p>
<p>If we would like to visualize the interval (3.61, 3.75), we can once again pipe the <code>bootstrap_means</code> data frame into the <code>visualize()</code> function and add a <code>shade_confidence_interval()</code> layer to our plot. We set the <code>endpoints</code> argument to be <code>standard_error_ci</code>. The resulting standard-error method based on a 95% confidence interval for <span class="math inline">\(\mu\)</span> can be seen in Figure <a href="8-confidence-intervals.html#fig:se-ci-viz">8.22</a>.</p>

<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="8-confidence-intervals.html#cb360-1" tabindex="-1"></a><span class="fu">visualize</span>(bootstrap_means) <span class="sc">+</span> </span>
<span id="cb360-2"><a href="8-confidence-intervals.html#cb360-2" tabindex="-1"></a>  <span class="fu">shade_confidence_interval</span>(<span class="at">endpoints =</span> standard_error_ci)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:se-ci-viz"></span>
<img src="ModernDive_files/figure-html/se-ci-viz-1.png" alt="Standard-error method 95% confidence interval." width="\textwidth" />
<p class="caption">
FIGURE 8.22: Standard-error method 95% confidence interval.
</p>
</div>
<p>Because we are using bootstrap samples to construct these intervals, we call the percentile and standard error methods simulation-based methods. We can compare the 95% confidence intervals from using both simulation-based methods as well as the one attained using the theory-based method described in <a href="8-confidence-intervals.html#CI-general">8.1.3</a>:</p>
<ul>
<li>Percentile method: (3.61, 3.76)</li>
<li>Standard error method: (3.61, 3.75)</li>
<li>Theory-based method: (3.61, 3.76)</li>
</ul>
<!--
#### Percentile-$t$ method {#infer-percentile-t .unnumbered}

It requires adding a type to generate or calculate, to allow for finding the t at each step without hypothesis
Advantage: Simplest and easiest to compute
Disadvantage: Only works for estimators where the sample standard deviation can be computed directly.
-->
<!--
#### Bias-corrected and accelerated, BCa, method {#bca .unnumbered}
-->
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC8.14)</strong> Construct a 95% confidence interval for the <em>median</em> weight of <em>all</em> almonds. Use the percentile method. Would it be appropriate to also use the standard-error method?</p>
<p><strong>(LC8.15)</strong> What are the advantages of using the <code>infer</code> package for constructing confidence intervals?</p>
<p><strong>(LC8.16)</strong> What is the main purpose of bootstrapping in statistical inference?</p>
<ul>
<li>A. To visualize data distributions and identify outliers.</li>
<li>B. To generate multiple samples from the original data for estimating population parameters and constructing confidence intervals.</li>
<li>C. To replace missing data points with the mean of the dataset.</li>
<li>D. To validate the assumptions of a regression model.</li>
</ul>
<p><strong>(LC8.17)</strong> Which function in the <code>infer</code> package is primarily used to denote the variables of interest for statistical inference?</p>
<ul>
<li>A. <code>rep_sample_n()</code></li>
<li>B. <code>calculate()</code></li>
<li>C. <code>specify()</code></li>
<li>D. <code>visualize()</code></li>
</ul>
<p><strong>(LC8.18)</strong> What is a key difference between the percentile method and the standard error method for constructing confidence intervals using bootstrap samples?</p>
<ul>
<li>A. The percentile method requires the population standard deviation, while the standard error method does not.</li>
<li>B. The percentile method uses the middle 95% of bootstrap sample statistics, while the standard error method uses an estimated standard error from bootstrap samples.</li>
<li>C. The standard error method always results in a narrower confidence interval than the percentile method.</li>
<li>D. The percentile method requires more bootstrap samples than the standard error method.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
</div>
<div id="boot-remarks" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Additional remarks about the bootstrap<a href="8-confidence-intervals.html#boot-remarks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section expands explanations on the bootstrap methods, provides some
historical context, gives a comparison between the theory-based approach and the simulation-based approach when working with confidence intervals, and provides
reasons for using the bootstrap. The presentation is more
theoretical than other sections in this chapter, and you are welcome to skip to
Section <a href="8-confidence-intervals.html#case-study-two-prop-ci">8.4</a> if you want to go over directly another
application of the bootstrap methods in R. Additional theoretical explanations
are available in the <a href="https://moderndive.com/a-appendix.html">Appendices of the online version of the book</a>.</p>
<div id="the-bootstrap-and-other-resampling-methods" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> The bootstrap and other resampling methods<a href="8-confidence-intervals.html#the-bootstrap-and-other-resampling-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The bootstrap is one of many resampling methods. Chernick and LaBudde noted that
the bootstrap’s roots trace back to the development of similar techniques like permutations
and the jackknife <span class="citation">(<a href="#ref-Chernick2011">Chernick and LaBudde 2011</a>)</span>.
The bootstrap was initially conceived as an approximation to another
resampling method called <em>the jackknife</em> but quickly gained
recognition for its broader applicability and efficiency.
Since then, it has been shown in multiple contexts that the bootstrap performs
at least as well as traditional methods in estimating standard errors,
constructing confidence intervals, performing hypothesis testing, and many other
statistical techniques.</p>
<p>Furthermore, since the 1980s (but even more in the last two decades), the use of
simulations to compare advanced
bootstrap methods against other techniques, such as cross-validation, further
established its superiority in specific contexts, particularly when dealing with
small sample sizes.
The use of the bootstrap and bootstrap-related methods has become a cornerstone
in modern statistical and data science practices.</p>
<p>In this section we introduce additional details about the bootstrap, and
explain the advantages and limitations of using the bootstrap.</p>
</div>
<div id="rate-convergence" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Confidence intervals and rate of convergence<a href="8-confidence-intervals.html#rate-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to compare how the bootstrap performs when building confidence intervals
with respect to the theory-based approach discussed in Section
<a href="8-confidence-intervals.html#theory-based-CI">8.1</a>.
The formal comparison requires mathematical concepts beyond the scope of this
book, and it is not pursued here. Instead, we provide just enough elements to
help you with the intuition of how this
comparison is made and why bootstrap-related methods can be as strong as or even
stronger than theory-based or alternative methods.</p>
<p>Let’s start with an illustration using the theory-based confidence interval.
A 95% confidence interval for <span class="math inline">\(\mu\)</span>, when a sample of size <span class="math inline">\(n = 100\)</span> is used,
is given by</p>
<p><span class="math display">\[\overline{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}\]</span></p>
<p>when <span class="math inline">\(\sigma\)</span> is unknown or</p>
<p><span class="math display">\[\overline{x} \pm 1.98 \frac{s}{\sqrt{n}}\]</span></p>
<p>when <span class="math inline">\(\sigma\)</span> is unknown. We use 95% and <span class="math inline">\(n = 100\)</span> for this illustration, but
the exposition is also true with any other confidence level or other
sample sizes.
We understand that 95% of all the possible samples would lead to an interval that
contains <span class="math inline">\(\mu\)</span>.
This statement is exactly true if the distribution of <span class="math inline">\(\overline X\)</span> is precisely normal,
and we say that 95% is the <em>true coverage probability</em>.
In reality, we do not know what is the distribution of the population,
but because of the Central Limit Theorem we know that for a large sample size
<span class="math inline">\(n\)</span> the distribution of <span class="math inline">\(\overline X\)</span> is <em>approximately</em> normal.
In this case, the 95% is an <em>approximate coverage probability</em>.
This means that if we were to take every possible sample of size <span class="math inline">\(n\)</span> and
construct a confidence interval using the formulas above, not exactly 95% of the
intervals will include <span class="math inline">\(\mu\)</span>.
Again, this happens because <span class="math inline">\(\overline X\)</span> is not exactly, but approximately,
normal.
Still, the Central Limit Theorem states that when <span class="math inline">\(n\)</span> tends to infinity,
the distribution of <span class="math inline">\(\overline X\)</span> tends to normal, so the
larger the sample size <span class="math inline">\(n\)</span> the closer the distribution of <span class="math inline">\(\overline X\)</span> is to
the normal distribution, and the smaller the difference between the true and the
approximate coverage probability.</p>
<p>Given that we can never make <span class="math inline">\(n\)</span> infinity in real-life applications, we would
like to produce 95% confidence intervals that make the difference
between the <em>approximate coverage probability</em> and the
<em>true coverage probability</em> as small as possible when we increase the sample
size of our sample.
Imagine a sequence of sample sizes <span class="math inline">\(n_1, n_2, n_3,\dots\)</span> that gets bigger
and bigger and bigger. The rate at which the corresponding consecutive differences
in confidence intervals’ coverage probability decreases is called the
<em>rate of convergence</em> of the difference between
approximate and true coverage probabilities.</p>
<p>In the case of a 95% confidence interval for <span class="math inline">\(\mu\)</span> using the theory-based
approach, the rate of convergence for the difference is about <span class="math inline">\(1/\sqrt{n}\)</span>.
This means that if we increase <span class="math inline">\(n\)</span> from 100 to 400, the difference between the
approximate and true coverage goes down from a factor of <span class="math inline">\(1/\sqrt{100} = 0.1\)</span>
to a factor of <span class="math inline">\(1/\sqrt{400} = 0.05\)</span>. Thus, increasing <span class="math inline">\(n\)</span> four times leads to a
decrease of the difference of about two times. In the statistical literature, a method that has this rate
of convergence is called a <em>first-order correct</em> <span class="citation">(<a href="#ref-Hall1992">Hall 1992</a>)</span>
or <em>first-order accurate</em> <span class="citation">(<a href="#ref-Chernick2011">Chernick and LaBudde 2011</a>)</span>.</p>
<p>The bootstrap percentile method discussed in <a href="8-confidence-intervals.html#conf-int-infer">8.2.4</a>, in the case
of a 95% confidence interval for <span class="math inline">\(\mu\)</span>, is also first-order accurate.
Thus, confidence intervals calculated using the theory-based and the bootstrap percentile method are comparable.
This is consistent with the results we obtained earlier in this chapter.</p>
<p>In general, if you have two different methods that produce similar 95% confidence intervals and
we need to choose one of them, we would choose the one that has a faster rate of
convergence. As we discuss in the next subsection, other bootstrap methods have,
in certain contexts, faster rates of convergence.</p>
</div>
<div id="why-bootstrap-methods" class="section level3 hasAnchor" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> Why bootstrap methods<a href="8-confidence-intervals.html#why-bootstrap-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Why is it suitable to learn and use bootstrap
methods for confidence intervals? The most important reason is that bootstrap
methods, in particular advanced bootstrap methods, can deal with many
limitations of the theory-based approach. Let’s discuss three of these limitations.</p>
<p>First, a 95% confidence interval for <span class="math inline">\(\mu\)</span> is appropriate if the population
distribution is not too extreme and the sample size is large enough for the
distribution of <span class="math inline">\(\overline X\)</span> to be approximately normal.
But there are situations where these conditions are not satisfied, for example,
if the population distribution is heavily skewed to the right, as in the case of
income or wealth; or the distribution is constructed from only two values
(1 or 0) but the chances of getting zero are
much greater than the ones of getting one (for example, chance of getting zero
is 0.999 and chance of getting 1 is 0.001) as it is the case in lottery outcomes
or the presence of some disease in a population.
When these situations are present, the confidence intervals would be inaccurate
because the sample mean <span class="math inline">\(\overline X\)</span> is biased when <span class="math inline">\(n\)</span> is not too large.
This means that if you were to take a large number of random samples and
construct the sample mean of these samples, their average will
be clearly different than <span class="math inline">\(\mu\)</span>, breaking down the theory we developed in
Subsection <a href="8-confidence-intervals.html#theory-based-CI">8.1</a>. This problem may be fixed if the
sample size is large, but, depending of how extreme the population distribution
is, the sample size may need to be extremely large; perhaps in
the order of thousands, or tens of thousands, or even more. Getting samples of
those sizes may not be doable in real-life situations.</p>
<p>Second, in this chapter we have study confidence intervals for the population
mean <span class="math inline">\(\mu\)</span>, because it is a fundamental quantity and it is the foundation for other cases.
However, building confidence intervals for other parameters using the
theory-based approach (for example, for the median, the first quartile,
the standard deviation, etc.) becomes more complicated or even unfeasible.</p>
<p>Third, when working with estimators more complicated than
<span class="math inline">\(\overline X\)</span>, it is often not possible to derive the standard
error estimator with a formula as clean as <span class="math inline">\(\sigma/\sqrt{n}\)</span>. Sometimes, there
is no formula for the standard error and alternative methods have to be used to estimate it.
This can create an additional source of bias. When bias is present, the confidence
intervals created using the
theory-based approach in Subsection <a href="8-confidence-intervals.html#CI-general">8.1.3</a> could be suspect, even
completely useless.</p>
<p>The bootstrap percentile method is not affected directly by the second and third limitations.
It can be implemented for any other parameter beyond the population mean,
as long as all the information needed can be extracted from each bootstrap sample.
On the other hand, the first limitation listed above can also affect the accuracy of
this method.</p>
<p>Fortunately, since the inception of the bootstrap, many
improvements have been made to the percentile method. Bootstrap methods have
been proposed that address the presence of bias either by the limitations discussed above
or the bias created when obtaining estimators or incorporating these methods.
In addition, in certain contexts, these methods also improve the rate of convergence
of the difference between the approximate and true coverage probability.
Some of these methods are the percentile-<span class="math inline">\(t\)</span> and the Bias Correction and
Acceleration bootstrap method (BCa). In terms of rates of convergence, these methods are
<em>second-order accurate</em>; that is, they have a rate
of convergence of about <span class="math inline">\(1/n\)</span>. Another method called the double bootstrap (or more generally,
the iterated bootstrap) can even be a <em>third-order accurate</em>.</p>
<p>We have not included these methods directly here because the theory that
justifies them goes beyond the scope of this book and, when dealing with
confidence intervals for <span class="math inline">\(\mu\)</span> from populations with distributions that are not
extreme, there are not real gains in using them over the theory-based approach
or the percentile method. (We encourage you to check out one implementation of
bias-corrected confidence intervals in the <code>infer</code> package by setting
<code>type = "bias-corrected"</code> in the <code>get_confidence_interval()</code>
function.)</p>
<p>To summarize, when working with skewed
distributions, small sample sizes, estimators of parameters other than <span class="math inline">\(\mu\)</span>
(such as the median), or the estimation of the standard error when there are no
formulas to obtain them, many advanced bootstrap methods would be preferred over
the theory-based approach. In the Appendices of the online version of the book,
we plan to explore some of these advanced methods and present simulations
that show when these methods are preferred over the percentile method or the
theory-based approach.</p>
</div>
</div>
<div id="case-study-two-prop-ci" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Case study: is yawning contagious?<a href="8-confidence-intervals.html#case-study-two-prop-ci" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s apply our knowledge of confidence intervals to answer the question: “Is yawning contagious?”. If you see someone else yawn, are you more likely to yawn? In an episode of the US show <em>Mythbusters</em> that aired on Discovery, the hosts conducted an experiment to answer this question. More information about the episode is available on <a href="https://www.imdb.com/title/tt0768479/">IMDb</a>.</p>
<div id="mythbusters-study-data" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> <em>Mythbusters</em> study data<a href="8-confidence-intervals.html#mythbusters-study-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the <em>Mythbusters</em> team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available as <code>mythbusters_yawn</code> included in the <code>moderndive</code> package: </p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="8-confidence-intervals.html#cb361-1" tabindex="-1"></a>mythbusters_yawn</span></code></pre></div>
<pre><code># A tibble: 50 × 3
    subj group   yawn 
   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;
 1     1 seed    yes  
 2     2 control yes  
 3     3 seed    no   
 4     4 seed    yes  
 5     5 seed    no   
 6     6 control no   
 7     7 seed    yes  
 8     8 control no   
 9     9 control no   
10    10 seed    no   
# ℹ 40 more rows</code></pre>
<p>The variables are:</p>
<ul>
<li><code>subj</code>: The participant ID with values 1 through 50.</li>
<li><code>group</code>: A binary <em>treatment</em> variable indicating whether the participant was exposed to yawning. <code>"seed"</code> indicates the participant was exposed to yawning while <code>"control"</code> indicates the participant was not.</li>
<li><code>yawn</code>: A binary <em>response</em> variable indicating whether the participant ultimately yawned.</li>
</ul>
<p>Recall that you learned about treatment and response variables in Subsection <a href="5-regression.html#correlation-is-not-causation">5.3.1</a> in our discussion on confounding variables. </p>
<p>Let’s use some data wrangling to calculate counts of the four possible outcomes:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="8-confidence-intervals.html#cb363-1" tabindex="-1"></a>mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb363-2"><a href="8-confidence-intervals.html#cb363-2" tabindex="-1"></a>  <span class="fu">group_by</span>(group, yawn) <span class="sc">|&gt;</span> </span>
<span id="cb363-3"><a href="8-confidence-intervals.html#cb363-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">count =</span> <span class="fu">n</span>(), <span class="at">.groups =</span> <span class="st">&quot;keep&quot;</span>)</span></code></pre></div>
<pre><code># A tibble: 4 × 3
# Groups:   group, yawn [4]
  group   yawn  count
  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;
1 control no       12
2 control yes       4
3 seed    no       24
4 seed    yes      10</code></pre>
<p>Let’s first focus on the <code>"control"</code> group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25% did yawn.</p>
<p>Let’s now focus on the <code>"seed"</code> group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4% - 25% = 4.4% more often than those who were not.</p>
</div>
<div id="sampling-scenario" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Sampling scenario<a href="8-confidence-intervals.html#sampling-scenario" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s review the terminology and notation related to sampling we studied in Subsection <a href="7-sampling.html#terminology-and-notation">7.2.1</a>. In Chapter <a href="7-sampling.html#sampling">7</a> our <em>study population</em> was the bowl of <span class="math inline">\(N\)</span> = 2400 balls. Our <em>population parameter</em> of interest was the <em>population proportion</em> of these balls that were red, denoted mathematically by <span class="math inline">\(p\)</span>. In order to estimate <span class="math inline">\(p\)</span>, we extracted a sample of 50 balls using the shovel and computed the relevant <em>point estimate</em>: the <em>sample proportion</em> that were red, denoted mathematically by <span class="math inline">\(\widehat{p}\)</span>.</p>
<p>Who is the study population here? All humans? All the people who watch the show <em>Mythbusters</em>? It’s hard to say! This question can only be answered if we know how the show’s hosts recruited participants! In other words, what was the <em>sampling methodology</em> used by the <em>Mythbusters</em> to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we’ll <em>assume</em> that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we’ll be assuming that any results of this experiment will generalize to all <span class="math inline">\(N\)</span> = 346 million Americans (2024 population estimate).</p>
<p>Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the <em>difference in population proportions</em> <span class="math inline">\(p_{seed} - p_{control}\)</span>, where <span class="math inline">\(p_{seed}\)</span> is the proportion of <em>all</em> Americans who if exposed to yawning will yawn themselves, and <span class="math inline">\(p_{control}\)</span> is the proportion of <em>all</em> Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the <em>Mythbusters</em>’ sample of participants will be the <em>difference in sample proportions</em> <span class="math inline">\(\widehat{p}_{seed} - \widehat{p}_{control}\)</span>. Let’s extend Table <a href="8-confidence-intervals.html#tab:table-ch8-c">8.1</a> of scenarios of sampling for inference to include our latest scenario.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:table-ch8-c">TABLE 8.1: </span>Scenarios of sampling for inference
</caption>
<thead>
<tr>
<th style="text-align:right;">
Scenario
</th>
<th style="text-align:left;">
Population parameter
</th>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
Point estimate
</th>
<th style="text-align:left;">
Symbol(s)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 0.5in; ">
1
</td>
<td style="text-align:left;width: 1.5in; ">
Population proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
2
</td>
<td style="text-align:left;width: 1.5in; ">
Population mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\mu\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\overline{x}\)</span> or <span class="math inline">\(\widehat{\mu}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
3
</td>
<td style="text-align:left;width: 1.5in; ">
Difference in population proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p_1 - p_2\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Difference in sample proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}_1 - \widehat{p}_2\)</span>
</td>
</tr>
</tbody>
</table>
<p>This is known as a <em>two-sample</em> inference situation since we have two separate samples. Based on their two-samples of size <span class="math inline">\(n_{seed}\)</span> = 34 and <span class="math inline">\(n_{control}\)</span> = 16, the point estimate is</p>
<p><span class="math display">\[
\widehat{p}_{seed} - \widehat{p}_{control} = \frac{24}{34} - \frac{12}{16} = 0.04411765 \approx 4.4\%
\]</span></p>
<p>However, say the <em>Mythbusters</em> repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they find the exact same estimated difference of 4.4%? Probably not, again, because of <em>sampling variation</em>.</p>
<p>How does this sampling variation affect their estimate of 4.4%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the <em>Mythbusters</em> only have a single two-sample of 50 participants, they would have to construct a 95% confidence interval for <span class="math inline">\(p_{seed} - p_{control}\)</span> using <em>bootstrap resampling with replacement</em>.</p>
<p>We make a couple of important notes. First, for the comparison between the <code>"seed"</code> and <code>"control"</code> groups to make sense, however, both groups need to be <em>independent</em> from each other. Otherwise, they could influence each other’s results. This means that a participant being selected for the <code>"seed"</code> or <code>"control"</code> group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn’t necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable.</p>
<p>Second, the order of the subtraction in the difference doesn’t matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of <span class="math inline">\(\widehat{p}_{seed} - \widehat{p}_{control}\)</span> or <span class="math inline">\(\widehat{p}_{control} - \widehat{p}_{seed}\)</span> does not make a material difference, you just need to stay consistent and interpret your results accordingly.</p>
</div>
<div id="ci-build" class="section level3 hasAnchor" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Constructing the confidence interval<a href="8-confidence-intervals.html#ci-build" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we did in Subsection <a href="8-confidence-intervals.html#infer-workflow">8.2.3</a>, let’s first construct the bootstrap distribution for <span class="math inline">\(\widehat{p}_{seed} - \widehat{p}_{control}\)</span> and then use this to construct 95% confidence intervals for <span class="math inline">\(p_{seed} - p_{control}\)</span>. We’ll do this using the <code>infer</code> workflow again. However, since the difference in proportions is a new scenario for inference, we’ll need to use some new arguments in the <code>infer</code> functions along the way.</p>
<div id="specify-variables-1" class="section level4 unnumbered hasAnchor">
<h4>1. <code>specify</code> variables<a href="8-confidence-intervals.html#specify-variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s take our <code>mythbusters_yawn</code> data frame and <code>specify()</code> which variables are of interest using the <code>y ~ x</code> formula interface where:</p>
<ul>
<li>Our response variable is <code>yawn</code>: whether or not a participant yawned. It has levels <code>"yes"</code> and <code>"no"</code>.</li>
<li>The explanatory variable is <code>group</code>. It has levels <code>"seed"</code> (exposed to yawning) and <code>"control"</code> (not exposed to yawning).</li>
</ul>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="8-confidence-intervals.html#cb365-1" tabindex="-1"></a>mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb365-2"><a href="8-confidence-intervals.html#cb365-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group)</span></code></pre></div>
<pre><code>Error: A level of the response variable `yawn` needs to be specified for the 
`success` argument in `specify()`.</code></pre>
<p>Alas, we got an error message that <code>infer</code> is telling us that one of the levels of the categorical variable <code>yawn</code> needs to be defined as the <code>success</code>. Recall that we define <code>success</code> to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who <code>"yes"</code> yawned or those who <code>"no"</code> didn’t yawn? This isn’t clear to R or someone just picking up the code and results for the first time, so we need to set the <code>success</code> argument to <code>"yes"</code> as follows to improve the transparency of the code:</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="8-confidence-intervals.html#cb367-1" tabindex="-1"></a>mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb367-2"><a href="8-confidence-intervals.html#cb367-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group, <span class="at">success =</span> <span class="st">&quot;yes&quot;</span>)</span></code></pre></div>
<pre><code>Response: yawn (factor)
Explanatory: group (factor)
# A tibble: 50 × 2
   yawn  group  
   &lt;fct&gt; &lt;fct&gt;  
 1 yes   seed   
 2 yes   control
 3 no    seed   
 4 yes   seed   
 5 no    seed   
 6 no    control
 7 yes   seed   
 8 no    control
 9 no    control
10 no    seed   
# ℹ 40 more rows</code></pre>
</div>
<div id="generate-replicates-1" class="section level4 unnumbered hasAnchor">
<h4>2. <code>generate</code> replicates<a href="8-confidence-intervals.html#generate-replicates-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Our next step is to perform <em>bootstrap resampling with replacement</em> like we did with the almonds in the activity in Section <a href="8-confidence-intervals.html#revisit-almond-bootstrap">8.2.1</a>. We saw how it works with both a single variable in computing bootstrap means in Section <a href="8-confidence-intervals.html#bootstrap-process">8.2.2</a>, but we haven’t yet worked with bootstrapping involving multiple variables.</p>
<p>In the <code>infer</code> package, bootstrapping with multiple variables means that each <em>row</em> is potentially resampled. Let’s investigate this by focusing only on the first six rows of <code>mythbusters_yawn</code>:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="8-confidence-intervals.html#cb369-1" tabindex="-1"></a>first_six_rows <span class="ot">&lt;-</span> <span class="fu">head</span>(mythbusters_yawn)</span>
<span id="cb369-2"><a href="8-confidence-intervals.html#cb369-2" tabindex="-1"></a>first_six_rows</span></code></pre></div>
<pre><code># A tibble: 6 × 3
   subj group   yawn 
  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;
1     1 seed    yes  
2     2 control yes  
3     3 seed    no   
4     4 seed    yes  
5     5 seed    no   
6     6 control no   </code></pre>
<p>When we bootstrap this data, we are potentially pulling the subject’s readings multiple times. Thus, we could see the entries of <code>"seed"</code> for <code>group</code> and <code>"no"</code> for <code>yawn</code> together in a new row in a bootstrap sample. This is further seen by exploring the <code>sample_n()</code> function in <code>dplyr</code> on this smaller 6-row data frame comprised of <code>head(mythbusters_yawn)</code>. The <code>sample_n()</code> function can perform this bootstrapping procedure and is similar to the <code>rep_sample_n()</code> function in <code>infer</code>, except that it is not repeated, but rather only performs one sample with or without replacement.</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="8-confidence-intervals.html#cb371-1" tabindex="-1"></a>first_six_rows <span class="sc">|&gt;</span> </span>
<span id="cb371-2"><a href="8-confidence-intervals.html#cb371-2" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="at">size =</span> <span class="dv">6</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code># A tibble: 6 × 3
   subj group   yawn 
  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;
1     6 control no   
2     1 seed    yes  
3     2 control yes  
4     6 control no   
5     4 seed    yes  
6     4 seed    yes  </code></pre>
<p>We can see that in this bootstrap sample generated from the first six rows of <code>mythbusters_yawn</code>, we have some rows repeated. The same is true when we perform the <code>generate()</code> step in <code>infer</code> as done in what follows. Using this fact, we <code>generate</code> 1000 replicates, or, in other words, we bootstrap resample the 50 participants with replacement 1000 times.</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="8-confidence-intervals.html#cb373-1" tabindex="-1"></a>mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb373-2"><a href="8-confidence-intervals.html#cb373-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group, <span class="at">success =</span> <span class="st">&quot;yes&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb373-3"><a href="8-confidence-intervals.html#cb373-3" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>, <span class="at">type =</span> <span class="st">&quot;bootstrap&quot;</span>)</span></code></pre></div>
<pre><code>Response: yawn (factor)
Explanatory: group (factor)
# A tibble: 50,000 × 3
# Groups:   replicate [1,000]
   replicate yawn  group  
       &lt;int&gt; &lt;fct&gt; &lt;fct&gt;  
 1         1 yes   seed   
 2         1 yes   control
 3         1 no    control
 4         1 no    control
 5         1 yes   seed   
 6         1 yes   seed   
 7         1 yes   seed   
 8         1 yes   seed   
 9         1 no    seed   
10         1 yes   seed   
# ℹ 49,990 more rows</code></pre>
<p>Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 participants with replacement 1000 times and 50,000 = 1000 <span class="math inline">\(\cdot\)</span> 50. The variable <code>replicate</code> indicates which resample each row belongs to. So it has the value <code>1</code> 50 times, the value <code>2</code> 50 times, all the way through to the value <code>1000</code> 50 times.</p>
</div>
<div id="calculate-summary-statistics-1" class="section level4 unnumbered hasAnchor">
<h4>3. <code>calculate</code> summary statistics<a href="8-confidence-intervals.html#calculate-summary-statistics-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After we <code>generate()</code> many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size 50 with a single summary statistic, the difference in proportions. We do this by setting the <code>stat</code> argument to <code>"diff in props"</code>:</p>
<!-- 
Chester: A challenging Learning check for those {dplyr} diehards is to get these values 
without using {infer}. It takes a double group_by() and some trickery, but could 
be a good exercise for those that don't quite see the power of {infer}.

Albert: Great idea!
-->
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="8-confidence-intervals.html#cb375-1" tabindex="-1"></a>mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb375-2"><a href="8-confidence-intervals.html#cb375-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group, <span class="at">success =</span> <span class="st">&quot;yes&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb375-3"><a href="8-confidence-intervals.html#cb375-3" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>, <span class="at">type =</span> <span class="st">&quot;bootstrap&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb375-4"><a href="8-confidence-intervals.html#cb375-4" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;diff in props&quot;</span>)</span></code></pre></div>
<pre><code>Warning message:
The statistic is based on a difference or ratio; by default, for 
difference-based statistics, the explanatory variable is subtracted in the 
order &quot;control&quot; - &quot;seed&quot;, or divided in the order &quot;control&quot; / &quot;seed&quot; for 
ratio-based statistics. To specify this order yourself, supply 
`order = c(&quot;control&quot;, &quot;seed&quot;)` to the calculate() function. </code></pre>
<p>We see another warning here. We need to specify the order of the subtraction. Is it <span class="math inline">\(\widehat{p}_{seed} - \widehat{p}_{control}\)</span> or <span class="math inline">\(\widehat{p}_{control} - \widehat{p}_{seed}\)</span>. We specify it to be <span class="math inline">\(\widehat{p}_{seed} - \widehat{p}_{control}\)</span> by setting <code>order = c("seed", "control")</code>. Note that you could’ve also set <code>order = c("control", "seed")</code>. As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly.</p>
<p>Let’s save the output in a data frame <code>bootstrap_distribution_yawning</code>:</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="8-confidence-intervals.html#cb377-1" tabindex="-1"></a>bootstrap_distribution_yawning <span class="ot">&lt;-</span> mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb377-2"><a href="8-confidence-intervals.html#cb377-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group, <span class="at">success =</span> <span class="st">&quot;yes&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb377-3"><a href="8-confidence-intervals.html#cb377-3" tabindex="-1"></a>  <span class="fu">generate</span>(<span class="at">reps =</span> <span class="dv">1000</span>, <span class="at">type =</span> <span class="st">&quot;bootstrap&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb377-4"><a href="8-confidence-intervals.html#cb377-4" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;diff in props&quot;</span>, <span class="at">order =</span> <span class="fu">c</span>(<span class="st">&quot;seed&quot;</span>, <span class="st">&quot;control&quot;</span>))</span>
<span id="cb377-5"><a href="8-confidence-intervals.html#cb377-5" tabindex="-1"></a>bootstrap_distribution_yawning</span></code></pre></div>
<pre><code># A tibble: 1,000 × 2
   replicate        stat
       &lt;int&gt;       &lt;dbl&gt;
 1         1  0.0357143 
 2         2  0.229167  
 3         3  0.00952381
 4         4  0.0106952 
 5         5  0.00483092
 6         6  0.00793651
 7         7 -0.0845588 
 8         8 -0.00466200
 9         9  0.164686  
10        10  0.124777  
# ℹ 990 more rows</code></pre>
<p>Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 <code>replicate</code> ID’s and the 1000 differences in proportions for each bootstrap resample in <code>stat</code>.</p>
</div>
<div id="visualize-the-results-1" class="section level4 unnumbered hasAnchor">
<h4>4. <code>visualize</code> the results<a href="8-confidence-intervals.html#visualize-the-results-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Figure <a href="8-confidence-intervals.html#fig:bootstrap-distribution-mythbusters">8.23</a> we <code>visualize()</code> the resulting bootstrap resampling distribution. Let’s also add a vertical line at 0 by adding a <code>geom_vline()</code> layer.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bootstrap-distribution-mythbusters"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-mythbusters-1.png" alt="Bootstrap distribution." width="\textwidth" />
<p class="caption">
FIGURE 8.23: Bootstrap distribution.
</p>
</div>
<p>First, let’s compute the 95% confidence interval for <span class="math inline">\(p_{seed} - p_{control}\)</span> using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped.</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="8-confidence-intervals.html#cb379-1" tabindex="-1"></a>bootstrap_distribution_yawning <span class="sc">|&gt;</span> </span>
<span id="cb379-2"><a href="8-confidence-intervals.html#cb379-2" tabindex="-1"></a>  <span class="fu">get_confidence_interval</span>(<span class="at">type =</span> <span class="st">&quot;percentile&quot;</span>, <span class="at">level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code># A tibble: 1 × 2
   lower_ci upper_ci
      &lt;dbl&gt;    &lt;dbl&gt;
1 -0.238276 0.302464</code></pre>
<p>Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the <code>point_estimate</code> argument. In our case, we need to set it to be the difference in sample proportions of 4.4% that the <em>Mythbusters</em> observed.</p>
<p>We can also use the <code>infer</code> workflow to compute this value by excluding the <code>generate()</code> 1000 bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the <code>generate()</code> line, telling R to ignore it:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="8-confidence-intervals.html#cb381-1" tabindex="-1"></a>obs_diff_in_props <span class="ot">&lt;-</span> mythbusters_yawn <span class="sc">|&gt;</span> </span>
<span id="cb381-2"><a href="8-confidence-intervals.html#cb381-2" tabindex="-1"></a>  <span class="fu">specify</span>(<span class="at">formula =</span> yawn <span class="sc">~</span> group, <span class="at">success =</span> <span class="st">&quot;yes&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb381-3"><a href="8-confidence-intervals.html#cb381-3" tabindex="-1"></a>  <span class="co"># generate(reps = 1000, type = &quot;bootstrap&quot;) |&gt; </span></span>
<span id="cb381-4"><a href="8-confidence-intervals.html#cb381-4" tabindex="-1"></a>  <span class="fu">calculate</span>(<span class="at">stat =</span> <span class="st">&quot;diff in props&quot;</span>, <span class="at">order =</span> <span class="fu">c</span>(<span class="st">&quot;seed&quot;</span>, <span class="st">&quot;control&quot;</span>))</span>
<span id="cb381-5"><a href="8-confidence-intervals.html#cb381-5" tabindex="-1"></a>obs_diff_in_props</span></code></pre></div>
<pre><code>Response: yawn (factor)
Explanatory: group (factor)
# A tibble: 1 × 1
       stat
      &lt;dbl&gt;
1 0.0441176</code></pre>
<p>We thus plug this value in as the <code>point_estimate</code> argument.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="8-confidence-intervals.html#cb383-1" tabindex="-1"></a>myth_ci_se <span class="ot">&lt;-</span> bootstrap_distribution_yawning <span class="sc">|&gt;</span> </span>
<span id="cb383-2"><a href="8-confidence-intervals.html#cb383-2" tabindex="-1"></a>  <span class="fu">get_confidence_interval</span>(<span class="at">type =</span> <span class="st">&quot;se&quot;</span>, <span class="at">point_estimate =</span> obs_diff_in_props)</span></code></pre></div>
<pre><code>Using `level = 0.95` to compute confidence interval.</code></pre>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="8-confidence-intervals.html#cb385-1" tabindex="-1"></a>myth_ci_se</span></code></pre></div>
<pre><code># A tibble: 1 × 2
   lower_ci upper_ci
      &lt;dbl&gt;    &lt;dbl&gt;
1 -0.227291 0.315526</code></pre>
<p>Let’s visualize both confidence intervals in Figure <a href="8-confidence-intervals.html#fig:bootstrap-distribution-mythbusters-CI">8.24</a>, with the percentile method interval marked with black lines and the standard-error method marked with grey lines. Observe that they are both similar to each other.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bootstrap-distribution-mythbusters-CI"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-mythbusters-CI-1.png" alt="Two 95\% confidence intervals: percentile method (black) and standard error method (grey)." width="\textwidth" />
<p class="caption">
FIGURE 8.24: Two 95% confidence intervals: percentile method (black) and standard error method (grey).
</p>
</div>
</div>
</div>
<div id="interpreting-the-confidence-interval" class="section level3 hasAnchor" number="8.4.4">
<h3><span class="header-section-number">8.4.4</span> Interpreting the confidence interval<a href="8-confidence-intervals.html#interpreting-the-confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given that both confidence intervals are quite similar, let’s focus our interpretation to only the percentile method confidence interval of (-0.238, 0.302). The precise statistical interpretation of a 95% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of <span class="math inline">\(p_{seed} - p_{control}\)</span>. In other words, if we gathered 100 samples of <span class="math inline">\(n\)</span> = 50 participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of <span class="math inline">\(p_{seed} - p_{control}\)</span> while about five won’t. Given that this is a little long winded, we use the shorthand interpretation: we’re 95% “confident” that the true difference in proportions <span class="math inline">\(p_{seed} - p_{control}\)</span> is between (-0.238, 0.302).</p>
<p>There is one value of particular interest that this 95% confidence interval contains: zero. If <span class="math inline">\(p_{seed} - p_{control}\)</span> were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself.</p>
<p>In our case, since the 95% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our 1000 bootstrap resamples with replacement, sometimes <span class="math inline">\(\widehat{p}_{seed}\)</span> was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened.</p>
<p>Say, on the other hand, the 95% confidence interval was entirely above zero. This would suggest that <span class="math inline">\(p_{seed} - p_{control} &gt; 0\)</span>, or, in other words <span class="math inline">\(p_{seed} &gt; p_{control}\)</span>, and thus we’d have evidence suggesting those exposed to yawning do yawn more often.</p>
</div>
</div>
<div id="summary-CI" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Summary and final remarks<a href="8-confidence-intervals.html#summary-CI" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="additional-resources-6" class="section level3 hasAnchor" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Additional resources<a href="8-confidence-intervals.html#additional-resources-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you want more examples of the <code>infer</code> workflow to construct confidence intervals, we suggest you check out the <code>infer</code> package homepage, in particular, a series of example analyses available at <a href="https://infer.netlify.app/articles/" class="uri">https://infer.netlify.app/articles/</a>.</p>
</div>
<div id="whats-to-come-7" class="section level3 hasAnchor" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> What’s to come?<a href="8-confidence-intervals.html#whats-to-come-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve equipped ourselves with confidence intervals, in Chapter <a href="9-hypothesis-testing.html#hypothesis-testing">9</a> we’ll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we’ll see that the framework for making such inferences is slightly different.</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Chernick2011" class="csl-entry">
Chernick, Michael R., and Robert A. LaBudde. 2011. <em>An Introduction to Bootstrap Methods with Applications to r</em>. First. Hoboken, NJ: Wiley.
</div>
<div id="ref-Efron1979" class="csl-entry">
Efron, Bradley. 1979. <span>“Bootstrap Methods: Another Look at the Jackknife.”</span> <em>The Annals of Statistics</em> Volume 7 (1).
</div>
<div id="ref-EfronTibshi1986" class="csl-entry">
Efron, Bradley, and Robert Tibshirani. 1986. <span>“Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.”</span> <em>Statistical Science</em> Volume 1 (1).
</div>
<div id="ref-Hall1986" class="csl-entry">
Hall, Peter. 1986. <span>“On the Bootstrap and Confidence Intervals.”</span> <em>The Annals of Statistics</em> Volume 14 (4).
</div>
<div id="ref-Hall1988" class="csl-entry">
———. 1988. <span>“Theoretical Comparison of Bootstrap Confidence Intervals.”</span> <em>The Annals of Statistics</em> Volume 16 (3).
</div>
<div id="ref-Hall1992" class="csl-entry">
———. 1992. <em>The Bootstrap and Edgeworth Expansion</em>. First. New York: Springer series in statistics.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9-hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/moderndive/moderndive_book/edit/master/08-confidence-intervals.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ModernDive.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
