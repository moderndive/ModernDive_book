
Needed packages

```{r message=FALSE}
library(tidyverse)
library(moderndive)
library(infer)
library(gridExtra)
library(GGally)
``` 

**(LC10.1)** Meaning of the error term $\epsilon$ in $Y = \beta_0 + \beta_1 X + \epsilon$.

**Solution:** **C.** 
It is the part of the response not explained by the line.

**(LC10.2)** A key property of the least squares estimates $b_0, b_1$.

**Solution:** **B.** 
They are linear combinations of the observed responses $y_1, y_2, \ldots, y_n$.

**(LC10.3)** How to encode a two-group difference in means with regression.

**Solution:** **C.** 
Include a dummy variable for the groups; its coefficient equals the mean difference.

**(LC10.4)** Meaning of the null hypothesis $H_0: \beta_1 = 0$.

**Solution:** **A.** 
There is no linear association between the explanatory variable and the response.

**(LC10.5)** An assumption required by the linear regression model.

**Solution:** **A.** 
The error terms are normally distributed with mean zero and constant variance.

**(LC10.6)** Why the slope estimate $b_1$ is a random variable.

**Solution:** **C.** 
It varies from sample to sample due to sampling variation.

**(LC10.7)** Residual analysis for UN data ($y =$ fertility, $x =$ life expectancy).

**Solution:**
Compute fitted values and residuals with `get_regression_points()`. 

```{r}
simple_model <- lm(fertility_rate_2022 ~ hdi_2022, 
                   data = na.omit(un_member_states_2024))
regression_points <- get_regression_points(simple_model, ID = "country")
regression_points
```

```{r model1residualshist2, echo=FALSE, warning=FALSE, fig.cap="Histogram of residuals."}
ggplot(regression_points, aes(x = residual)) +
  geom_histogram(binwidth = 0.25, color = "white") +
  labs(x = "Residual")
```

The residuals appear to follow a **N**ormal distribution pretty closely. There is one bin near 0.5 that is a bit high, but overall the histogram looks reasonably symmetric and mound-shaped.

```{r fig.cap="Plot of residuals over HDI."}
ggplot(regression_points, aes(x = hdi_2022, y = residual)) +
  geom_point() +
  labs(x = "HDI", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", linewidth = 1)
```

This plot seems to fit equality of variance. There doesn't appear to be a strong "fan formation" in this graph.

**(LC10.8)** Interpretation of a near-zero $p$-value for the slope.

**Solution:** **B.** 
There is strong evidence against $H_0: \beta_1 = 0$, indicating a linear relationship between the explanatory and response variables.

**(LC10.9)** Which assumptions a residual plot helps assess.

**Solution:**
Residual plots help check linearity and equal variance. They do not directly check independence (need residuals vs time if sequential data). They are not sufficient for normality, which requires histograms or QQ-plots.

**(LC10.10)** Meaning of a U-shaped residual plot.

**Solution:** **B.** 
It suggests violation of the linearity assumption.

**(LC10.11)** Simulation-based inference for the correlation coefficient.

**Solution:**
Use the `infer` workflow with `stat = "correlation"` on `waiting ~ duration`.

* CI: `specify() |> generate(reps, type = "bootstrap") |> calculate(stat = "correlation") |> get_confidence_interval(type = "percentile", level = 0.95)`.
* Test $H_0:\rho=0$: `specify() |> hypothesize(null = "independence") |> generate(reps, type = "permute") |> calculate(stat = "correlation")`, then compare the observed correlation with `shade_p_value()` / `get_p_value(direction = "both")`.

**(LC10.12)** Why the bootstrap percentile method is appropriate for $\beta_1$ here.

**Solution:** **D.** 
It does not require the bootstrap distribution to be normally shaped.

**(LC10.13)** Role of the permutation test for $\beta_1$.

**Solution:** **B.** 
It assesses whether the observed slope could occur by chance under $H_0$ of no relationship.

**(LC10.14)** $p$-value near 0 from the null distribution of slopes.

**Solution:** **C.** 
The observed slope is significantly different from 0, suggesting a meaningful relationship.

**(LC10.15)** Meaning of $\beta_j$ in multiple regression.

**Solution:** **D.** 
The partial slope for $X_j$, accounting for all other regressors.

**(LC10.16)** Why convert `continent_of_origin` to a factor.

**Solution:** **B.** 
To create dummy variables representing its categories.

**(LC10.17)** Purpose of a scatterplot matrix here.

**Solution:** **C.** 
Examine pairwise linear relationships and spot multicollinearity among regressors.

**(LC10.18)** Role of dummy variables for `continent_of_origin`.

**Solution:** **B.** 
They shift the intercept according to the category.

**(LC10.19)** Why it matters that estimators are unbiased.

**Solution:** **B.** 
On average, the estimates equal the true population parameters.

**(LC10.20)** Why coefficients change with different regressor sets.

**Solution:** **C.** 
Each coefficient depends on the specific combination of regressors in the model.

**(LC10.21)** Constructing a 95% CI for a coefficient in MLR.

**Solution:** **A.** 
Point estimate $\pm$ (critical $t$ value $\times$ standard error).

**(LC10.22)** What the ANOVA model-comparison test evaluates.

**Solution:** **B.** 
Whether the reduced model is adequate or the full model is needed.

**(LC10.23)** Why use simulation-based methods in MLR.

**Solution:** **D.** 
They donâ€™t rely on normality assumptions or large sample sizes.

**(LC10.24)** Purpose of the bootstrap distribution for partial slopes.

**Solution:** **B.** 
To approximate the sampling distribution by resampling with replacement.

**(LC10.25)** CI for a partial slope includes 0.

**Solution:** **A.** 
No statistically significant relationship with the response (at the chosen level).

**(LC10.26)** Observed test statistic far to the right of the null distribution.

**Solution:** **C.** 
Likely statistically significant; reject the null.
